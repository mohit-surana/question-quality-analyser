Multiprocessor and Real-Time Scheduling

     10.1   Multiprocessor Scheduling
            Granularity
            Design Issues
            Process Scheduling
            Thread Scheduling
     10.2   Real-Time Scheduling
            Background
            Characteristics of Real-Time Operating Systems
            Real-Time Scheduling
            Deadline Scheduling
            Rate Monotonic Scheduling
            Priority Inversion
     10.3   Linux Scheduling
     10.4   UNIX SVR4 Scheduling
     10.5   UNIX FreeBSD Scheduling
     10.6   Windows Scheduling
     10.7   Linux Virtual Machine Process Scheduling
     10.8   Summary
     10.9   Recommended Reading
     10.10  Key Terms, Review Questions, and Problems
430                                                         430

         Bear in mind, Sir Henry, one of the phrases in that queer old legend
         which Dr. Mortimer has read to us, and avoid the moor in those hours
         of darkness when the powers of evil are exalted.
                     --THE HOUND OF THE BASKERVILLES, Arthur Conan Doyle
      LEARNING OBJECTIVES
      After studying this chapter, you should be able to:
      ·     Understand the concept of thread granularity.
      ·     Discuss the key design issues in multiprocessor thread scheduling and some
            of the key approaches to scheduling.
      ·     Understand the requirements imposed by real-time scheduling.
      ·     Explain the scheduling methods used in Linux, UNIX SVR4, and Windows 7.
      This chapter continues our survey of process and thread scheduling. We begin with
      an examination of issues raised by the availability of more than one processor. A
      number of design issues are explored. This is followed by a look at the scheduling of
      processes on a multiprocessor system. Then the somewhat different design consid-
      erations for multiprocessor thread scheduling are examined. The second section of
      this chapter covers real-time scheduling. The section begins with a discussion of the
      characteristics of real-time processes and then looks at the nature of the schedul-
      ing process. Two approaches to real-time scheduling, deadline scheduling and rate
      monotonic scheduling, are examined.
10.1  MULTIPROCESSOR SCHEDULING
      When a computer system contains more than a single processor, several new issues
      are introduced into the design of the scheduling function. We begin with a brief
      overview of multiprocessors and then look at the rather different considerations
      when scheduling is done at the process level and at the thread level.
            We can classify multiprocessor systems as follows:
         ·  Loosely coupled or distributed multiprocessor, or cluster: Consists of a col-
            lection of relatively autonomous systems, each processor having its own main
            memory and I/O channels. We address this type of configuration in Chapter 16.
         ·  Functionally specialized processors: An example is an I/O processor. In this
            case, there is a master, general-purpose processor; specialized processors are
            controlled by the master processor and provide services to it. Issues relating to
            I/O processors are addressed in Chapter 11.
         ·  Tightly coupled multiprocessor: Consists of a set of processors that share a
            common main memory and are under the integrated control of an operating
            system.

Table 10.1   Synchronization Granularity and Processes
                                                                                 Synchronization
Grain Size                          Description                                  Interval (Instructions)
Fine         Parallelism inherent in a single instruction stream                            6 20
Medium       Parallel processing or multitasking within a single application     20­200
Coarse       Multiprocessing of concurrent processes in a                        200­2,000
             multiprogramming environment
Very Coarse  Distributed processing across network nodes to form a single        2,000­1M
             computing environment
Independent  Multiple unrelated processes                                        Not applicable
        Our concern in this section is with the last category, and specifically with issues
        relating to scheduling.
        Granularity
        A good way of characterizing multiprocessors and placing them in context with
        other architectures is to consider the synchronization granularity, or frequency of
        synchronization, between processes in a system. We can distinguish five catego-
        ries of parallelism that differ in the degree of granularity. These are summarized in
        Table 10.1, which is adapted from [GEHR87] and [WOOD89].
        INDEPENDENT  PARALLELISM           With independent parallelism, there is no explicit
        synchronization  among      processes.   Each      represents         a  separate,  independent
        application or job. A typical use of this type of parallelism is in a time-sharing system.
        Each user is performing a particular application such as word processing or using a
        spreadsheet. The multiprocessor provides the same service as a multiprogrammed
        uniprocessor. Because more than one processor is available, average response time
        to the users will be less.
             It is possible to achieve a similar performance gain by providing each user with
        a personal computer or workstation. If any files or information are to be shared,
        then the individual systems must be hooked together into a distributed system sup-
        ported by a network. This approach is examined in Chapter 16. On the other hand,
        a single, multiprocessor shared system in many instances is more cost-effective than
        a distributed system, allowing economies of scale in disks and other peripherals.
        COARSE AND VERY COARSE-GRAINED PARALLELISM                            With coarse and very coarse-
        grained parallelism, there is synchronization among processes, but at a very gross
        level. This kind of situation is easily handled as a set of concurrent processes running
        on a multiprogrammed uniprocessor and can be supported on a multiprocessor with
        little or no change to user software.
             A simple example of an application that can exploit the existence of a multi-
        processor is given in [WOOD89]. The authors have developed a program that takes
        a specification of files needing recompilation to rebuild a piece of software and
        determines which of these compiles (usually all of them) can be run simultaneously.

The program then spawns one process for each parallel compile. The authors report
that the speedup on a multiprocessor actually exceeds what would be expected
by simply adding up the number of processors in use, due to synergies in the disk
buffer caches (a topic explored in Chapter 11) and sharing of compiler code, which
is loaded into memory only once.
   In general, any collection of concurrent processes that need to communicate or
synchronize can benefit from the use of a multiprocessor architecture. In the case of
very infrequent interaction among processes, a distributed system can provide good
support. However, if the interaction is somewhat more frequent, then the overhead
of communication across the network may negate some of the potential speedup. In
that case, the multiprocessor organization provides the most effective support.
MEDIUM-GRAINED PARALLELISM        We saw in Chapter 4 that a single application
can be effectively implemented as a collection of threads within a single process.
In this case, the programmer must explicitly specify the potential parallelism of an
application. Typically, there will need to be rather a high degree of coordination
and interaction among the threads of an application, leading to a medium-grain
level of synchronization.
   Whereas independent, very coarse, and coarse-grained parallelism can be sup-
ported on either a multiprogrammed uniprocessor or a multiprocessor with little or
no impact on the scheduling function, we need to reexamine scheduling when deal-
ing with the scheduling of threads. Because the various threads of an application
interact so frequently, scheduling decisions concerning one thread may affect the
performance of the entire application. We return to this issue later in this section.
FINE-GRAINED PARALLELISM          Fine-grained parallelism represents a much more
complex use of parallelism than is found in the use of threads. Although much
work has been done on highly parallel applications, this is so far a specialized and
fragmented area, with many different approaches.
   Chapter 4 provides an example of the use of granularity for the Valve game
software.
Design Issues
Scheduling on a multiprocessor involves three interrelated issues:
·  The assignment of processes to processors
·  The use of multiprogramming on individual processors
·  The actual dispatching of a process
   In looking at these three issues, it is important to keep in mind that the
approach taken will depend, in general, on the degree of granularity of the applica-
tions and on the number of processors available.
ASSIGNMENT OF PROCESSES TO PROCESSORS             If we assume that the architecture
of the multiprocessor is uniform, in the sense that no processor has a particular
physical advantage with respect to access to main memory or to I/O devices, then
the simplest scheduling approach is to treat the processors as a pooled resource and

     assign processes to processors on demand. The question then arises as to whether
     the assignment should be static or dynamic.
     If a process is permanently assigned to one processor from activation until its
     completion, then a dedicated short-term queue is maintained for each processor.
     An advantage of this approach is that there may be less overhead in the scheduling
     function, because the processor assignment is made once and for all. Also, the use
     of dedicated processors allows a strategy known as group or gang scheduling, as
     discussed later.
     A disadvantage of static assignment is that one processor can be idle, with an
     empty queue, while another processor has a backlog. To prevent this situation, a
     common queue can be used. All processes go into one global queue and are sched-
     uled to any available processor. Thus, over the life of a process, the process may
     be executed on different processors at different times. In a tightly coupled shared-
     memory architecture, the context information for all processes will be available to
     all processors, and therefore the cost of scheduling a process will be independent of
     the identity of the processor on which it is scheduled. Yet another option is dynamic
     load balancing, in which threads are moved for a queue for one processor to a queue
     for another processor; Linux uses this approach.
     Regardless of whether processes are dedicated to processors, some means is
     needed to assign processes to processors. Two approaches have been used: master/
     slave and peer. With a master/slave architecture, key kernel functions of the oper-
     ating system always run on a particular processor. The other processors may only
     execute user programs. The master is responsible for scheduling jobs. Once a proc-
     ess is active, if the slave needs service (e.g., an I/O call), it must send a request to the
     master and wait for the service to be performed. This approach is quite simple and
     requires little enhancement to a uniprocessor multiprogramming operating system.
     Conflict resolution is simplified because one processor has control of all memory
     and I/O resources. There are two disadvantages to this approach: (1) A failure of
     the master brings down the whole system, and (2) the master can become a per-
     formance bottleneck.
     In a peer architecture, the kernel can execute on any processor, and each
     processor does self-scheduling from the pool of available processes. This approach
     complicates the operating system. The operating system must ensure that two proc-
     essors do not choose the same process and that the processes are not somehow lost
     from the queue. Techniques must be employed to resolve and synchronize compet-
     ing claims to resources.
     There is, of course, a spectrum of approaches between these two extremes. One
     approach is to provide a subset of processors dedicated to kernel processing instead
     of just one. Another approach is simply to manage the difference between the needs
     of kernel processes and other processes on the basis of priority and execution history.
     THE USE OF MULTIPROGRAMMING ON INDIVIDUAL PROCESSORS  When each process
     is statically assigned to a processor for the duration of its lifetime, a new question
     arises: Should that processor be multiprogrammed? The reader's first reaction may
     be to wonder why the question needs to be asked; it would appear particularly
     wasteful to tie up a processor with a single process when that process may frequently
     be blocked waiting for I/O or because of concurrency/synchronization considerations.

In the traditional multiprocessor, which is dealing with coarse-grained or
independent synchronization granularity (see Table 10.1), it is clear that each individ-
ual processor should be able to switch among a number of processes to achieve high
utilization and therefore better performance. However, for medium-grained applica-
tions running on a multiprocessor with many processors, the situation is less clear.
When many processors are available, it is no longer paramount that every single proc-
essor be busy as much as possible. Rather, we are concerned to provide the best per-
formance, on average, for the applications. An application that consists of a number
of threads may run poorly unless all of its threads are available to run simultaneously.
PROCESS DISPATCHING  The final design issue related to multiprocessor scheduling
is the actual selection of a process to run. We have seen that, on a multiprogrammed
uniprocessor, the use of priorities or of sophisticated scheduling algorithms based
on past usage may improve performance over a simple-minded first-come-first-
served strategy. When we consider multiprocessors, these complexities may be
unnecessary or even counterproductive, and a simpler approach may be more
effective with less overhead. In the case of thread scheduling, new issues come into
play that may be more important than priorities or execution histories. We address
each of these topics in turn.
Process Scheduling
In most traditional multiprocessor systems, processes are not dedicated to proces-
sors. Rather there is a single queue for all processors, or if some sort of priority
scheme is used, there are multiple queues based on priority, all feeding into the
common pool of processors. In any case, we can view the system as being a multi-
server queueing architecture.
Consider the case of a dual-processor system in which each processor of the
dual-processor system has half the processing rate of a processor in the single-processor
system. [SAUE81] reports a queueing analysis that compares FCFS scheduling to round
robin and to shortest remaining time. The study is concerned with process service time,
which measures the amount of processor time a process needs, either for a total job or
the amount of time needed each time the process is ready to use the processor. In the
case of round robin, it is assumed that the time quantum is large compared to context-
switching overhead and small compared to mean service time. The results depend on
the variability that is seen in service times. A common measure of variability is the
coefficient of variation, Cs.1 A value of Cs  0 corresponds to the case where there is
no variability: the service times of all processes are equal. Increasing values of Cs corre-
spond to increasing variability among the service times. That is, the larger the value of
Cs, the more widely do the values of the service times vary. Values of Cs of 5 or more
are not unusual for processor service time distributions.
Figure 10.1a compares round-robin throughput to FCFS throughput as a func-
tion of Css. Note that the difference in scheduling algorithms is much smaller in the
dual-processor case. With two processors, a single process with long service time is
1The value of Cs is calculated as s/Ts, where s is the standard deviation of service time and Ts is the
mean service time. For a further explanation of Cs, see the discussion in Chapter 20.

                                   1.15
                                                                                             Single
     RR to FCFS throughput ratio                                                             processor
                                   1.10
                                   1.05                                                      Dual
                                                                                             processor
                                   1.00
                                   0.98
                                         0  1  2                3              4          5
                                               Coefficient of variation
                                               (a) Comparison of RR and FCFS
                                   1.25
                                                                                             Single
                                   1.20                                                      processor
     SRT to FCFS throughput ratio  1.15
                                   1.10
                                                                                             Dual
                                   1.05                                                      processor
                                   1.00  0  1  2                3              4          5
                                               Coefficient of variation
                                               (b) Comparison of SRT and FCFS
     Figure 10.1                            Comparison of Scheduling Performance for One
                                            and Two Processors
     much less disruptive in the FCFS case; other processes can use the other processor.
     Similar results are shown in Figure 10.1b.
     The study in [SAUE81] repeated this analysis under a number of assumptions
     about degree of multiprogramming, mix of I/O-bound versus CPU-bound proc-
     esses, and the use of priorities. The general conclusion is that the specific scheduling
     discipline is much less important with two processors than with one. It should be
     evident that this conclusion is even stronger as the number of processors increases.
     Thus, a simple FCFS discipline or the use of FCFS within a static priority scheme
     may suffice for a multiple-processor system.

Thread Scheduling
As we have seen, with threads, the concept of execution is separated from the rest
of the definition of a process. An application can be implemented as a set of threads
that cooperate and execute concurrently in the same address space.
   On a uniprocessor, threads can be used as a program structuring aid and to
overlap I/O with processing. Because of the minimal penalty in doing a thread
switch compared to a process switch, these benefits are realized with little cost.
However, the full power of threads becomes evident in a multiprocessor system. In
this environment, threads can be used to exploit true parallelism in an application.
If the various threads of an application are simultaneously run on separate proces-
sors, dramatic gains in performance are possible. However, it can be shown that
for applications that require significant interaction among threads (medium-grain
parallelism), small differences in thread management and scheduling can have a
significant performance impact [ANDE89].
   Among the many proposals for multiprocessor thread scheduling and proces-
sor assignment, four general approaches stand out:
·  Load sharing: Processes are not assigned to a particular processor. A global
   queue of ready threads is maintained, and each processor, when idle, selects a
   thread from the queue. The term load sharing is used to distinguish this strat-
   egy from load-balancing schemes in which work is allocated on a more perma-
   nent basis (e.g., see [FEIT90a]).2
·  Gang scheduling: A set of related threads is scheduled to run on a set of proc-
   essors at the same time, on a one-to-one basis.
·  Dedicated processor assignment: This is the opposite of the load-sharing
   approach and provides implicit scheduling defined by the assignment of
   threads to processors. Each program, for the duration of its execution, is allo-
   cated a number of processors equal to the number of threads in the program.
   When the program terminates, the processors return to the general pool for
   possible allocation to another program.
·  Dynamic scheduling: The number of threads in a process can be altered during
   the course of execution.
LOAD SHARING  Load sharing is perhaps the simplest approach and the one that
carries over most directly from a uniprocessor environment. It has several advantages:
·  The load is distributed evenly across the processors, assuring that no processor
   is idle while work is available to do.
·  No centralized scheduler is required; when a processor is available, the
   scheduling routine of the operating system is run on that processor to select
   the next thread.
2Some of the literature on this topic refers to this approach as self-scheduling, because each processor
schedules itself without regard to other processors. However, this term is also used in the literature to
refer to programs written in a language that allows the programmer to specify the scheduling (e.g., see
[FOST91]).

     ·  The global queue can be organized and accessed using any of the schemes dis-
        cussed in Chapter 9, including priority-based schemes and schemes that con-
        sider execution history or anticipated processing demands.
        [LEUT90] analyzes three different versions of load sharing:
     ·  First-come-first-served (FCFS): When a job arrives, each of its threads is
        placed consecutively at the end of the shared queue. When a processor be-
        comes idle, it picks the next ready thread, which it executes until completion
        or blocking.
     ·  Smallest number of threads first: The shared ready queue is organized as a pri-
        ority queue, with highest priority given to threads from jobs with the smallest
        number of unscheduled threads. Jobs of equal priority are ordered according
        to which job arrives first. As with FCFS, a scheduled thread is run to comple-
        tion or blocking.
     ·  Preemptive smallest number of threads first: Highest priority is given to jobs
        with the smallest number of unscheduled threads. An arriving job with a
        smaller number of threads than an executing job will preempt threads belong-
        ing to the scheduled job.
     Using simulation models, the authors report that, over a wide range of job charac-
     teristics, FCFS is superior to the other two policies in the preceding list. Further, the
     authors find that some form of gang scheduling, discussed in the next subsection, is
     generally superior to load sharing.
        There are several disadvantages of load sharing:
     ·  The central queue occupies a region of memory that must be accessed in a
        manner that enforces mutual exclusion. Thus, it may become a bottleneck if
        many processors look for work at the same time. When there is only a small
        number of processors, this is unlikely to be a noticeable problem. However,
        when the multiprocessor consists of dozens or even hundreds of processors,
        the potential for bottleneck is real.
     ·  Preempted threads are unlikely to resume execution on the same processor. If
        each processor is equipped with a local cache, caching becomes less efficient.
     ·  If all threads are treated as a common pool of threads, it is unlikely that all of
        the threads of a program will gain access to processors at the same time. If a
        high degree of coordination is required between the threads of a program, the
        process switches involved may seriously compromise performance.
        Despite the potential disadvantages, load sharing is one of the most commonly
     used schemes in current multiprocessors.
        A refinement of the load-sharing technique is used in the Mach operating
     system [BLAC90, WEND89]. The operating system maintains a local run queue
     for each processor and a shared global run queue. The local run queue is used by
     threads that have been temporarily bound to a specific processor. A processor
     examines the local run queue first to give bound threads absolute preference over
     unbound threads. As an example of the use of bound threads, one or more proces-
     sors could be dedicated to running processes that are part of the operating system.

Another example is that the threads of a particular application could be distributed
among a number of processors; with the proper additional software, this provides
support for gang scheduling, discussed next.
GANG SCHEDULING  The concept of scheduling a set of processes simultaneously
on a set of processors predates the use of threads. [JONE80] refers to the concept as
group scheduling and cites the following benefits:
·  If closely related processes execute in parallel, synchronization blocking may
   be reduced, less process switching may be necessary, and performance will
   increase.
· Scheduling overhead may be reduced because a single decision affects a
   number of processors and processes at one time.
   On the Cm* multiprocessor, the term coscheduling is used [GEHR87].
Coscheduling is based on the concept of scheduling a related set of tasks, called a
task force. The individual elements of a task force tend to be quite small and are
hence close to the idea of a thread.
   The term gang scheduling has been applied to the simultaneous scheduling of
the threads that make up a single process [FEIT90b]. Gang scheduling is useful for
medium-grained to fine-grained parallel applications whose performance severely
degrades when any part of the application is not running while other parts are ready
to run. It is also beneficial for any parallel application, even one that is not quite
so performance sensitive. The need for gang scheduling is widely recognized, and
implementations exist on a variety of multiprocessor operating systems.
   One obvious way in which gang scheduling improves the performance of a
single application is that process switches are minimized. Suppose one thread of a
process is executing and reaches a point at which it must synchronize with another
thread of the same process. If that other thread is not running, but is in a ready
queue, the first thread is hung up until a process switch can be done on some other
processor to bring in the needed thread. In an application with tight coordination
among threads, such switches will dramatically reduce performance. The simultane-
ous scheduling of cooperating threads can also save time in resource allocation. For
example, multiple gang-scheduled threads can access a file without the additional
overhead of locking during a seek, read/write operation.
   The use of gang scheduling creates a requirement for processor allocation.
One possibility is the following. Suppose that we have N processors and M applica-
tions, each of which has N or fewer threads. Then each application could be given
1/M of the available time on the N processors, using time slicing. [FEIT90a] notes
that this strategy can be inefficient. Consider an example in which there are two
applications, one with four threads and one with one thread. Using uniform time
allocation wastes 37.5% of the processing resource, because when the single-thread
application runs, three processors are left idle (see Figure 10.2). If there are several
one-thread applications, these could all be fit together to increase processor utiliza-
tion. If that option is not available, an alternative to uniform scheduling is schedul-
ing that is weighted by the number of threads. Thus, the four-thread application
could be given four-fifths of the time and the one-thread application given only one-
fifth of the time, reducing the processor waste to 15%.

                  Uniform division                                Division by weights
                  Group 1    Group 2                              Group 1           Group    2
           PE1                                          PE1
           PE2                      Idle                PE2                            Idle
           PE3                      Idle                PE3                            Idle
           PE4                      Idle                PE4                            Idle
     Time         1/2               1/2                           4/5                  1/5
                       37.5% Waste                                15% Waste
     Figure 10.2  Example of Scheduling   Groups  with  Four and  One Threads [FEIT90b]
     DEDICATED    PROCESSOR  ASSIGNMENT   An extreme form of gang scheduling,
     suggested in [TUCK89], is to dedicate a group of processors to an application for
     the duration of the application. That is, when an application is scheduled, each of
     its threads is assigned a processor that remains dedicated to that thread until the
     application runs to completion.
         This approach would appear to be extremely wasteful of processor time. If
     a thread of an application is blocked waiting for I/O or for synchronization with
     another thread, then that thread's processor remains idle: there is no multiprogram-
     ming of processors. Two observations can be made in defense of this strategy:
     1.  In a highly parallel system, with tens or hundreds of processors, each of which
         represents a small fraction of the cost of the system, processor utilization is no
         longer so important as a metric for effectiveness or performance.
     2.  The total avoidance of process switching during the lifetime of a program
         should result in a substantial speedup of that program.
         Both [TUCK89] and [ZAHO90] report analyses that support statement 2.
     Figure 10.3 shows the results of one experiment [TUCK89]. The authors ran two
     applications simultaneously (executing concurrently), a matrix multiplication and a
     fast Fourier transform (FFT) calculation, on a system with 16 processors. Each appli-
     cation breaks its problem into a number of tasks, which are mapped onto the threads
     executing that application. The programs are written in such a way as to allow the
     number of threads to be used to vary. In essence, a number of tasks are defined
     and queued by an application. Tasks are taken from the queue and mapped onto
     the available threads by the application. If there are fewer threads than tasks, then
     leftover tasks remain queued and are picked up by threads as they complete their
     assigned tasks. Clearly, not all applications can be structured in this way, but many
     numerical problems and some other applications can be dealt with in this fashion.
         Figure 10.3 shows the speedup for the applications as the number of threads
     executing the tasks in each application is varied from 1 to 24. For example, we see
     that when both applications are started simultaneously with 24 threads each, the
     speedup obtained, compared to using a single thread for each application, is 2.8 for
     matrix multiplication and 2.4 for FFT. The figure shows that the performance of
     both applications worsens considerably when the number of threads in each applica-
     tion exceeds eight and thus the total number of processes in the system exceeds the
     number of processors. Furthermore, the larger the number of threads, the worse the

         7
         6                                                     Matrix multiplication
                                                               FFT
         5
Speedup  4
         3
         2
         1
         0  0  4  8  12                              16             20                        24
                  Number of threads per application
Figure 10.3    Application Speedup as a Function of Number of Threads [TUCK89]
performance gets, because there is a greater frequency of thread preemption and
rescheduling. This excessive preemption results in inefficiency from many sources,
including time spent waiting for a suspended thread to leave a critical section, time
wasted in process switching, and inefficient cache behavior.
            The authors conclude that an effective strategy is to limit the number of active
threads to the number of processors in the system. If most of the applications are
either single thread or can use the task-queue structure, this will provide an effec-
tive and reasonably efficient use of the processor resources.
            Both dedicated processor assignment and gang scheduling attack the schedul-
ing problem by addressing the issue of processor allocation. One can observe that
the processor allocation problem on a multiprocessor more closely resembles the
memory allocation problem on a uniprocessor than the scheduling problem on a
uniprocessor. The issue is how many processors to assign to a program at any given
time, which is analogous to how many page frames to assign to a given process at
any time. [GEHR87] proposes the term activity working set, analogous to a virtual
memory working set, as the minimum number of activities (threads) that must be
scheduled simultaneously on processors for the application to make acceptable
progress. As with memory management schemes, the failure to schedule all of the
elements of an activity working set can lead to processor thrashing. This occurs when
the scheduling of threads whose services are required induces the descheduling of
other threads whose services will soon be needed. Similarly, processor fragmenta-
tion refers to a situation in which some processors are left over when others are
allocated, and the leftover processors are either insufficient in number or unsuitably
organized to support the requirements of waiting applications. Gang scheduling and
dedicated processor allocation are meant to avoid these problems.

     DYNAMIC SCHEDULING  For some applications, it is possible to provide language
     and system tools that permit the number of threads in the process to be altered
     dynamically. This would allow the operating system to adjust the load to improve
     utilization.
         [ZAHO90] proposes an approach in which both the operating system and the
     application are involved in making scheduling decisions. The operating system is
     responsible for partitioning the processors among the jobs. Each job uses the proc-
     essors currently in its partition to execute some subset of its runnable tasks by map-
     ping these tasks to threads. An appropriate decision about which subset to run, as
     well as which thread to suspend when a process is preempted, is left to the individ-
     ual applications (perhaps through a set of run-time library routines). This approach
     may not be suitable for all applications. However, some applications could default
     to a single thread while others could be programmed to take advantage of this par-
     ticular feature of the operating system.
         In this approach, the scheduling responsibility of the operating system is pri-
     marily limited to processor allocation and proceeds according to the following pol-
     icy. When a job requests one or more processors (either when the job arrives for the
     first time or because its requirements change),
     1.  If there are idle processors, use them to satisfy the request.
     2.  Otherwise, if the job making the request is a new arrival, allocate it a single
         processor by taking one away from any job currently allocated more than one
         processor.
     3.  If any portion of the request cannot be satisfied, it remains outstanding until
         either a processor becomes available for it or the job rescinds the request (e.g.,
         if there is no longer a need for the extra processors).
     Upon release of one or more processors (including job departure),
     4.  Scan the current queue of unsatisfied requests for processors. Assign a single
         processor to each job in the list that currently has no processors (i.e., to all
         waiting new arrivals). Then scan the list again, allocating the rest of the proc-
         essors on an FCFS basis.
         Analyses reported in [ZAHO90] and [MAJU88] suggest that for applica-
     tions that can take advantage of dynamic scheduling, this approach is superior to
     gang scheduling or dedicated processor assignment. However, the overhead of this
     approach may negate this apparent performance advantage. Experience with actual
     systems is needed to prove the worth of dynamic scheduling.
10.2 REAL-TIME SCHEDULING
     Background
     Real-time computing is becoming an increasingly important discipline. The operat-
     ing system, and in particular the scheduler, is perhaps the most important compo-
     nent of a real-time system. Examples of current applications of real-time systems

include control of laboratory experiments, process control in industrial plants,
robotics, air traffic control, telecommunications, and military command and con-
trol systems. Next-generation systems will include the autonomous land rover,
controllers of robots with elastic joints, systems found in intelligent manufacturing,
the space station, and undersea exploration.
   Real-time computing may be defined as that type of computing in which the
correctness of the system depends not only on the logical result of the computation
but also on the time at which the results are produced. We can define a real-time
system by defining what is meant by a real-time process, or task.3 In general, in a
real-time system, some of the tasks are real-time tasks, and these have a certain
degree of urgency to them. Such tasks are attempting to control or react to events
that take place in the outside world. Because these events occur in "real time," a
real-time task must be able to keep up with the events with which it is concerned.
Thus, it is usually possible to associate a deadline with a particular task, where
the deadline specifies either a start time or a completion time. Such a task may be
classified as hard or soft. A hard real-time task is one that must meet its deadline;
otherwise it will cause unacceptable damage or a fatal error to the system. A soft
real-time task has an associated deadline that is desirable but not mandatory; it still
makes sense to schedule and complete the task even if it has passed its deadline.
   Another characteristic of real-time tasks is whether they are periodic or aperi-
odic. An aperiodic task has a deadline by which it must finish or start, or it may have
a constraint on both start and finish time. In the case of a periodic task, the require-
ment may be stated as "once per period T" or "exactly T units apart."
Characteristics of Real-Time Operating Systems
Real-time operating systems can be characterized as having unique requirements in
five general areas [MORG92]:
·  Determinism
·  Responsiveness
·  User control
·  Reliability
·  Fail-soft operation
   An operating system is deterministic to the extent that it performs operations
at fixed, predetermined times or within predetermined time intervals. When mul-
tiple processes are competing for resources and processor time, no system will be
fully deterministic. In a real-time operating system, process requests for service are
dictated by external events and timings. The extent to which an operating system
can deterministically satisfy requests depends first on the speed with which it can
3As usual, terminology poses a problem, because various words are used in the literature with varying
meanings. It is common for a particular process to operate under real-time constraints of a repetitive
nature. That is, the process lasts for a long time and, during that time, performs some repetitive function
in response to real-time events. Let us, for this section, refer to an individual function as a task. Thus,
the process can be viewed as progressing through a sequence of tasks. At any given time, the process is
engaged in a single task, and it is the process/task that must be scheduled.

     respond to interrupts and, second, on whether the system has sufficient capacity to
     handle all requests within the required time.
         One useful measure of the ability of an operating system to function deterministi-
     cally is the maximum delay from the arrival of a high-priority device interrupt to when
     servicing begins. In non-real-time operating systems, this delay may be in the range of
     tens to hundreds of milliseconds, while in real-time operating systems that delay may
     have an upper bound of anywhere from a few microseconds to a millisecond.
         A related but distinct characteristic is responsiveness. Determinism is con-
     cerned with how long an operating system delays before acknowledging an interrupt.
     Responsiveness is concerned with how long, after acknowledgment, it takes an oper-
     ating system to service the interrupt. Aspects of responsiveness include the following:
     1.  The amount of time required to initially handle the interrupt and begin execu-
         tion of the interrupt service routine (ISR). If execution of the ISR requires a
         process switch, then the delay will be longer than if the ISR can be executed
         within the context of the current process.
     2.  The amount of time required to perform the ISR. This generally is dependent
         on the hardware platform.
     3.  The effect of interrupt nesting. If an ISR can be interrupted by the arrival of
         another interrupt, then the service will be delayed.
     Determinism and responsiveness together make up the response time to external
     events. Response time requirements are critical for real-time systems, because such
     systems must meet timing requirements imposed by individuals, devices, and data
     flows external to the system.
         User control is generally much broader in a real-time operating system than
     in ordinary operating systems. In a typical non-real-time operating system, the user
     either has no control over the scheduling function of the operating system or can
     only provide broad guidance, such as grouping users into more than one priority
     class. In a real-time system, however, it is essential to allow the user fine-grained
     control over task priority. The user should be able to distinguish between hard and
     soft tasks and to specify relative priorities within each class. A real-time system may
     also allow the user to specify such characteristics as the use of paging or process
     swapping, what processes must always be resident in main memory, what disk trans-
     fer algorithms are to be used, what rights the processes in various priority bands
     have, and so on.
         Reliability is typically far more important for real-time systems than non-real-
     time systems. A transient failure in a non-real-time system may be solved by simply
     rebooting the system. A processor failure in a multiprocessor non-real-time system
     may result in a reduced level of service until the failed processor is repaired or
     replaced. But a real-time system is responding to and controlling events in real time.
     Loss or degradation of performance may have catastrophic consequences, ranging
     from financial loss to major equipment damage and even loss of life.
         As in other areas, the difference between a real-time and a non-real-time oper-
     ating system is one of degree. Even a real-time system must be designed to respond
     to various failure modes. Fail-soft operation is a characteristic that refers to the
     ability of a system to fail in such a way as to preserve as much capability and data as

possible. For example, a typical traditional UNIX system, when it detects a corrup-
tion of data within the kernel, issues a failure message on the system console, dumps
the memory contents to disk for later failure analysis, and terminates execution of
the system. In contrast, a real-time system will attempt either to correct the problem
or minimize its effects while continuing to run. Typically, the system notifies a user
or user process that it should attempt corrective action and then continues opera-
tion perhaps at a reduced level of service. In the event a shutdown is necessary, an
attempt is made to maintain file and data consistency.
   An important aspect of fail-soft operation is referred to as stability. A real-
time system is stable if, in cases where it is impossible to meet all task deadlines,
the system will meet the deadlines of its most critical, highest-priority tasks, even if
some less critical task deadlines are not always met.
   To meet the foregoing requirements, real-time operating systems typically
include the following features [STAN89]:
·  Fast process or thread switch
·  Small size (with its associated minimal functionality)
·  Ability to respond to external interrupts quickly
·  Multitasking with interprocess communication tools such as semaphores,
   signals, and events
·  Use of special sequential files that can accumulate data at a fast rate
·  Preemptive scheduling based on priority
·  Minimization of intervals during which interrupts are disabled
·  Primitives to delay tasks for a fixed amount of time and to pause/resume tasks
·  Special alarms and timeouts
   The heart of a real-time system is the short-term task scheduler. In design-
ing such a scheduler, fairness and minimizing average response time are not para-
mount. What is important is that all hard real-time tasks complete (or start) by their
deadline and that as many as possible soft real-time tasks also complete (or start)
by their deadline.
   Most contemporary real-time operating systems are unable to deal directly
with deadlines. Instead, they are designed to be as responsive as possible to real-time
tasks so that, when a deadline approaches, a task can be quickly scheduled. From
this point of view, real-time applications typically require deterministic response
times in the several-millisecond to submillisecond span under a broad set of condi-
tions; leading-edge applications--in simulators for military aircraft, for example--
often have constraints in the range of 10­100 s [ATLA89].
   Figure 10.4 illustrates a spectrum of possibilities. In a preemptive scheduler that
uses simple round-robin scheduling, a real-time task would be added to the ready
queue to await its next timeslice, as illustrated in Figure 10.4a. In this case, the sched-
uling time will generally be unacceptable for real-time applications. Alternatively,
in a nonpreemptive scheduler, we could use a priority scheduling mechanism, giv-
ing real-time tasks higher priority. In this case, a real-time task that is ready would
be scheduled as soon as the current process blocks or runs to completion (Figure
10.4b). This could lead to a delay of several seconds if a slow, low-priority task were

                 Request from a
                 real-time process         Real-time process added to
                                    run queue to await its next slice
                 Process 1          Process 2                         Process n         Real-time
                                                                                        process
     Clock
     tick                                  Scheduling time
                                    (a) Round-robin preemptive scheduler
                        Request from a
                        real-time process      Real-time process added
                                                     to head of run queue
                                    Current process                   Real-time
                                                                       process
                                                                       Current process
                                    Scheduling time                    blocked or completed
                                    (b) Priority-driven nonpreemptive scheduler
                                    Request from a
                                    real-time process         Wait for next
                                                       preemption point
                                    Current process                   Real-time
                                                                       process
     Preemption                            Scheduling time
                 point
                        (c) Priority-driven preemptive scheduler on preemption points
                                           Request from a
                                           real-time process
                                                              Real-time process preempts current
                                                              process and executes immediately
                                    Current process                   Real-time
                                                                       process
                                                     Scheduling time
                                        (d) Immediate preemptive scheduler
     Figure 10.4        Scheduling of Real-Time Process

executing at a critical time. Again, this approach is not acceptable. A more promis-
ing approach is to combine priorities with clock-based interrupts. Preemption points
occur at regular intervals. When a preemption point occurs, the currently running
task is preempted if a higher-priority task is waiting. This would include the preemp-
tion of tasks that are part of the operating system kernel. Such a delay may be on the
order of several milliseconds (Figure 10.4c). While this last approach may be ade-
quate for some real-time applications, it will not suffice for more demanding applica-
tions. In those cases, the approach that has been taken is sometimes referred to as
immediate preemption. In this case, the operating system responds to an interrupt
almost immediately, unless the system is in a critical-code lockout section. Scheduling
delays for a real-time task can then be reduced to 100 s or less.
Real-Time Scheduling
Real-time scheduling is one of the most active areas of research in computer science.
In this subsection, we provide an overview of the various approaches to real-time
scheduling and look at two popular classes of scheduling algorithms.
   In a survey of real-time scheduling algorithms, [RAMA94] observes that the
various scheduling approaches depend on (1) whether a system performs sched-
ulability analysis, (2) if it does, whether it is done statically or dynamically, and
(3) whether the result of the analysis itself produces a schedule or plan according to
which tasks are dispatched at run time. Based on these considerations, the authors
identify the following classes of algorithms:
·  Static table-driven approaches: These perform a static analysis of feasible
   schedules of dispatching. The result of the analysis is a schedule that deter-
   mines, at run time, when a task must begin execution.
·  Static priority-driven preemptive approaches: Again, a static analysis is per-
   formed, but no schedule is drawn up. Rather, the analysis is used to assign
   priorities to tasks, so that a traditional priority-driven preemptive scheduler
   can be used.
·  Dynamic planning-based approaches: Feasibility is determined at run time
   (dynamically) rather than offline prior to the start of execution (statically).
   An arriving task is accepted for execution only if it is feasible to meet its time
   constraints. One of the results of the feasibility analysis is a schedule or plan
   that is used to decide when to dispatch this task.
·  Dynamic best effort approaches: No feasibility analysis is performed. The sys-
   tem tries to meet all deadlines and aborts any started process whose deadline
   is missed.
   Static table-driven scheduling is applicable to tasks that are periodic. Input
to the analysis consists of the periodic arrival time, execution time, periodic end-
ing deadline, and relative priority of each task. The scheduler attempts to develop
a schedule that enables it to meet the requirements of all periodic tasks. This is
a predictable approach but one that is inflexible, because any change to any task
requirements requires that the schedule be redone. Earliest-deadline-first or other
periodic deadline techniques (discussed subsequently) are typical of this category of
scheduling algorithms.

        Static priority-driven preemptive scheduling makes use of the priority-driven
     preemptive scheduling mechanism common to most non-real-time multiprogram-
     ming systems. In a non-real-time system, a variety of factors might be used to
     determine priority. For example, in a time-sharing system, the priority of a process
     changes depending on whether it is processor bound or I/O bound. In a real-time
     system, priority assignment is related to the time constraints associated with each
     task. One example of this approach is the rate monotonic algorithm (discussed
     subsequently), which assigns static priorities to tasks based on the length of their
     periods.
        With dynamic planning-based scheduling, after a task arrives, but before its
     execution begins, an attempt is made to create a schedule that contains the previ-
     ously scheduled tasks as well as the new arrival. If the new arrival can be scheduled
     in such a way that its deadlines are satisfied and that no currently scheduled task
     misses a deadline, then the schedule is revised to accommodate the new task.
        Dynamic best effort scheduling is the approach used by many real-time sys-
     tems that are currently commercially available. When a task arrives, the system
     assigns a priority based on the characteristics of the task. Some form of deadline
     scheduling, such as earliest-deadline scheduling, is typically used. Typically, the
     tasks are aperiodic and so no static scheduling analysis is possible. With this type
     of scheduling, until a deadline arrives or until the task completes, we do not know
     whether a timing constraint will be met. This is the major disadvantage of this form
     of scheduling. Its advantage is that it is easy to implement.
     Deadline Scheduling
     Most contemporary real-time operating systems are designed with the objective of
     starting real-time tasks as rapidly as possible, and hence emphasize rapid interrupt
     handling and task dispatching. In fact, this is not a particularly useful metric in eval-
     uating real-time operating systems. Real-time applications are generally not con-
     cerned with sheer speed but rather with completing (or starting) tasks at the most
     valuable times, neither too early nor too late, despite dynamic resource demands
     and conflicts, processing overloads, and hardware or software faults. It follows that
     priorities provide a crude tool and do not capture the requirement of completion
     (or initiation) at the most valuable time.
        There have been a number of proposals for more powerful and appropriate
     approaches to real-time task scheduling. All of these are based on having additional
     information about each task. In its most general form, the following information
     about each task might be used:
     ·  Ready time: Time at which task becomes ready for execution. In the case of a
        repetitive or periodic task, this is actually a sequence of times that is known in
        advance. In the case of an aperiodic task, this time may be known in advance,
        or the operating system may only be aware when the task is actually ready.
     ·  Starting deadline: Time by which a task must begin.
     ·  Completion deadline: Time by which a task must be completed. The typical
        real-time application will either have starting deadlines or completion dead-
        lines, but not both.

·  Processing time: Time required to execute the task to completion. In some
   cases, this is supplied. In others, the operating system measures an exponen-
   tial average (as defined in Chapter 9). For still other scheduling systems, this
   information is not used.
·  Resource requirements: Set of resources (other than the processor) required
   by the task while it is executing.
·  Priority: Measures relative importance of the task. Hard real-time tasks may
   have an "absolute" priority, with the system failing if a deadline is missed. If
   the system is to continue to run no matter what, then both hard and soft real-
   time tasks may be assigned relative priorities as a guide to the scheduler.
·  Subtask structure: A task may be decomposed into a mandatory subtask and
   an optional subtask. Only the mandatory subtask possesses a hard deadline.
   There are several dimensions to the real-time scheduling function when dead-
lines are taken into account: which task to schedule next, and what sort of preemp-
tion is allowed. It can be shown, for a given preemption strategy and using either
starting or completion deadlines, that a policy of scheduling the task with the ear-
liest deadline minimizes the fraction of tasks that miss their deadlines [BUTT99,
HONG89, PANW88]. This conclusion holds for both single-processor and multi-
processor configurations.
   The other critical design issue is that of preemption. When starting deadlines
are specified, then a nonpreemptive scheduler makes sense. In this case, it would be
the responsibility of the real-time task to block itself after completing the manda-
tory or critical portion of its execution, allowing other real-time starting deadlines
to be satisfied. This fits the pattern of Figure 10.4b. For a system with completion
deadlines, a preemptive strategy (Figure 10.4c or 10.4d) is most appropriate. For
example, if task X is running and task Y is ready, there may be circumstances in
which the only way to allow both X and Y to meet their completion deadlines is to
preempt X, execute Y to completion, and then resume X to completion.
   As an example of scheduling periodic tasks with completion deadlines, consider
a system that collects and processes data from two sensors, A and B. The deadline for
collecting data from sensor A must be met every 20 ms, and that for B every 50 ms.
It takes 10 ms, including operating system overhead, to process each sample of data
from A and 25 ms to process each sample of data from B. Table 10.2 summarizes the
execution profile of the two tasks. Figure 10.5 compares three scheduling techniques
using the execution profile of Table 10.2. The first row of Figure 10.5 repeats the infor-
mation in Table 10.2; the remaining three rows illustrate three scheduling techniques.
   The computer is capable of making a scheduling decision every 10 ms.4 Suppose
that, under these circumstances, we attempted to use a priority scheduling scheme.
The first two timing diagrams in Figure 10.5 show the result. If A has higher prior-
ity, the first instance of task B is given only 20 ms of processing time, in two 10-ms
chunks, by the time its deadline is reached, and thus fails. If B is given higher priority,
then A will miss its first deadline. The final timing diagram shows the use of earli-
est-deadline scheduling. At time t = 0, both A1 and B1 arrive. Because A1 has the
4This need not be on a 10-ms boundary if more than 10 ms has elapsed since the last scheduling decision.

     Table  10.2     Execution     Profile of Two Periodic            Tasks
            Process                       Arrival Time                    Execution       Time            Ending Deadline
            A(1)                                  0                               10                                   20
            A(2)                                  20                              10                                   40
            A(3)                                  40                              10                                   60
            A(4)                                  60                              10                                   80
            A(5)                                  80                              10                                   100
                  ·                               ·                               ·                                    ·
                  ·                               ·                               ·                                    ·
                  ·                               ·                               ·                                    ·
            B(1)                                  0                               25                                   50
            B(2)                                  50                              25                                   100
                  ·                               ·                               ·                                    ·
                  ·                               ·                               ·                                    ·
                  ·                               ·                               ·                                    ·
                                                                          B1                                      B2
                                                                      deadline                                deadline
                                                  A1              A2              A3              A4              A5
                                              deadline        deadline        deadline        deadline        deadline
     Arrival times, execution         A1              A2              A3              A4              A5
     times, and deadlines                 B1                                      B2
                                   0      10      20      30      40      50      60      70      80      90      100       Time (ms)
     Fixed-priority scheduling;       A1      B1      A2      B1      A3      B2      A4      B2      A5  B2
     A has priority
                                                  A1              A2      B1      A3              A4          A5, B2
                                                                      (missed)
     Fixed-priority scheduling;           B1              A2          A3          B2                  A5
     B has priority
                                                  A1              A2      B1      A3              A4          A5, B2
                                              (missed)                                        (missed)
     Earliest-deadline scheduling     A1      B1      A2      B1          A3  B2      A4          B2          A5
     using completion deadlines
                                                  A1              A2      B1      A3              A4          A5, B2
     Figure 10.5     Scheduling of Periodic Real-Time Tasks with Completion Deadlines
                     (Based on Table 10.2)

                                  0  10  20      30  40      50      60  70         80  90  100  110       120
               Arrival times         A   B           C       D       E
Requirements
               Starting deadline         B                   C           E              D        A
               Arrival times         A   B           C       D       E
Earliest       Service                   A                   C           E              D
deadline
               Starting deadline     B (missed)              C           E              D        A
               Arrival times         A   B           C       D       E
Earliest
deadline       Service                           B           C           E              D        A
with unforced
idle times     Starting deadline         B                   C           E              D        A
               Arrival times         A   B           C       D       E
First-come                               A                   C           D
first-served   Service
(FCFS)
               Starting deadline     B (missed)              C          E (missed)      D        A
Figure 10.6    Scheduling of Aperiodic Real-Time     Tasks   with  Starting Deadlines
earliest deadline, it is scheduled first. When A1 completes, B1 is given the processor.
At t = 20, A2 arrives. Because A2 has an earlier deadline than B1, B1 is interrupted
so that A2 can execute to completion. Then B1 is resumed at t = 30. At t = 40, A3
arrives. However, B1 has an earlier ending deadline and is allowed to execute to
completion at t = 45. A3 is then given the processor and finishes at t = 55.
        In this example, by scheduling to give priority at any preemption point to the
task with the nearest deadline, all system requirements can be met. Because the
tasks are periodic and predictable, a static table-driven scheduling approach is used.
        Now consider a scheme for dealing with aperiodic tasks with starting dead-
lines. The top part of Figure 10.6 shows the arrival times and starting deadlines for
an example consisting of five tasks each of which has an execution time of 20 ms.
Table 10.3 summarizes the execution profile of the five tasks.
Table 10.3        Execution Profile of Five Aperiodic Tasks
              Process             Arrival Time       Execution Time                     Starting Deadline
               A                     10                          20                         110
               B                     20                          20                         20
               C                     40                          20                         50
               D                     50                          20                         90
               E                     60                          20                         70

     A straightforward scheme is to always schedule the ready task with the earli-
     est deadline and let that task run to completion. When this approach is used in the
     example of Figure 10.6, note that although task B requires immediate service, the
     service is denied. This is the risk in dealing with aperiodic tasks, especially with
     starting deadlines. A refinement of the policy will improve performance if deadlines
     can be known in advance of the time that a task is ready. This policy, referred to as
     earliest deadline with unforced idle times, operates as follows: Always schedule the
     eligible task with the earliest deadline and let that task run to completion. An eligi-
     ble task may not be ready, and this may result in the processor remaining idle even
     though there are ready tasks. Note that in our example the system refrains from
     scheduling task A even though that is the only ready task. The result is that, even
     though the processor is not used to maximum efficiency, all scheduling require-
     ments are met. Finally, for comparison, the FCFS policy is shown. In this case, tasks
     B and E do not meet their deadlines.
     Rate Monotonic Scheduling
     One of the more promising methods of resolving multitask scheduling conflicts for
     periodic tasks is rate monotonic scheduling (RMS). The scheme was first proposed
     in [LIU73] but has only recently gained popularity [BRIA99, SHA94]. RMS assigns
     priorities to tasks on the basis of their periods.
     For RMS, the highest-priority task is the one with the shortest period, the
     second highest-priority task is the one with the second shortest period, and so on.
     When more than one task is available for execution, the one with the shortest period
     is serviced first. If we plot the priority of tasks as a function of their rate, the result is
     a monotonically increasing function (Figure 10.7); hence the name "rate monotonic
     scheduling."
     High                                  Highest rate and
                                           highest-priority task
     Priority
                                           Lowest rate and
                                lowest-priority task
     Low                                   Rate (Hz)
     Figure 10.7   A Task Set with RMS [WARR91]

                            Cycle 1                                        Cycle 2
P     Processing                     Idle                Processing
   Task P execution time C                                                                   Time
                  Task P period T
Figure 10.8  Periodic Task Timing Diagram
      Figure 10.8 illustrates the relevant parameters for periodic tasks. The task's
   period, T, is the amount of time between the arrival of one instance of the task
   and the arrival of the next instance of the task. A task's rate (in hertz) is simply
   the inverse of its period (in seconds). For example, a task with a period of 50 ms
   occurs at a rate of 20 Hz. Typically, the end of a task's period is also the task's hard
   deadline, although some tasks may have earlier deadlines. The execution (or com-
   putation) time, C, is the amount of processing time required for each occurrence of
   the task. It should be clear that in a uniprocessor system, the execution time must
   be no greater than the period (must have C ... T). If a periodic task is always run to
   completion, that is, if no instance of the task is ever denied service because of insuf-
   ficient resources, then the utilization of the processor by this task is U = C/T. For
   example, if a task has a period of 80 ms and an execution time of 55 ms, its processor
   utilization is 55/80 = 0.6875.
      One measure of the effectiveness of a periodic scheduling algorithm is whether
   or not it guarantees that all hard deadlines are met. Suppose that we have n tasks,
   each with a fixed period and execution time. Then for it to be possible to meet all
   deadlines, the following inequality must hold:
                                     C1    +   C2  +  g   +  Cn      ...1             (10.1)
                                     T1        T2            Tn
   The sum of the processor utilizations of the individual tasks cannot exceed a value
   of 1, which corresponds to total utilization of the processor. Equation (10.1)
   provides a bound on the number of tasks that a perfect scheduling algorithm can
   successfully schedule. For any particular algorithm, the bound may be lower. For
   RMS, it can be shown that the following inequality holds:
                               C1    +     C2  +  g   +  Cn  ... n(21/n - 1)          (10.2)
                               T1          T2            Tn
   Table 10.4 gives some values for this upper bound. As the number of tasks increases,
   the scheduling bound converges to ln 2  0.693.
      As an example, consider the case of three periodic tasks, where Ui = Ci/Ti:
   ·  Task P1: C1           =  20; T1 = 100; U1 = 0.2
   ·  Task P2: C2           =  40; T2 = 150; U2 = 0.267
   ·  Task P3: C3           =  100; T3 = 350; U3 = 0.286

                     Table 10.4      Value  of  the  RMS  Upper Bound
                                 n                        n(21/n - 1)
                                 1                          1.0
                                 2                        0.828
                                 3                        0.779
                                 4                        0.756
                                 5                        0.743
                                 6                        0.734
                                 ·                          ·
                                 ·                          ·
                                 ·                          ·
                                                          ln 2  0.693
         The total utilization of these three tasks is 0.2 + 0.267 + 0.286 = 0.753. The
     upper bound for the schedulability of these three tasks using RMS is
                     C1  +       C2    +    C3  ... n(21/3 - 1) = 0.779
                     T1          T2         T3
     Because the total utilization required for the three tasks is less than the upper bound
     for RMS (0.753 6 0.779), we know that if RMS is used, all tasks will be successfully
     scheduled.
         It can also be shown that the upper bound of Equation (10.1) holds for
     earliest deadline scheduling. Thus, it is possible to achieve greater overall processor
     utilization and therefore accommodate more periodic tasks with earliest deadline
     scheduling. Nevertheless, RMS has been widely adopted for use in industrial appli-
     cations. [SHA91] offers the following explanation:
     1.  The performance difference is small in practice. The upper bound of Equation
         (10.2) is a conservative one and, in practice, utilization as high as 90% is often
         achieved.
     2.  Most hard real-time systems also have soft real-time components, such as
         certain noncritical displays and built-in self tests that can execute at lower
         priority levels to absorb the processor time that is not used with RMS schedul-
         ing of hard real-time tasks.
     3.  Stability is easier to achieve with RMS. When a system cannot meet all dead-
         lines because of overload or transient errors, the deadlines of essential tasks
         need to be guaranteed provided that this subset of tasks is schedulable. In a
         static priority assignment approach, one only needs to ensure that essential
         tasks have relatively high priorities. This can be done in RMS by structuring
         essential tasks to have short periods or by modifying the RMS priorities to
         account for essential tasks. With earliest deadline scheduling, a periodic task's
         priority changes from one period to another. This makes it more difficult to
         ensure that essential tasks meet their deadlines.

Priority Inversion
Priority inversion is a phenomenon that can occur in any priority-based preemptive
scheduling scheme but is particularly relevant in the context of real-time schedul-
ing. The best-known instance of priority inversion involved the Mars Pathfinder
mission. This rover robot landed on Mars on July 4, 1997 and began gathering and
transmitting voluminous data back to Earth. But a few days into the mission, the
lander software began experiencing total system resets, each resulting in losses of
data. After much effort by the Jet Propulsion Laboratory (JPL) team that built the
Pathfinder, the problem was traced to priority inversion [JONE97].
In any priority scheduling scheme, the system should always be executing the
task with the highest priority. Priority inversion occurs when circumstances within
the system force a higher-priority task to wait for a lower-priority task. A simple
example of priority inversion occurs if a lower-priority task has locked a resource
(such as a device or a binary semaphore) and a higher-priority task attempts to lock
that same resource. The higher-priority task will be put in a blocked state until the
resource is available. If the lower-priority task soon finishes with the resource and
releases it, the higher-priority task may quickly resume and it is possible that no
real-time constraints are violated.
A more serious condition is referred to as an unbounded priority inversion, in
which the duration of a priority inversion depends not only on the time required to
handle a shared resource but also on the unpredictable actions of other unrelated
tasks. The priority inversion experienced in the Pathfinder software was unbounded
and serves as a good example of the phenomenon. Our discussion follows that of
[TIME02]. The Pathfinder software included the following three tasks, in decreas-
ing order of priority:
T1: Periodically checks the health of the spacecraft systems and software
T2: Processes image data
T3: Performs an occasional test on equipment status
After T1 executes, it reinitializes a timer to its maximum value. If this timer
ever expires, it is assumed that the integrity of the lander software has somehow
been compromised. The processor is halted, all devices are reset, the software is
completely reloaded, the spacecraft systems are tested, and the system starts over.
This recovery sequence does not complete until the next day. T1 and T3 share a
common data structure, protected by a binary semaphore s. Figure 10.9a shows the
sequence that caused the priority inversion:
t1: T3 begins executing.
t2: T3 locks semaphore s and enters its critical section.
t3: T1, which has a higher priority than T3, preempts T3 and begins executing.
t4: T1 attempts to enter its critical section but is blocked because the semaphore
is locked by T3; T3 resumes execution in its critical section.
t5: T2, which has a higher priority than T3, preempts T3 and begins executing.
t6: T2 is suspended for some reason unrelated to T1 and T3; T3 resumes.
t7: T3 leaves its critical section and unlocks the semaphore. T1 preempts T3,
locks the semaphore, and enters its critical section.

                             Blocked by T3
                         (attempt to lock s)                              s locked
     T1
     T2
                         Preempted           Preempted                    s unlocked
         s   locked          by T1               by T2
     T3
         t1          t2  t3              t4  t5                           t6        t7  t8
                                                     Time
                             (a) Unbounded priority inversion
                                             s locked
                  Blocked by T3                  by T1
             (attempt to lock s)                         s unlocked
     T1
     T2
         s locked        Preempted           s unlocked
             by T3           by T1
     T3
         t1          t2  t3              t4  t5  t6        t7
                                         (b) Use of priority inheritance
             Normal execution                           Execution in      critical  section
     Figure 10.9     Priority Inversion
     In this set of circumstances, T1 must wait for both T3 and T2 to complete and fails to
     reset the timer before it expires.
     In practical systems, two alternative approaches are used to avoid unbounded
     priority inversion: priority inheritance protocol and priority ceiling protocol.
     The basic idea of priority inheritance is that a lower-priority task inherits
     the priority of any higher-priority task pending on a resource they share. This
     priority change takes place as soon as the higher-priority task blocks on the

      resource; it should end when the resource is released by the lower-priority task.
      Figure 10.9b shows that priority inheritance resolves the problem of unbounded
      priority inversion illustrated in Figure 10.9a. The relevant sequence of events is
      as follows:
         t1: T3 begins executing.
         t2: T3 locks semaphore s and enters its critical section.
         t3: T1, which has a higher priority than T3, preempts T3 and begins executing.
         t4: T1 attempts to enter its critical section but is blocked because the sema-
         phore is locked by T3. T3 is immediately and temporarily assigned the same
         priority as T1. T3 resumes execution in its critical section.
         t5: T2 is ready to execute but, because T3 now has a higher priority, T2 is unable
         to preempt T3.
         t6: T3 leaves its critical section and unlocks the semaphore: its priority level is
         downgraded to its previous default level. T1 preempts T3, locks the sema-
         phore, and enters its critical section.
         t7: T1 is suspended for some reason unrelated to T2, and T2 begins executing.
      This was the approach taken to solving the Pathfinder problem.
         In the priority ceiling approach, a priority is associated with each resource.
      The priority assigned to a resource is one level higher than the priority of its highest-
      priority user. The scheduler then dynamically assigns this priority to any task that
      accesses the resource. Once the task finishes with the resource, its priority returns
      to normal.
10.3  LINUX SCHEDULING
      For Linux 2.4 and earlier, Linux provided a real-time scheduling capability
      coupled with a scheduler for non-real-time processes that made use of the tra-
      ditional UNIX scheduling algorithm described in Section 9.3. Linux 2.6 includes
      essentially the same real-time scheduling capability as previous releases and a sub-
      stantially revised scheduler for non-real-time processes. We examine these two
      areas in turn.
      Real-Time Scheduling
      The three Linux scheduling classes are
      ·  SCHED_FIFO: First-in-first-out real-time threads
      ·  SCHED_RR: Round-robin real-time threads
      ·  SCHED_OTHER: Other, non-real-time threads
         Within each class, multiple priorities may be used, with priorities in the real-time
      classes higher than the priorities for the SCHED_OTHER class. The default values are as
      follows: Real-time priority classes range from 0 to 99 inclusively, and SCHED_OTHER
      classes range from 100 to 139. A lower number equals a higher priority.

         For FIFO threads, the following rules apply:
     1.  The system will not interrupt an executing FIFO thread except in the following
         cases:
         a. Another FIFO thread of higher priority becomes ready.
         b. The executing FIFO thread becomes blocked waiting for an event, such as I/O.
         c. The executing FIFO thread voluntarily gives up the processor following a
         call to the primitive sched_yield.
     2.  When an executing FIFO thread is interrupted, it is placed in the queue asso-
         ciated with its priority.
     3.  When a FIFO thread becomes ready and if that thread has a higher priority than
         the currently executing thread, then the currently executing thread is preempted
         and the highest-priority ready FIFO thread is executed. If more than one thread
         has that highest priority, the thread that has been waiting the longest is chosen.
         The SCHED_RR policy is similar to the SCHED_FIFO policy, except for the
     addition of a timeslice associated with each thread. When a SCHED_RR thread has
     executed for its timeslice, it is suspended and a real-time thread of equal or higher
     priority is selected for running.
         Figure 10.10 is an example that illustrates the distinction between FIFO and RR
     scheduling. Assume a process has four threads with three relative priorities assigned
     as shown in Figure 10.10a. Assume that all waiting threads are ready to execute when
     the current thread waits or terminates and that no higher-priority thread is awakened
     while a thread is executing. Figure 10.10b shows a flow in which all of the threads
     are in the SCHED_FIFO class. Thread D executes until it waits or terminates. Next,
     although threads B and C have the same priority, thread B starts because it has been
     waiting longer than thread C. Thread B executes until it waits or terminates, then
     thread C executes until it waits or terminates. Finally, thread A executes.
         Figure 10.10c shows a sample flow if all of the threads are in the SCHED_RR
     class. Thread D executes until it waits or terminates. Next, threads B and C are time
     sliced, because they both have the same priority. Finally, thread A executes.
         The final scheduling class is SCHED_OTHER. A thread in this class can only
     execute if there are no real-time threads ready to execute.
                    A  Minimum
                    B  Middle
                                                    D     B         C  A
                    C  Middle
                    D  Maximum
                 (a) Relative thread priorities        (b) Flow with FIFO scheduling
                 D     B            C            B     C         A
                                    (c) Flow with RR scheduling
                 Figure 10.10  Example of Linux Real-Time Scheduling

        Non-Real-Time Scheduling
        The Linux 2.4 scheduler for the SCHED_OTHER class did not scale well with increas-
        ing number of processors and increasing number of processes. The drawbacks of
        this scheduler include the following:
          ·  The Linux 2.4 scheduler uses a single runqueue for all processors in a symmet-
             ric multiprocessing system (SMP). This means a task can be scheduled on any
             processor, which can be good for load balancing but bad for memory caches.
             For example, suppose a task executed on CPU-1, and its data were in that pro-
             cessor's cache. If the task got rescheduled to CPU-2, its data would need to be
             invalidated in CPU-1 and brought into CPU-2.
          ·  The Linux 2.4 scheduler uses a single runqueue lock. Thus, in an SMP sys-
             tem, the act of choosing a task to execute locks out any other processor from
             manipulating the runqueues. The result is idle processors awaiting release of
             the runqueue lock and decreased efficiency.
          ·  Preemption is not possible in the Linux 2.4 scheduler; this means that a lower-
             priority task can execute while a higher-priority task waited for it to complete.
             To correct these problems, Linux 2.6 uses a completely new priority scheduler
        known as the O(1) scheduler.5 The scheduler is designed so that the time to select
        the appropriate process and assign it to a processor is constant, regardless of the
        load on the system or the number of processors.
             The kernel maintains two scheduling data structure for each processor in the
        system, of the following form (Figure 10.11):
struct  prio_array  {
int                 nr_active;                          /*  number  of  tasks   in  this  array*/
unsigned  long      bitmap[BITMAP_SIZE];                /*  priority    bitmap  */
struct  list_head   queue[MAX_PRIO];                    /*  priority    queues  */
             A separate queue is maintained for each priority level. The total number of
        queues in the structure is MAX_PRIO, which has a default value of 140. The struc-
        ture also includes a bitmap array of sufficient size to provide one bit per priority
        level. Thus, with 140 priority levels and 32-bit words, BITMAP_SIZE has a value of
        5. This creates a bitmap of 160 bits, of which 20 bits are ignored. The bitmap indi-
        cates which queues are not empty. Finally, nr_active indicates the total number
        of tasks present on all queues. Two structures are maintained: an active queues
        structure and an expired queues structure.
             Initially, both bitmaps are set to all zeroes and all queues are empty. As a
        process becomes ready, it is assigned to the appropriate priority queue in the active
        queues structure and is assigned the appropriate timeslice. If a task is preempted
        before it completes its timeslice, it is returned to an active queue. When a task com-
        pletes its timeslice, it goes into the appropriate queue in the expired queues structure
        and is assigned a new timeslice. All scheduling is done from among tasks in the active
        5The term O(1) is an example of the "big-O" notation, used for characterizing the time complexity of
        algorithms. Appendix I explains this notation.

              Highest-priority
Bit 0         nonempty
(priority 0)  active queue
                                                                      Active queues:
                                                                      140 queues by priority;
                                                                      each queue contains ready
                                                                      tasks for that priority
                                           Bit 139
140-bit priority array for active queues   (priority 139)
                                                                      Expired queues:
                                                                      140 queues by priority;
                                                                      each queue contains ready
                                                                      tasks with expired timeslices
                                                                      for that priority
140-bit priority array for expired queues
Figure 10.11  Linux Scheduling Data Structures for Each Processor
              queues structure. When the active queues structure is empty, a simple pointer assign-
              ment results in a switch of the active and expired queues, and scheduling continues.
              Scheduling is simple and efficient. On a given processor, the scheduler picks
              the highest-priority nonempty queue. If multiple tasks are in that queue, the tasks
              are scheduled in round-robin fashion.
              Linux also includes a mechanism for moving tasks from the queue lists of one
              processor to that of another. Periodically, the scheduler checks to see if there is a sub-
              stantial imbalance among the number of tasks assigned to each processor. To balance
              the load, the schedule can transfer some tasks. The highest-priority active tasks are
              selected for transfer, because it is more important to distribute high-priority tasks fairly.
              CALCULATING PRIORITIES AND TIMESLICES        Each non-real-time task is assigned
              an initial priority in the range of 100­139, with a default of 120. This is the task's
              static priority and is specified by the user. As the task executes, a dynamic priority is
              calculated as a function of the task's static priority and its execution behavior. The
              Linux scheduler is designed to favor I/O-bound tasks over processor-bound tasks.
              This preference tends to provide good interactive response. The technique used by
              Linux to determine the dynamic priority is to keep a running tab on how much time
              a process sleeps (waiting for an event) versus how much time the process runs. In
              essence, a task that spends most of its time sleeping is given a higher priority.

          Timeslices are assigned in the range of 10­200 ms. In general, higher-priority
      tasks are assigned larger timeslices.
      RELATIONSHIP   TO      REAL-TIME       TASKS    Real-time  tasks       are  handled                      in  a
      different manner from non-real-time tasks in the priority queues. The following
      considerations apply:
      1.  All real-time tasks have only a static priority; no dynamic priority changes
          are made.
      2.  SCHED_FIFO tasks do not have assigned timeslices. Such tasks are scheduled
          in FIFO discipline. If a SHED_FIFO task is blocked, it returns to the same pri-
          ority queue in the active queue list when it becomes unblocked.
      3.  Although SCHED_RR tasks do have assigned timeslices, they also are never
          moved to the expired queue list. When a SCHED_RR task exhausts its timeslice,
          it is returned to its priority queue with the same timeslice value. Timeslice val-
          ues are never changed.
          The effect of these rules is that the switch between the active queue list and
      the expired queue list only happens when there are no ready real-time tasks waiting
      to execute.
10.4  UNIX SVR4 SCHEDULING
      The scheduling algorithm used in UNIX SVR4 is a complete overhaul of the sched-
      uling algorithm used in earlier UNIX systems (described in Section 9.3). The new
      algorithm is designed to give highest preference to real-time processes, next-highest
      preference to kernel-mode processes, and lowest preference to other user-mode
      processes, referred to as time-shared processes.6
          The two major modifications implemented in SVR4 are as follows:
      1.  The addition of a preemptable static priority scheduler and the introduction of
          a set of 160 priority levels divided into three priority classes.
      2.  The insertion of preemption points. Because the basic kernel is not preemp-
          tive, it can only be split into processing steps that must run to completion
          without interruption. In between the processing steps, safe places known as
          preemption points have been identified where the kernel can safely interrupt
          processing and schedule a new process. A safe place is defined as a region
          of code where all kernel data structures are either updated and consistent or
          locked via a semaphore.
          Figure 10.12 illustrates the 160 priority levels defined in SVR4. Each process
      is defined to belong to one of three priority classes and is assigned a priority level
      within that class. The classes are as follows:
      · Real time (159-100): Processes at these priority levels are guaranteed to be
          selected to run before any kernel or time-sharing process. In addition, real-time
      6Time-shared processes are the processes that correspond to users in a traditional time-sharing system.

                                Priority      Global     Scheduling
                                class         value      sequence
                                              159        First
                                Real time
                                              100
                                              99
                                Kernel
                                              60
                                              59
                                Time shared
                                              0          Last
                                Figure 10.12  SVR4 Priority Classses
               processes can make use of preemption points to preempt kernel processes and
               user processes.
            ·  Kernel (99-60): Processes at these priority levels are guaranteed to be selected
               to run before any time-sharing process but must defer to real-time processes.
            ·  Time-shared (59-0): The lowest-priority processes, intended for user applica-
               tions other than real-time applications.
               Figure 10.13 indicates how scheduling is implemented in SVR4. A dispatch
     queue is associated with each priority level, and processes at a given priority level
     are executed in round-robin fashion. A bit-map vector, dqactmap, contains one bit
     for each priority level; the bit is set to one for any priority level with a nonempty
     queue. Whenever a running process leaves the Running state, due to a block, times-
     lice expiration, or preemption, the dispatcher checks dqactmap and dispatches a
     ready process from the highest-priority nonempty queue. In addition, whenever a
     defined preemption point is reached, the kernel checks a flag called kprunrun. If
     set, this indicates that at least one real-time process is in the Ready state, and the
     kernel preempts the current process if it is of lower priority than the highest-priority
     real-time ready process.
               Within the time-sharing class, the priority of a process is variable. The scheduler
     reduces the priority of a process each time it uses up a time quantum, and it raises its
dqactmap       0         1                                            1  1  0
     dispq  159          n                                            2  1  0
                         P                                            P  P
                         P                                            P
                         P                                            P
                         P
Figure 10.13      SVR4 Dispatch Queues

      priority if it blocks on an event or resource. The time quantum allocated to a time-
      sharing process depends on its priority, ranging from 100 ms for priority 0 to 10 ms
      for priority 59. Each real-time process has a fixed priority and a fixed time quantum.
10.5  UNIX FREEBSD SCHEDULING
      The UNIX FreeBSD scheduler is designed to provide a more efficient operation
      than previous UNIX schedulers under heavy load and when used on a multiproces-
      sor or multicore platform. The scheduler is quite complex and here we present an
      overview of the most significant design features; for more detail, see [MCKU05]
      and [ROBE03].
      Priority Classes
      The underlying priority mechanism in the FreeBSD 5.1 scheduler is similar to that
      of UNIX SVR4. For FreeBSD, five priority classes are defined (Table 10.5); the
      first two classes are for kernel-mode thread and the remaining classes for user-mode
      threads. Kernel threads execute code that is complied into the kernel's load image
      and operate with the kernel's privileged execution code.
            The highest-priority threads are referred to as bottom-half kernel. Threads in
      this class run in the kernel are scheduled based on interrupt priorities. These pri-
      orities are set when the corresponding devices are configured and do not change.
      Top-half kernel threads also run in the kernel and execute various kernel functions.
      These priorities are set based on predefined priorities and never change.
            The next lower priority class is referred to as real-time user. A thread with a
      real-time priority is not subject to priority degradation. That is, a real-time thread
      maintains the priority it began with and does not drop to a lower priority as a result
      of using resources. Next comes the time-sharing user priority class. For threads in
      this class, priority is periodically recalculated based on a number of parameters,
      including the amount of processor time used, the amount of memory resources
      held, and other resource consumption parameters. The lowest range of priorities is
      referred to as the idle user class. This class is intended for applications that will only
      consume processor time when no other threads are ready to execute.
      Table 10.5  FreeBSD Thread Scheduling Classes
      Priority Class  Thread Type         Description
      0­63            Bottom-half kernel  Scheduled by interrupts. Can block to await a resource.
      64­127          Top-half kernel     Runs until blocked or done. Can block to await a resource.
      128­159         Real-time user      Allowed to run until blocked or until a higher-priority
                                          thread becomes available. Preemptive scheduling.
      160­223         Time-sharing user   Adjusts priorities based on processor usage.
      224­255         Idle user           Only run when there are no time sharing or real-time
                                          threads to run.
      Note: Lower number corresponds to higher priority.

     SMP and Multicore Support
     The latest version of the FreeBSD scheduler, introduced with FreeBSD 5.0, was
     designed to provide effective scheduling for a SMP or multicore system. The new
     scheduler meets three design goals:
     ·   Address the need for processor affinity in SMP and multicore systems. The term
         processor affinity refers to a scheduler that only migrates a thread (moves thread
         from one processor to another) when necessary to avoid having an idle processor.
     ·   Provide better support for multithreading on multicore systems.
     ·   Improve the performance of the scheduling algorithm, so that it is no longer a
         function of the number of threads in the system.
         In this subsection, we look at three key features of the new scheduler: queue
     structure, interactivity scoring, and thread migration.
     QUEUE STRUCTURE        The previous version of the FreeBSD schedule used a single
     global scheduling queue for all processors that it traverses once per second to
     recalculate their priorities. The use of a single list for all threads means that the
     performance of the scheduler is dependent on the number of tasks in the system, and
     as the number of tasks grows, more processor time must be spent in the scheduler
     maintaining the list.
         The new scheduler performs scheduling independently for each processor. For
     each processor, three queues are maintained. Each of the queues has the structure
     shown in Figure 10.14 for SVR4. Two runqueues implement the kernel, real-time,
     and time-sharing scheduling classes (priorities 0 through 223). The third queue is
     only for the idle class (priorities 224 through 255).
         The two runqueues are designated current and next. Every thread that is granted
     a timeslice (place in the Ready state) is placed in either the current queue or the next
     queue, as explained subsequently, at the appropriate priority for that thread. The
     scheduler for a processor selects threads from the current queue in priority order until
     the current queue is empty. When the current queue is empty, the scheduler swaps the
     current and next queue and begins to schedule threads from the new current queue.
     The use of two runqueues guarantees that each thread will be granted processor time
     at least once every two queue switches regardless of priority, avoiding starvation.
         Several rules determine the assignment of a thread to either the current queue
     or the next queue:
     1.  Kernel and real-time threads are always inserted onto the current queue.
     2.  A time-sharing thread is assigned to the current queue if it is interactive
         (explained in the next subsection) or to the next queue otherwise. Inserting
         interactive threads onto the current queue results in a low interactive response
         time for such threads, compared to other time-sharing threads that do not
         exhibit a high degree of interactivity.
     INTERACTIVITY  SCORING  A thread is considered to be interactive if the ratio of
     its voluntary sleep time versus its run time is below a certain threshold. Interactive
     threads typically have high sleep times as they wait for user input. These sleep intervals
     are followed by bursts of processor activity as the thread processes the user's request.

                        Highest (31)
          Real-time
          priority
          classes
                        Lowest (16)
                        Highest (15)
          Variable
          priority
          classes
                        Lowest (0)
          Figure 10.14  Windows Thread Dispatching Priorities
The interactivity threshold is defined in the scheduler code and is not config-
urable. The scheduler uses two equations to compute the interactivity score of a
thread. First, we define a scaling factor:
                     Scaling factor =  Maximum interactivity score
                                            2
For threads whose sleep time exceeds their run time, the following equation
is used:
                     Interactivity score = Scaling factor a     run    b
                                                                sleep
When a thread's run time exceeds its sleep time, the following equation is used
instead:
                    Interactivity score = Scaling factor a 1 +  sleep     b
                                                                run

      The result is that threads whose sleep time exceeds their run time score in the
      lower half of the range of interactivity scores, and threads whose run time exceeds
      their sleep time score in the upper half of the range.
      THREAD MIGRATION  In general, it is desirable to schedule a Ready thread onto the
      last processor that it ran on; this is called processor affinity. The alternative is to allow
      a thread to migrate to another processor for its next execution time slice. Processor
      affinity is significant because of local caches dedicated to a single processor. When
      a thread is run, it may still have data in the cache of its last processor. Changing to
      another processor means that the necessary data must be loaded into caches in the
      new processor and cache lines from the preceding processor must be invalidated.
      On the other hand, processor migration may allow a better load balancing and may
      prevent idle periods on some processors while other processor have more work
      than they can handle in a timely fashion.
      The FreeBSD scheduler supports two mechanisms for thread migration to bal-
      ance load: pull and push. With the pull mechanism, and idle processor steals a thread
      from a nonidle processor. When a processor has no work to do, it sets a bit in a global
      bit-mask indicating that it is idle. When an active processor is about to add work to
      its own run queue, it first checks for such idle bits and if a set idle bit is found, passes
      the thread to the idle processor. It is primarily useful when there is a light or sporadic
      load, or in situations where processes are starting and exiting very frequently.
      The pull mechanism is effective in preventing the waste of a processor due
      to idleness. But it is not effective, or indeed relevant, in a situation in which every
      processor has work to do but the load has developed in an uneven fashion. With
      the push mechanism, a periodic scheduler task evaluates the current load situation
      and evens it out. Twice per second, this task picks the most-loaded and least-loaded
      processors in the system and equalizes their run queues. Push migration ensures
      fairness among the runnable threads.
10.6  WINDOWS SCHEDULING
      Windows is designed to be as responsive as possible to the needs of a single user in
      a highly interactive environment or in the role of a server. Windows implements a
      preemptive scheduler with a flexible system of priority levels that includes round-
      robin scheduling within each level and, for some levels, dynamic priority variation
      on the basis of their current thread activity. Threads are the unit of scheduling in
      Windows rather than processes.
      Process and Thread Priorities
      Priorities in Windows are organized into two bands, or classes: real time and vari-
      able. Each of these bands consists of 16 priority levels. Threads requiring immediate
      attention are in the real-time class, which includes functions such as communica-
      tions and real-time tasks.
      Overall, because Windows makes use of a priority-driven preemptive scheduler,
      threads with real-time priorities have precedence over other threads. When a thread

becomes ready whose priority is higher than the currently executing thread, the lower-
priority thread is preempted and the processor given to the higher-priority thread.
Priorities are handled somewhat differently in the two classes (Figure 10.14).
In the real-time priority class, all threads have a fixed priority that never changes.
All of the active threads at a given priority level are in a round-robin queue. In the
variable priority class, a thread's priority begins an initial priority value and then
may be temporarily boosted (raised) during the thread's lifetime. There is a FIFO
queue at each priority level; a thread will change queues among the variable priority
classes as its own priority changes. However, a thread at priority level 15 or below is
never boosted to level 16 or any other level in the real-time class.
The initial priority of a thread in the variable priority class is determined by two
quantities: process base priority and thread base priority. The process base priority
is an attribute of the process object, and can take on any value from 1 through 15
(priority 0 is reserved for the Executive's per-processor idle threads). Each thread
object associated with a process object has a thread base priority attribute that
indicates the thread's base priority relative to that of the process. The thread's base
priority can be equal to that of its process or within two levels above or below that of
the process. So, for example, if a process has a base priority of 4 and one of its threads
has a base priority of -1, then the initial priority of that thread is 3.
Once a thread in the variable priority class has been created, its actual prior-
ity, referred to as the thread's current priority, may fluctuate within given bounda-
ries. The current priority may never fall below the thread's base priority and it may
never exceed 15. Figure 10.15 gives an example. The process object has a base prior-
ity attribute of 4. Each thread object associated with this process object must have
an initial priority of between 2 and 6. Suppose the base priority for thread is 4. Then
the current priority for that thread may fluctuate in the range from 4 through 15
depending on what boosts it has been given. If a thread is interrupted to wait on an
I/O event, the kernel boosts its priority. If a boosted thread is interrupted because
15
14
13
12
11
10
9
8
7
6                       Highest
5                       Above normal
4   Base priority       Normal
3                       Below normal
2                       Lowest
1
0
              Process   Thread's base  Thread's dynamic
              priority  priority       priority
Figure 10.15  Example of Windows Priority Relationship

      it has used up its current time quantum, the kernel lowers its priority. Thus, proces-
      sor-bound threads tend toward lower priorities and I/O-bound threads tend toward
      higher priorities. In the case of I/O-bound threads, the kernel boosts the priority
      more for interactive waits (e.g., wait on keyboard or mouse) than for other types
      of I/O (e.g., disk I/O). Thus, interactive threads tend to have the highest priorities
      within the variable priority class.
      Multiprocessor Scheduling
      When Windows is run on a single processor, the highest-priority thread is always
      active unless it is waiting on an event. If there is more than one thread that has
      the same highest priority, then the processor is shared, round robin, among all the
      threads at that priority level. In a multiprocessor system with N processors, the ker-
      nel tries to give the N processors to the N highest-priority threads that are ready
      to run. The remaining, lower priority, threads must wait until the other threads
      block or have their priority decay. Lower-priority threads may also have their pri-
      ority boosted to 15 for a very short time if they are being starved, solely to correct
      instances of priority inversion.
      The foregoing scheduling discipline is affected by the processor affinity
      attribute of a thread. If a thread is ready to execute but the only available processors
      are not in its processor affinity set, then that thread is forced to wait, and the kernel
      schedules the next available thread.
10.7  LINUX VIRTUAL MACHINE PROCESS SCHEDULING
      The Linux VServer virtual machine facility, introduced in Chapter 2, provides a
      way of controlling VM use of processor time. VServer overlays a token bucket filter
      (TBF) on top of the standard Linux schedule. The purpose of the TBF is to deter-
      mine how much of the processor execution time (single processor, multiprocessor,
      or multicore) is allocated to each VM. If only the underlying Linux scheduler is
      used to globally schedule processes across all VMs, then resource hunger processes
      in one VM crowd out processes in other VMs.
      Figure 10.16 illustrates the TBF concept. For each VM, a bucket is defined
      with a capacity of S tokens. Tokens are added to the bucket at a rate of R tokens
      during every time interval of length T. When the bucket is full, additional incoming
      tokens are simply discarded. When a process is executing on this VM, it consumes
      one token for each timer clock tick. If the bucket empties, the process is put on
      hold and cannot be restarted until the bucket is refilled to a minimum threshold
      value of M tokens. At that point, the process is rescheduled. A significant conse-
      quence of the TBF approach is that a VM may accumulate tokens during a period of
      quiescence, and then later use the tokens in a burst when required.
      Adjusting the values of R and T allows for regulating the percentage of capac-
      ity that a VM can claim. For a single processor, we can define capacity allocation
      as follows:
                   R  = Fraction of processor allocation
                   T

                                  token input rate =
                               R/T tokens per second
                                                                         bucket size =
               current bucket                                            S tokens
               occupancy                                    minimum
                                                            threshold =
                                                            M tokens
                                  running process consumes
                                     1 token/timer tick
               Figure 10.16    Linux VServer Token Bucket Scheme
      This equation denotes the fraction of a single processor in a system. Thus, for exam-
      ple, if a system is multicore with four cores and we wish to provide one VM on
      an average of one dedicated processor, then we set R = 1 and T = 4. The overall
      system is limited as follows. If there are N VMs, then:
                                        N    Ri
                                        a    Ti       ...1
                                        i=1
      The parameters S and M are set so as to penalize a VM after a certain amount
      of burst time. The following parameters must be configured or allocated for a VM:
      following a burst time of B, the VM suffers a hold time of H. With these parameters,
      it is possible to calculate the desired values of S and M as follows:
                                  M=W*H*                       R
                                                               T
                               S  =  W  *    B  *     a1    -  Rb
                                                                  T
      where W is the rate at which the schedule runs (makes decisions). For example, consider
      a VM with a limit of 1/2 of processor time, and we wish to say that after using the proces-
      sor for 30 seconds, there will be a hold time of 5 seconds. The scheduler runs at 1,000 Hz.
      This requirement is met with the following values: M  1,000 * 5 * 0.5  2,500
      tokens; S = 1,000 * 30 * (1 - 0.5) = 15,000 tokens.
10.8  SUMMARY
      With a tightly coupled multiprocessor, multiple processors have access to the same
      main memory. In this configuration, the scheduling structure is somewhat more
      complex. For example, a given process may be assigned to the same processor for

      its entire life or dispatched to any processor each time it enters the Running state.
      Performance studies suggest that the differences among various scheduling algo-
      rithms are less significant in a multiprocessor system.
      A real-time process or task is one that is executed in connection with some
      process or function or set of events external to the computer system and that must
      meet one or more deadlines to interact effectively and correctly with the external
      environment. A real-time operating system is one that is capable of managing real-
      time processes. In this context, the traditional criteria for a scheduling algorithm
      do not apply. Rather, the key factor is the meeting of deadlines. Algorithms that
      rely heavily on preemption and on reacting to relative deadlines are appropriate
      in this context.
10.9  RECOMMENDED READING
      [WEND89] is an interesting discussion of approaches to multiprocessor scheduling.
      A good treatment of real-time scheduling is contained in [LIU00]. The following
      collections of papers all contain important articles on real-time operating systems
      and scheduling: [KRIS94], [STAN93], [LEE93], and [TILB91]. [SHA90] provides
      a good explanation of priority inversion, priority inheritance, and priority ceiling.
      [ZEAD97] analyzes the performance of the SVR4 real-time scheduler. [LIND04]
      provides an overview of the Linux 2.6 scheduler; [LOVE10] contains a more
      detailed discussion.
      KRIS94  Krishna, C., and Lee, Y., eds. "Special Issue on Real-Time Systems."
      Proceedings of the IEEE, January 1994.
      LEE93   Lee, Y., and Krishna, C., eds. Readings in Real-Time Systems. Los Alamitos,
      CA: IEEE Computer Society Press, 1993.
      LIND04  Lindsley, R. "What's New in the 2.6 Scheduler." Linux Journal, March 2004.
      LIU00   Liu, J. Real-Time Systems. Upper Saddle River, NJ: Prentice Hall, 2000.
      LOVE10  Love, R. Linux Kernel Development. Upper Saddle River, NJ: Addison-
      Wesley, 2010.
      SHA90   Sha, L., Rajkumar, R., and Lehoczky, J. "Priority Inheritance Protocols: An
      Approach to Real-Time Synchronization." IEEE Transactions on Computers,
      September 1990.
      STAN93  Stankovic, J., and Ramamritham, K., eds. Advances in Real-Time Systems.
      Los Alamitos, CA: IEEE Computer Society Press, 1993.
      TILB91  Tilborg, A., and Koob, G., eds. Foundations of Real-Time Computing:
      Scheduling and Resource Management. Boston: Kluwer Academic Publishers, 1991.
      WEND89  Wendorf, J., Wendorf, R., and Tokuda, H. "Scheduling Operating System
      Processing on Small-Scale Microprocessors." Proceedings, 22nd Annual Hawaii
      International Conference on System Science, January 1989.
      ZEAD97  Zeadally, S. "An Evaluation of the Real-Time Performance of SVR4.0 and
      SVR4.2." Operating Systems Review, January 1977.

10.10  KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS
Key Terms
aperiodic task                      hard real-time task                  real-time scheduling
deadline scheduling                 load sharing                         responsiveness
deterministic operating system      periodic task                        soft real-time task
fail-soft operation                 priority inversion                   thread scheduling
gang scheduling                     rate monotonic scheduling            unbounded priority
granularity                         real-time operating system           inversion
Review Questions
       10.1     List and briefly define five different categories of synchronization granularity.
       10.2     List and briefly define four techniques for thread scheduling.
       10.3     List and briefly define three versions of load sharing.
       10.4     What is the difference between hard and soft real-time tasks?
       10.5     What is the difference between periodic and aperiodic real-time tasks?
       10.6     List and briefly define five general areas of requirements for a real-time operating system.
       10.7     List and briefly define four classes of real-time scheduling algorithms.
       10.8     What items of information about a task might be useful in real-time scheduling?
Problems
       10.1     Consider a set of three periodic tasks with the execution profiles of Table 10.6.
                Develop scheduling diagrams similar to those of Figure 10.5 for this set of tasks.
Table           10.6     Execution  Profile for Problem  10.1
                 Process            Arrival Time               Execution Time       Ending Deadline
                 A(1)               0                           10                             20
                 A(2)               20                          10                             40
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·
                 B(1)               0                           10                             50
                 B(2)               50                          10                            100
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·
                 C(1)               0                           15                             50
                 C(2)               50                          15                            100
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·
                      ·             ·                           ·                              ·

     Table 10.7      Execution Profile for Problem 10.2
            Process                    Arrival Time             Execution Time       Starting Deadline
                  A                          10                      20                        100
                  B                          20                      20                        30
                  C                          40                      20                        60
                  D                          50                      20                        80
                  E                          60                      20                        70
     10.2   Consider       a  set  of  five  aperiodic  tasks  with  the  execution  profiles  of  Table  10.7.
            Develop scheduling diagrams similar to those of Figure 10.6 for this set of tasks.
     10.3   Least laxity first (LLF) is a real-time scheduling algorithm for periodic tasks. Slack
            time, or laxity, is the amount of time between when a task would complete if it started
            now and its next deadline. This is the size of the available scheduling window. Laxity
            can be expressed as
                      Laxity = (deadline time) - (current time) - (processor time needed)
            LLF selects the task with the minimum laxity to execute next. If two or more tasks
            have the same minimum laxity value, they are serviced on a FCFS basis.
            a.    Suppose a task currently has a laxity of t. By how long may the scheduler delay
                  starting this task and still meet its deadline?
            b.    Suppose a task currently has a laxity of 0. What does this mean?
            c.    What does it mean if a task has negative laxity?
            d.    Consider a set of three periodic tasks with the execution profiles of Table 10.8a.
                  Develop scheduling diagrams similar to those of Figure 10.4 for this set of tasks
                  that compare rate monotonic, earliest deadline first, and LLF.Assume preemption
                  may occur at 5-ms intervals. Comment on the results.
     10.4   Repeat Problem 10.3d for the execution profiles of Table 10.8b. Comment on the
            results.
     10.5   Maximum urgency first (MUF) is a real-time scheduling algorithm for periodic tasks.
            Each task is assigned an urgency that is defined as a combination of two fixed pri-
            orities and one dynamic priority. One of the fixed priorities, the criticality, has pre-
            cedence over the dynamic priority. Meanwhile, the dynamic priority has precedence
            over the other fixed priority, called the user priority. The dynamic priority is inversely
     Table  10.8     Execution     Profiles  for  Problems 10.3 through   10.6
                                                  (a) Light load
                     Task                               Period                       Execution Time
                      A                                 6                            2
                      B                                 8                            2
                      C                                 12                           3
                                                  (b) Heavy load
                     Task                               Period                       Execution Time
                      A                                 6                            2
                      B                                 8                            5
                      C                                 12                           3

      proportional to the laxity of a task. MUF can be explained as follows. First, tasks are
      ordered from shortest to longest period. Define the critical task set as the first N tasks
      such that worst-case processor utilization does not exceed 100%. Among critical set
      tasks that are ready, the scheduler selects the task with the least laxity. If no critical
      set tasks are ready, the schedule chooses among the noncritical tasks the one with
      the least laxity. Ties are broken through an optional user priority and then by FCFS.
      Repeat Problem 10.3d, adding MUF to the diagrams.Assume that user-defined priori-
      ties are A highest, B next, C lowest. Comment on the results.
10.6  Repeat Problem 10.4, adding MUF to the diagrams. Comment on the results.
10.7  This   problem   demonstrates      that  although  Equation  (10.2)  for  rate  monotonic
      scheduling is a sufficient condition for successful scheduling, it is not a necessary
      condition (i.e., sometimes successful scheduling is possible even if Equation (10.2) is
      not satisfied).
      a.  Consider a task set with the following independent periodic tasks:
          ·  Task P1: C1 = 20; T1 = 100
          ·  Task P2: C2 = 30; T2 = 145
          Can these tasks be successfully scheduled using rate monotonic scheduling?
      b.  Now add the following task to the set:
          ·  Task P3: C3 = 68; T3 = 150
          Is Equation (10.2) satisfied?
      c.  Suppose that the first instance of the preceding three tasks arrives  at    time  t     =  0.
          Assume that the first deadline for each task is the following:
                       D1 = 100;               D2 = 145;  D3 = 150
      Using rate monotonic scheduling, will all three deadlines be met? What about dead-
      lines for future repetitions of each task?
10.8  Draw a diagram similar to that of Figure 10.9b that shows the sequence events for this
      same example using priority ceiling.

                                            CHAPTER
