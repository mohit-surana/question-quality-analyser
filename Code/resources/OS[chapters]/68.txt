Operating System Overview
OPERATING SYSTEM OVERVIEW
    2.1   Operating System Objectives and Functions
                 The Operating System as a User/Computer Interface
                 The Operating System as Resource Manager
                 Ease of Evolution of an Operating System
    2.2   The Evolution of Operating Systems
                 Serial Processing
                 Simple Batch Systems
                 Multiprogrammed Batch Systems
                 Time-Sharing Systems
    2.3   Major Achievements
                 The Process
                 Memory Management
                 Information Protection and Security
                 Scheduling and Resource Management
    2.4   Developments Leading to Modern Operating Systems
    2.5   Virtual Machines
                 Virtual Machines and Virtualizing
                 Virtual Machine Architecture
    2.6   OS Design Considerations for Multiprocessor and Multicore
                 Symmetric Multiprocessor OS Considerations
                 Multicore OS Considerations
    2.7   Microsoft Windows Overview
                 History
                 The Modern OS
                 Architecture
                 Client/Server Model
                 Threads and SMP
                 Windows Objects
                 What Is New in Windows 7
    2.8   Traditional Unix Systems
                 History
                 Description
    2.9   Modern Unix Systems
                 System V Release 4 (SVR4)
                 BSD
                 Solaris 10
    2.10  Linux
                 History
                 Modular Structure
                 Kernel Components
    2.11  Linux Vserver Virtual Machine Architecture
    2.12  Recommended Reading and Web Sites
    2.13  Key Terms, Review Questions, and Problems
46

   Operating systems are those programs that interface the machine with
   the applications programs. The main function of these systems is to
   dynamically allocate the shared system resources to the executing
   programs. As such, research in this area is clearly concerned with
   the management and scheduling of memory, processes, and other
   devices. But the interface with adjacent levels continues to shift with
   time. Functions that were originally part of the operating system have
   migrated to the hardware. On the other side, programmed functions
   extraneous to the problems being solved by the application programs
   are included in the operating system.
   --WHAT CAN BE AUTOMATED?: THE COMPUTER SCIENCE AND
                                  ENGINEERING RESEARCH STUDY, MIT Press, 1980
LEARNING OBJECTIVES
After studying this chapter, you should be able to:
  Summarize, at a top level, the key functions of an operating system (OS).
  Discuss the evolution of operating systems for early simple batch systems to
   modern complex systems.
  Give a brief explanation of each of the major achievements in OS research,
   as defined in Section 2.3.
  Discuss the key design areas that have been instrumental in the development
   of modern operating systems.
  Define and discuss virtual machines and virtualization
  Understand the OS design issues raised by the introduction of multiprocessor
   and multicore organization.
  Understand the basic structure of Windows 7.
  Describe the essential elements of a traditional UNIX system.
  Explain the new features found in modern UNIX systems.
  Discuss Linux and its relationship to UNIX.
We begin our study of operating systems (OSs) with a brief history. This history is
itself interesting and also serves the purpose of providing an overview of OS prin-
ciples. The first section examines the objectives and functions of operating systems.
Then we look at how operating systems have evolved from primitive batch systems
to sophisticated multitasking, multiuser systems. The remainder of the chapter looks
at the history and general characteristics of the two operating systems that serve as
examples throughout this book. All of the material in this chapter is covered in
greater depth later in the book.

2.1  OPERATING SYSTEM OBJECTIVES AND FUNCTIONS
     An OS is a program that controls the execution of application programs and acts as
     an interface between applications and the computer hardware. It can be thought of
     as having three objectives:
       Convenience: An OS makes a computer more convenient to use.
       Efficiency: An OS allows the computer system resources to be used in an effi-
        cient manner.
       Ability to evolve: An OS should be constructed in such a way as to permit the
        effective development, testing, and introduction of new system functions with-
        out interfering with service.
        Let us examine these three aspects of an OS in turn.
     The Operating System as a User/Computer Interface
     The hardware and software used in providing applications to a user can be viewed
     in a layered or hierarchical fashion, as depicted in Figure 2.1. The user of those
     applications, the end user, generally is not concerned with the details of computer
     hardware. Thus, the end user views a computer system in terms of a set of applica-
     tions. An application can be expressed in a programming language and is developed
     by an application programmer. If one were to develop an application program as a
     set of machine instructions that is completely responsible for controlling the com-
     puter hardware, one would be faced with an overwhelmingly complex undertaking.
     To ease this chore, a set of system programs is provided. Some of these programs
     are referred to as utilities, or library programs. These implement frequently used
     functions that assist in program creation, the management of files, and the control of
        Application                         Application programs
     programming interface
        Application                         Libraries/utilities                Software
        binary interface
                                            Operating system
        Instruction set
        architecture
                                       Execution hardware
                                       System interconnect        Memory
                                                    (bus)         translation  Hardware
                                       I/O devices                Main
                                       and                        memory
                                       networking
     Figure 2.1  Computer   Hardware and Software Structure

I/O devices. A programmer will make use of these facilities in developing an appli-
cation, and the application, while it is running, will invoke the utilities to perform
certain functions. The most important collection of system programs comprises the
OS. The OS masks the details of the hardware from the programmer and provides
the programmer with a convenient interface for using the system. It acts as media-
tor, making it easier for the programmer and for application programs to access and
use those facilities and services.
   Briefly, the OS typically provides services in the following areas:
  Program development: The OS provides a variety of facilities and services,
   such as editors and debuggers, to assist the programmer in creating programs.
   Typically, these services are in the form of utility programs that, while not
   strictly part of the core of the OS, are supplied with the OS and are referred to
   as application program development tools.
  Program execution: A number of steps need to be performed to execute a
   program. Instructions and data must be loaded into main memory, I/O devices
   and files must be initialized, and other resources must be prepared. The OS
   handles these scheduling duties for the user.
  Access to I/O devices: Each I/O device requires its own peculiar set of instruc-
   tions or control signals for operation. The OS provides a uniform interface
   that hides these details so that programmers can access such devices using sim-
   ple reads and writes.
  Controlled access to files: For file access, the OS must reflect a detailed under-
   standing of not only the nature of the I/O device (disk drive, tape drive) but
   also the structure of the data contained in the files on the storage medium.
   In the case of a system with multiple users, the OS may provide protection
   mechanisms to control access to the files.
  System access: For shared or public systems, the OS controls access to the
   system as a whole and to specific system resources. The access function must
   provide protection of resources and data from unauthorized users and must
   resolve conflicts for resource contention.
  Error detection and response: A variety of errors can occur while a computer
   system is running. These include internal and external hardware errors, such
   as a memory error, or a device failure or malfunction; and various software
   errors, such as division by zero, attempt to access forbidden memory location,
   and inability of the OS to grant the request of an application. In each case,
   the OS must provide a response that clears the error condition with the least
   impact on running applications. The response may range from ending the pro-
   gram that caused the error, to retrying the operation, to simply reporting the
   error to the application.
  Accounting: A good OS will collect usage statistics for various resources and
   monitor performance parameters such as response time. On any system, this
   information is useful in anticipating the need for future enhancements and in
   tuning the system to improve performance. On a multiuser system, the infor-
   mation can be used for billing purposes.

       Figure 2.1 also indicates three key interfaces in a typical computer system:
      Instruction set architecture (ISA): The ISA defines the repertoire of machine
       language instructions that a computer can follow. This interface is the bound-
       ary between hardware and software. Note that both application programs
       and utilities may access the ISA directly. For these programs, a subset of the
       instruction repertoire is available (user ISA). The OS has access to additional
       machine language instructions that deal with managing system resources
       (system ISA).
      Application binary interface (ABI): The ABI defines a standard for binary
       portability across programs. The ABI defines the system call interface to
       the operating system and the hardware resources and services available in a
       system through the user ISA.
      Application programming interface (API): The API gives a program access
       to the hardware resources and services available in a system through the user
       ISA supplemented with high-level language (HLL) library calls. Any system
       calls are usually performed through libraries. Using an API enables applica-
       tion software to be ported easily, through recompilation, to other systems that
       support the same API.
    The Operating System as Resource Manager
    A computer is a set of resources for the movement, storage, and processing of data
    and for the control of these functions. The OS is responsible for managing these
    resources.
       Can we say that it is the OS that controls the movement, storage, and process-
    ing of data? From one point of view, the answer is yes: By managing the computer's
    resources, the OS is in control of the computer's basic functions. But this control is
    exercised in a curious way. Normally, we think of a control mechanism as something
    external to that which is controlled, or at least as something that is a distinct and
    separate part of that which is controlled. (For example, a residential heating sys-
    tem is controlled by a thermostat, which is separate from the heat-generation and
    heat-distribution apparatus.) This is not the case with the OS, which as a control
    mechanism is unusual in two respects:
      The OS functions in the same way as ordinary computer software; that is, it is
       a program or suite of programs executed by the processor.
      The OS frequently relinquishes control and must depend on the processor to
       allow it to regain control.
       Like other computer programs, the OS provides instructions for the proces-
    sor. The key difference is in the intent of the program. The OS directs the processor
    in the use of the other system resources and in the timing of its execution of other
    programs. But in order for the processor to do any of these things, it must cease
    executing the OS program and execute other programs. Thus, the OS relinquishes
    control for the processor to do some "useful" work and then resumes control long
    enough to prepare the processor to do the next piece of work. The mechanisms
    involved in all this should become clear as the chapter proceeds.

              Computer system
   Memory                                                  I/O devices
   Operating  I/O controller                                                                  Printers,
   system                                                                                     keyboards,
   software                                                                                   digital camera,
              I/O controller                                                                  etc.
   Programs
   and data
              I/O controller
   Processor                   Processor
                                                           Storage
                                                           OS
                                                           Programs
                                                           Data
Figure 2.2  The Operating System as Resource Manager
   Figure 2.2 suggests the main resources that are managed by the OS. A portion
of the OS is in main memory. This includes the kernel, or nucleus, which contains
the most frequently used functions in the OS and, at a given time, other portions
of the OS currently in use. The remainder of main memory contains user programs
and data. The memory management hardware in the processor and the OS jointly
control the allocation of main memory, as we shall see. The OS decides when an I/O
device can be used by a program in execution and controls access to and use of files.
The processor itself is a resource, and the OS must determine how much processor
time is to be devoted to the execution of a particular user program. In the case of a
multiple-processor system, this decision must span all of the processors.
Ease of Evolution of an Operating System
A major OS will evolve over time for a number of reasons:
  Hardware upgrades plus new types of hardware: For example, early versions
   of UNIX and the Macintosh OS did not employ a paging mechanism because
   they were run on processors without paging hardware.1 Subsequent versions
   of these operating systems were modified to exploit paging capabilities. Also,
1Paging is introduced briefly later in this chapter and is discussed in detail in Chapter 7.

        the use of graphics terminals and page-mode terminals instead of line-at-a-
        time scroll mode terminals affects OS design. For example, a graphics terminal
        typically allows the user to view several applications at the same time through
        "windows" on the screen. This requires more sophisticated support in the OS.
       New services: In response to user demand or in response to the needs of sys-
        tem managers, the OS expands to offer new services. For example, if it is found
        to be difficult to maintain good performance for users with existing tools, new
        measurement and control tools may be added to the OS.
       Fixes: Any OS has faults. These are discovered over the course of time and
        fixes are made. Of course, the fix may introduce new faults.
        The need to change an OS regularly places certain requirements on its design.
     An obvious statement is that the system should be modular in construction, with
     clearly defined interfaces between the modules, and that it should be well docu-
     mented. For large programs, such as the typical contemporary OS, what might be
     referred to as straightforward modularization is inadequate [DENN80a]. That is,
     much more must be done than simply partitioning a program into modules. We
     return to this topic later in this chapter.
2.2  THE EVOLUTION OF OPERATING SYSTEMS
     In attempting to understand the key requirements for an OS and the significance
     of the major features of a contemporary OS, it is useful to consider how operating
     systems have evolved over the years.
     Serial Processing
     With the earliest computers, from the late 1940s to the mid-1950s, the programmer
     interacted directly with the computer hardware; there was no OS. These computers
     were run from a console consisting of display lights, toggle switches, some form of
     input device, and a printer. Programs in machine code were loaded via the input
     device (e.g., a card reader). If an error halted the program, the error condition was
     indicated by the lights. If the program proceeded to a normal completion, the out-
     put appeared on the printer.
        These early systems presented two main problems:
       Scheduling: Most installations used a hardcopy sign-up sheet to reserve com-
        puter time. Typically, a user could sign up for a block of time in multiples of a
        half hour or so. A user might sign up for an hour and finish in 45 minutes; this
        would result in wasted computer processing time. On the other hand, the user
        might run into problems, not finish in the allotted time, and be forced to stop
        before resolving the problem.
       Setup time: A single program, called a job, could involve loading the com-
        piler plus the high-level language program (source program) into memory,
        saving the compiled program (object program) and then loading and linking
        together the object program and common functions. Each of these steps could

   involve mounting or dismounting tapes or setting up card decks. If an error
   occurred, the hapless user typically had to go back to the beginning of the
   setup sequence. Thus, a considerable amount of time was spent just in setting
   up the program to run.
   This mode of operation could be termed serial processing, reflecting the fact
that users have access to the computer in series. Over time, various system software
tools were developed to attempt to make serial processing more efficient. These
include libraries of common functions, linkers, loaders, debuggers, and I/O driver
routines that were available as common software for all users.
Simple Batch Systems
Early computers were very expensive, and therefore it was important to maxi-
mize processor utilization. The wasted time due to scheduling and setup time was
unacceptable.
   To improve utilization, the concept of a batch OS was developed. It appears
that the first batch OS (and the first OS of any kind) was developed in the mid-1950s
by General Motors for use on an IBM 701 [WEIZ81]. The concept was subsequently
refined and implemented on the IBM 704 by a number of IBM customers. By the
early 1960s, a number of vendors had developed batch operating systems for their
computer systems. IBSYS, the IBM OS for the 7090/7094 computers, is particularly
notable because of its widespread influence on other systems.
   The central idea behind the simple batch-processing scheme is the use of a
piece of software known as the monitor. With this type of OS, the user no longer has
direct access to the processor. Instead, the user submits the job on cards or tape to a
computer operator, who batches the jobs together sequentially and places the entire
batch on an input device, for use by the monitor. Each program is constructed to
branch back to the monitor when it completes processing, at which point the moni-
tor automatically begins loading the next program.
   To understand how this scheme works, let us look at it from two points of
view: that of the monitor and that of the processor.
  Monitor point of view: The monitor controls the sequence of events. For this
   to be so, much of the monitor must always be in main memory and available
   for execution (Figure 2.3). That portion is referred to as the resident monitor.
   The rest of the monitor consists of utilities and common functions that are
   loaded as subroutines to the user program at the beginning of any job that
   requires them. The monitor reads in jobs one at a time from the input device
   (typically a card reader or magnetic tape drive). As it is read in, the current job
   is placed in the user program area, and control is passed to this job. When the
   job is completed, it returns control to the monitor, which immediately reads
   in the next job. The results of each job are sent to an output device, such as a
   printer, for delivery to the user.
  Processor point of view: At a certain point, the processor is executing instruc-
   tions from the portion of main memory containing the monitor. These
   instructions cause the next job to be read into another portion of main

                                                    Interrupt
                                                    processing
                                                    Device
                                                    drivers
                                          Monitor   Job
                                                    sequencing
                                                   Control language
                                                    interpreter
                              Boundary
                                                    User
                                                    program
                                                    area
                              Figure 2.3  Memory Layout for a
                                          Resident Monitor
    memory. Once a job has been read in, the processor will encounter a branch
    instruction in the monitor that instructs the processor to continue execution
    at the start of the user program. The processor will then execute the instruc-
    tions in the user program until it encounters an ending or error condition.
    Either event causes the processor to fetch its next instruction from the moni-
    tor program. Thus the phrase "control is passed to a job" simply means that
    the processor is now fetching and executing instructions in a user program,
    and "control is returned to the monitor" means that the processor is now
    fetching and executing instructions from the monitor program.
    The monitor performs a scheduling function: A batch of jobs is queued up,
    and jobs are executed as rapidly as possible, with no intervening idle time. The mon-
    itor improves job setup time as well. With each job, instructions are included in a
    primitive form of job control language (JCL). This is a special type of programming
    language used to provide instructions to the monitor. A simple example is that of a
    user submitting a program written in the programming language FORTRAN plus
    some data to be used by the program. All FORTRAN instructions and data are on a
    separate punched card or a separate record on tape. In addition to FORTRAN and
    data lines, the job includes job control instructions, which are denoted by the begin-
    ning $. The overall format of the job looks like this:
    $JOB
    $FTN
    
                    FORTRAN instructions
    

   $LOAD
   $RUN
   
              Data
   
   $END
   To execute this job, the monitor reads the $FTN line and loads the appropri-
ate language compiler from its mass storage (usually tape). The compiler translates
the user's program into object code, which is stored in memory or mass storage.
If it is stored in memory, the operation is referred to as "compile, load, and go."
If it is stored on tape, then the $LOAD instruction is required. This instruction is
read by the monitor, which regains control after the compile operation. The moni-
tor invokes the loader, which loads the object program into memory (in place of
the compiler) and transfers control to it. In this manner, a large segment of main
memory can be shared among different subsystems, although only one such subsys-
tem could be executing at a time.
   During the execution of the user program, any input instruction causes one
line of data to be read. The input instruction in the user program causes an input
routine that is part of the OS to be invoked. The input routine checks to make
sure that the program does not accidentally read in a JCL line. If this happens, an
error occurs and control transfers to the monitor. At the completion of the user
job, the monitor will scan the input lines until it encounters the next JCL instruc-
tion. Thus, the system is protected against a program with too many or too few
data lines.
   The monitor, or batch OS, is simply a computer program. It relies on the abil-
ity of the processor to fetch instructions from various portions of main memory to
alternately seize and relinquish control. Certain other hardware features are also
desirable:
  Memory protection: While the user program is executing, it must not alter the
   memory area containing the monitor. If such an attempt is made, the proces-
   sor hardware should detect an error and transfer control to the monitor. The
   monitor would then abort the job, print out an error message, and load in the
   next job.
  Timer: A timer is used to prevent a single job from monopolizing the system.
   The timer is set at the beginning of each job. If the timer expires, the user pro-
   gram is stopped, and control returns to the monitor.
  Privileged instructions: Certain machine level instructions are designated priv-
   ileged and can be executed only by the monitor. If the processor encounters
   such an instruction while executing a user program, an error occurs causing
   control to be transferred to the monitor. Among the privileged instructions
   are I/O instructions, so that the monitor retains control of all I/O devices. This
   prevents, for example, a user program from accidentally reading job control
   instructions from the next job. If a user program wishes to perform I/O, it must
   request that the monitor perform the operation for it.

      Interrupts: Early computer models did not have this capability. This feature
       gives the OS more flexibility in relinquishing control to and regaining control
       from user programs.
       Considerations of memory protection and privileged instructions lead to the
    concept of modes of operation. A user program executes in a user mode, in which
    certain areas of memory are protected from the user's use and in which certain
    instructions may not be executed. The monitor executes in a system mode, or what
    has come to be called kernel mode, in which privileged instructions may be executed
    and in which protected areas of memory may be accessed.
       Of course, an OS can be built without these features. But computer vendors
    quickly learned that the results were chaos, and so even relatively primitive batch
    operating systems were provided with these hardware features.
       With a batch OS, processor time alternates between execution of user pro-
    grams and execution of the monitor. There have been two sacrifices: Some main
    memory is now given over to the monitor and some processor time is consumed by
    the monitor. Both of these are forms of overhead. Despite this overhead, the simple
    batch system improves utilization of the computer.
    Multiprogrammed Batch Systems
    Even with the automatic job sequencing provided by a simple batch OS, the proces-
    sor is often idle. The problem is that I/O devices are slow compared to the processor.
    Figure 2.4 details a representative calculation. The calculation concerns a program
    that processes a file of records and performs, on average, 100 machine instructions
    per record. In this example, the computer spends over 96% of its time waiting for
    I/O devices to finish transferring data to and from the file. Figure 2.5a illustrates this
    situation, where we have a single program, referred to as uniprogramming. The pro-
    cessor spends a certain amount of time executing, until it reaches an I/O instruction.
    It must then wait until that I/O instruction concludes before proceeding.
       This inefficiency is not necessary. We know that there must be enough
    memory to hold the OS (resident monitor) and one user program. Suppose that
    there is room for the OS and two user programs. When one job needs to wait for
    I/O, the processor can switch to the other job, which is likely not waiting for I/O
    (Figure 2.5b). Furthermore, we might expand memory to hold three, four, or more
    programs and switch among all of them (Figure 2.5c). The approach is known as
    multiprogramming, or multitasking. It is the central theme of modern operating
    systems.
              Read one record from file                    15 ms
              Execute 100 instructions                     1 ms
              Write one record to file                     15 ms
              Total                                        31 ms
              Percent CPU Utilization =    1            =  0.032  =  3.2%
                                           31
              Figure 2.4    System Utilization Example

           Program A   Run                  Wait              Run             Wait
                             Time
                                            (a) Uniprogramming
           Program A   Run                  Wait              Run             Wait
           Program B   Wait  Run            Wait                   Run        Wait
           Combined    Run   Run            Wait              Run  Run        Wait
                       A        B                             A       B
                             Time
                             (b) Multiprogramming with two programs
           Program A   Run                  Wait              Run             Wait
           Program B   Wait  Run            Wait                   Run        Wait
           Program C      Wait         Run              Wait             Run        Wait
           Combined    Run   Run       Run        Wait        Run  Run   Run        Wait
                       A        B      C                      A       B  C
                             Time
                             (c) Multiprogramming with three programs
           Figure 2.5  Multiprogramming Example
To illustrate the benefit of multiprogramming, we give a simple example.
Consider a computer with 250 Mbytes of available memory (not used by the OS),
a disk, a terminal, and a printer. Three programs, JOB1, JOB2, and JOB3, are
submitted for execution at the same time, with the attributes listed in Table 2.1.
We assume minimal processor requirements for JOB2 and JOB3 and continuous
disk and printer use by JOB3. For a simple batch environment, these jobs will be
executed in sequence. Thus, JOB1 completes in 5 minutes. JOB2 must wait until
Table 2.1    Sample Program Execution Attributes
                                JOB1                          JOB2                        JOB3
Type of job            Heavy compute                    Heavy I/O                   Heavy I/O
Duration                        5 min                         15 min                      10 min
Memory required                 50 M                          100 M                       75 M
Need disk?                         No                         No                          Yes
Need terminal?                     No                         Yes                         No
Need printer?                      No                         No                          Yes

    Table 2.2    Effects of  Multiprogramming on Resource Utilization
                                    Uniprogramming                     Multiprogramming
    Processor use                   20%                                40%
    Memory use                      33%                                67%
    Disk use                        33%                                67%
    Printer use                     33%                                67%
    Elapsed time                    30 min                             15 min
    Throughput                      6 jobs/hr                          12 jobs/hr
    Mean response time              18 min                             10 min
    the 5 minutes are over and then completes 15 minutes after that. JOB3 begins after
    20 minutes and completes at 30 minutes from the time it was initially submitted.
    The average resource utilization, throughput, and response times are shown in the
    uniprogramming column of Table 2.2. Device-by-device utilization is illustrated in
    Figure 2.6a. It is evident that there is gross underutilization for all resources when
    averaged over the required 30-minute time period.
    Now suppose that the jobs are run concurrently under a multiprogramming
    OS. Because there is little resource contention between the jobs, all three can run
    in nearly minimum time while coexisting with the others in the computer (assum-
    ing that JOB2 and JOB3 are allotted enough processor time to keep their input
    and output operations active). JOB1 will still require 5 minutes to complete, but at
    the end of that time, JOB2 will be one-third finished and JOB3 half finished. All
    three jobs will have finished within 15 minutes. The improvement is evident when
    examining the multiprogramming column of Table 2.2, obtained from the histogram
    shown in Figure 2.6b.
    As with a simple batch system, a multiprogramming batch system must rely
    on certain computer hardware features. The most notable additional feature that
    is useful for multiprogramming is the hardware that supports I/O interrupts and
    DMA (direct memory access). With interrupt-driven I/O or DMA, the processor
    can issue an I/O command for one job and proceed with the execution of another
    job while the I/O is carried out by the device controller. When the I/O operation is
    complete, the processor is interrupted and control is passed to an interrupt-handling
    program in the OS. The OS will then pass control to another job.
    Multiprogramming operating systems are fairly sophisticated compared to
    single-program, or uniprogramming, systems. To have several jobs ready to run,
    they must be kept in main memory, requiring some form of memory management.
    In addition, if several jobs are ready to run, the processor must decide which one
    to run, this decision requires an algorithm for scheduling. These concepts are dis-
    cussed later in this chapter.
    Time-Sharing Systems
    With the use of multiprogramming, batch processing can be quite efficient.
    However, for many jobs, it is desirable to provide a mode in which the user interacts
    directly with the computer. Indeed, for some jobs, such as transaction processing, an
    interactive mode is essential.

    CPU                                                             CPU
                                                              0%                                                0%
                                                              100%                                              100%
    Memory                                                          Memory
                                                              0%                                                0%
                                                              100%                                              100%
    Disk                                                            Disk
                                                              0%                                                0%
                                                              100%                                              100%
    Terminal                                                        Terminal
                                                              0%                                                0%
                                                              100%                                              100%
    Printer                                                         Printer
                                                              0%                                                0%
    Job history     JOB1         JOB2               JOB3            Job history     JOB1
                                                                                             JOB2
                 0        5  10             15  20  25    30                        JOB3
                                 minutes
                                                    time                         0        5           10  15
                                                                                             minutes      time
                             (a) Uniprogramming                                     (b) Multiprogramming
59  Figure 2.6      Utilization Histograms

    Today, the requirement for an interactive computing facility can be, and often
    is, met by the use of a dedicated personal computer or workstation. That option was
    not available in the 1960s, when most computers were big and costly. Instead, time
    sharing was developed.
    Just as multiprogramming allows the processor to handle multiple batch jobs
    at a time, multiprogramming can also be used to handle multiple interactive jobs. In
    this latter case, the technique is referred to as time sharing, because processor time is
    shared among multiple users. In a time-sharing system, multiple users simultaneously
    access the system through terminals, with the OS interleaving the execution of each
    user program in a short burst or quantum of computation. Thus, if there are n users
    actively requesting service at one time, each user will only see on the average 1/n
    of the effective computer capacity, not counting OS overhead. However, given the
    relatively slow human reaction time, the response time on a properly designed system
    should be similar to that on a dedicated computer.
    Both batch processing and time sharing use multiprogramming. The key
    differences are listed in Table 2.3.
    One of the first time-sharing operating systems to be developed was the
    Compatible Time-Sharing System (CTSS) [CORB62], developed at MIT by a
    group known as Project MAC (Machine-Aided Cognition, or Multiple-Access
    Computers). The system was first developed for the IBM 709 in 1961 and later
    transferred to an IBM 7094.
    Compared to later systems, CTSS is primitive. The system ran on a computer
    with 32,000 36-bit words of main memory, with the resident monitor consuming 5000
    of that. When control was to be assigned to an interactive user, the user's program
    and data were loaded into the remaining 27,000 words of main memory. A pro-
    gram was always loaded to start at the location of the 5000th word; this simplified
    both the monitor and memory management. A system clock generated interrupts
    at a rate of approximately one every 0.2 seconds. At each clock interrupt, the OS
    regained control and could assign the processor to another user. This technique is
    known as time slicing. Thus, at regular time intervals, the current user would be
    preempted and another user loaded in. To preserve the old user program status for
    later resumption, the old user programs and data were written out to disk before the
    new user programs and data were read in. Subsequently, the old user program code
    and data were restored in main memory when that program was next given a turn.
    To minimize disk traffic, user memory was only written out when the incoming
    program would overwrite it. This principle is illustrated in Figure 2.7. Assume that
    there are four interactive users with the following memory requirements, in words:
     JOB1: 15,000
     JOB2: 20,000
    Table 2.3  Batch Multiprogramming versus Time Sharing
                                 Batch Multiprogramming         Time Sharing
    Principal objective          Maximize processor use         Minimize response time
    Source of directives to      Job control language commands  Commands entered at the
    operating system             provided with the job          terminal

0            Monitor             0      Monitor        0          Monitor
5000                             5000                  5000       JOB 3
             JOB 1                                     10000
                                        JOB 2                     (JOB 2)
20000
             Free                25000                 25000
                                        Free                      Free
32000                            32000                 32000
             (a)                        (b)                       (c)
0            Monitor             0      Monitor        0          Monitor
5000                             5000                  5000
             JOB 1                      JOB 4
                                 15000  (JOB 1)                   JOB 2
20000        (JOB 2)             20000  (JOB 2)
25000                            25000                 25000
             Free                       Free                      Free
32000                            32000                 32000
             (d)                        (e)                       (f)
Figure  2.7  CTSS     Operation
 JOB3: 5000
 JOB4: 10,000
Initially, the monitor loads JOB1 and transfers control to it (a). Later, the
monitor decides to transfer control to JOB2. Because JOB2 requires more mem-
ory than JOB1, JOB1 must be written out first, and then JOB2 can be loaded (b).
Next, JOB3 is loaded in to be run. However, because JOB3 is smaller than JOB2,
a portion of JOB2 can remain in memory, reducing disk write time (c). Later, the
monitor decides to transfer control back to JOB1. An additional portion of JOB2
must be written out when JOB1 is loaded back into memory (d). When JOB4 is
loaded, part of JOB1 and the portion of JOB2 remaining in memory are retained
(e). At this point, if either JOB1 or JOB2 is activated, only a partial load will be
required. In this example, it is JOB2 that runs next. This requires that JOB4 and the
remaining resident portion of JOB1 be written out and that the missing portion of
JOB2 be read in (f).
The CTSS approach is primitive compared to present-day time sharing, but
it was effective. It was extremely simple, which minimized the size of the monitor.
Because a job was always loaded into the same locations in memory, there was no
need for relocation techniques at load time (discussed subsequently). The technique
of only writing out what was necessary minimized disk activity. Running on the
7094, CTSS supported a maximum of 32 users.
Time sharing and multiprogramming raise a host of new problems for the OS.
If multiple jobs are in memory, then they must be protected from interfering with
each other by, for example, modifying each other's data. With multiple interactive
users, the file system must be protected so that only authorized users have access

     to a particular file. The contention for resources, such as printers and mass storage
     devices, must be handled. These and other problems, with possible solutions, will be
     encountered throughout this text.
2.3  MAJOR ACHIEVEMENTS
     Operating systems are among the most complex pieces of software ever devel-
     oped. This reflects the challenge of trying to meet the difficult and in some cases
     competing objectives of convenience, efficiency, and ability to evolve. [DENN80a]
     proposes that there have been four major theoretical advances in the development
     of operating systems:
       Processes
       Memory management
       Information protection and security
       Scheduling and resource management
        Each advance is characterized by principles, or abstractions, developed to
     meet difficult practical problems. Taken together, these five areas span many of
     the key design and implementation issues of modern operating systems. The brief
     review of these five areas in this section serves as an overview of much of the rest
     of the text.
     The Process
     Central to the design of operating systems is the concept of process. This term was
     first used by the designers of Multics in the 1960s [DALE68]. It is a somewhat
     more general term than job. Many definitions have been given for the term process,
     including
       A program in execution
       An instance of a program running on a computer
       The entity that can be assigned to and executed on a processor
       A unit of activity characterized by a single sequential thread of execution, a
        current state, and an associated set of system resources
     This concept should become clearer as we proceed.
        Three major lines of computer system development created problems in timing
     and synchronization that contributed to the development of the concept of the
     process: multiprogramming batch operation, time sharing, and real-time transaction
     systems. As we have seen, multiprogramming was designed to keep the processor
     and I/O devices, including storage devices, simultaneously busy to achieve maxi-
     mum efficiency. The key mechanism is this: In response to signals indicating the
     completion of I/O transactions, the processor is switched among the various pro-
     grams residing in main memory.

   A second line of development was general-purpose time sharing. Here, the
key design objective is to be responsive to the needs of the individual user and yet,
for cost reasons, be able to support many users simultaneously. These goals are
compatible because of the relatively slow reaction time of the user. For example,
if a typical user needs an average of 2 seconds of processing time per minute, then
close to 30 such users should be able to share the same system without noticeable
interference. Of course, OS overhead must be factored into such calculations.
   A third important line of development has been real-time transaction process-
ing systems. In this case, a number of users are entering queries or updates against a
database. An example is an airline reservation system. The key difference between
the transaction processing system and the time-sharing system is that the former
is limited to one or a few applications, whereas users of a time-sharing system can
engage in program development, job execution, and the use of various applications.
In both cases, system response time is paramount.
   The principal tool available to system programmers in developing the early
multiprogramming and multiuser interactive systems was the interrupt. The activity
of any job could be suspended by the occurrence of a defined event, such as an I/O
completion. The processor would save some sort of context (e.g., program coun-
ter and other registers) and branch to an interrupt-handling routine, which would
determine the nature of the interrupt, process the interrupt, and then resume user
processing with the interrupted job or some other job.
   The design of the system software to coordinate these various activities turned
out to be remarkably difficult. With many jobs in progress at any one time, each of
which involved numerous steps to be performed in sequence, it became impossible
to analyze all of the possible combinations of sequences of events. In the absence of
some systematic means of coordination and cooperation among activities, program-
mers resorted to ad hoc methods based on their understanding of the environment
that the OS had to control. These efforts were vulnerable to subtle programming
errors whose effects could be observed only when certain relatively rare sequences
of actions occurred. These errors were difficult to diagnose because they needed to
be distinguished from application software errors and hardware errors. Even when
the error was detected, it was difficult to determine the cause, because the precise
conditions under which the errors appeared were very hard to reproduce. In general
terms, there are four main causes of such errors [DENN80a]:
  Improper synchronization: It is often the case that a routine must be sus-
   pended awaiting an event elsewhere in the system. For example, a program
   that initiates an I/O read must wait until the data are available in a buffer
   before proceeding. In such cases, a signal from some other routine is required.
   Improper design of the signaling mechanism can result in signals being lost or
   duplicate signals being received.
  Failed mutual exclusion: It is often the case that more than one user or pro-
   gram will attempt to make use of a shared resource at the same time. For
   example, two users may attempt to edit the same file at the same time. If
   these accesses are not controlled, an error can occur. There must be some
   sort of mutual exclusion mechanism that permits only one routine at a time
   to perform an update against the file. The implementation of such mutual

       exclusion is difficult to verify as being correct under all possible sequences
       of events.
      Nondeterminate program operation: The results of a particular program
       normally should depend only on the input to that program and not on
       the activities of other programs in a shared system. But when programs share
       memory, and their execution is interleaved by the processor, they may inter-
       fere with each other by overwriting common memory areas in unpredictable
       ways. Thus, the order in which various programs are scheduled may affect the
       outcome of any particular program.
      Deadlocks: It is possible for two or more programs to be hung up waiting for
       each other. For example, two programs may each require two I/O devices to
       perform some operation (e.g., disk to tape copy). One of the programs has
       seized control of one of the devices and the other program has control of
       the other device. Each is waiting for the other program to release the desired
       resource. Such a deadlock may depend on the chance timing of resource
       allocation and release.
       What is needed to tackle these problems is a systematic way to monitor
    and control the various programs executing on the processor. The concept of the
    process provides the foundation. We can think of a process as consisting of three
    components:
      An executable program
      The associated data needed by the program (variables, work space, buffers, etc.)
      The execution context of the program
       This last element is essential. The execution context, or process state, is the
    internal data by which the OS is able to supervise and control the process. This
    internal information is separated from the process, because the OS has information
    not permitted to the process. The context includes all of the information that the OS
    needs to manage the process and that the processor needs to execute the process
    properly. The context includes the contents of the various processor registers, such
    as the program counter and data registers. It also includes information of use to the
    OS, such as the priority of the process and whether the process is waiting for the
    completion of a particular I/O event.
       Figure 2.8 indicates a way in which processes may be managed. Two proc-
    esses, A and B, exist in portions of main memory. That is, a block of memory is
    allocated to each process that contains the program, data, and context information.
    Each process is recorded in a process list built and maintained by the OS. The
    process list contains one entry for each process, which includes a pointer to the
    location of the block of memory that contains the process. The entry may also
    include part or all of the execution context of the process. The remainder of the
    execution context is stored elsewhere, perhaps with the process itself (as indicated
    in Figure 2.8) or frequently in a separate region of memory. The process index
    register contains the index into the process list of the process currently controlling
    the processor. The program counter points to the next instruction in that process
    to be executed. The base and limit registers define the region in memory occupied

                  Main                                Processor
                  memory                              registers
                           Process index                      i
                                               PC
               i
Process                                        Base           b
list                                           limit          h
               j
                           Other
                           registers
                  Context
Process           Data
A
                  Program
                  (code)
               b
                  Context
Process     h     Data
B
                  Program
                  (code)
Figure 2.8     Typical Process Implementation
by the process: The base register is the starting address of the region of memory
and the limit is the size of the region (in bytes or words). The program counter and
all data references are interpreted relative to the base register and must not exceed
the value in the limit register. This prevents interprocess interference.
In Figure 2.8, the process index register indicates that process B is execut-
ing. Process A was previously executing but has been temporarily interrupted. The
contents of all the registers at the moment of A's interruption were recorded in its
execution context. Later, the OS can perform a process switch and resume execution
of process A. The process switch consists of storing the context of B and restoring
the context of A. When the program counter is loaded with a value pointing into A's
program area, process A will automatically resume execution.
Thus, the process is realized as a data structure. A process can either be
executing or awaiting execution. The entire state of the process at any instant is con-
tained in its context. This structure allows the development of powerful techniques
for ensuring coordination and cooperation among processes. New features can be
designed and incorporated into the OS (e.g., priority) by expanding the context to

    include any new information needed to support the feature. Throughout this book,
    we will see a number of examples where this process structure is employed to solve
    the problems raised by multiprogramming and resource sharing.
       A final point, which we introduce briefly here, is the concept of thread. In
    essence, a single process, which is assigned certain resources, can be broken up into
    multiple, concurrent threads that execute cooperatively to perform the work of the
    process. This introduces a new level of parallel activity to be managed by the hard-
    ware and software.
    Memory Management
    The needs of users can be met best by a computing environment that supports
    modular programming and the flexible use of data. System managers need efficient
    and orderly control of storage allocation. The OS, to satisfy these requirements, has
    five principal storage management responsibilities:
      Process isolation: The OS must prevent independent processes from interfer-
       ing with each other's memory, both data and instructions.
      Automatic allocation and management: Programs should be dynamically
       allocated across the memory hierarchy as required. Allocation should be
       transparent to the programmer. Thus, the programmer is relieved of concerns
       relating to memory limitations, and the OS can achieve efficiency by assigning
       memory to jobs only as needed.
      Support of modular programming: Programmers should be able to define pro-
       gram modules, and to create, destroy, and alter the size of modules dynamically.
      Protection and access control: Sharing of memory, at any level of the memory
       hierarchy, creates the potential for one program to address the memory space
       of another. This is desirable when sharing is needed by particular applications.
       At other times, it threatens the integrity of programs and even of the OS itself.
       The OS must allow portions of memory to be accessible in various ways by
       various users.
      Long-term storage: Many application programs require means for storing
       information for extended periods of time, after the computer has been
       powered down.
       Typically, operating systems meet these requirements with virtual memory
    and file system facilities. The file system implements a long-term store, with infor-
    mation stored in named objects, called files. The file is a convenient concept for the
    programmer and is a useful unit of access control and protection for the OS.
       Virtual memory is a facility that allows programs to address memory from
    a logical point of view, without regard to the amount of main memory physically
    available. Virtual memory was conceived to meet the requirement of having multi-
    ple user jobs reside in main memory concurrently, so that there would not be a hia-
    tus between the execution of successive processes while one process was written out
    to secondary store and the successor process was read in. Because processes vary
    in size, if the processor switches among a number of processes it is difficult to pack
    them compactly into main memory. Paging systems were introduced, which allow

processes to be comprised of a number of fixed-size blocks, called pages. A pro-
gram references a word by means of a virtual address consisting of a page number
and an offset within the page. Each page of a process may be located anywhere
in main memory. The paging system provides for a dynamic mapping between the
virtual address used in the program and a real address, or physical address, in main
memory.
With dynamic mapping hardware available, the next logical step was to
eliminate the requirement that all pages of a process reside in main memory simul-
taneously. All the pages of a process are maintained on disk. When a process is
executing, some of its pages are in main memory. If reference is made to a page
that is not in main memory, the memory management hardware detects this and
arranges for the missing page to be loaded. Such a scheme is referred to as virtual
memory and is depicted in Figure 2.9.
         A.1
              A.0  A.2
              A.5                            0                           0
                                             1                           1
         B.0  B.1  B.2  B.3                  2                           2
                                             3                           3
                                             4                           4
                                             5                           5
                   A.7                       6                           6
              A.9                            7                           User
                                             8                           program
                                                                         B
                   A.8                       9
                                             10
                                             User
                                             program
                                             A
              B.5  B.6
         Main memory                                  Disk
Main memory consists of a                    Secondary memory (disk) can
number of fixed-length frames,               hold many fixed-length pages. A
each equal to the size of a page.            user program consists of some
For a program to execute, some               number of pages. Pages for all
or all of its pages must be in               programs plus the operating system
main memory.                                 are on disk, as are files.
Figure 2.9    Virtual Memory       Concepts

                                                             Real
                                        Memory-              address
    Processor                           management
                 Virtual                    unit
                 address                                              Main
                                                                      memory
                                                    Disk
                                                    address
                                                                      Secondary
                                                                      memory
    Figure 2.10  Virtual Memory Addressing
    The processor hardware, together with the OS, provides the user with a
    "virtual processor" that has access to a virtual memory. This memory may be a
    linear address space or a collection of segments, which are variable-length blocks
    of contiguous addresses. In either case, programming language instructions can
    reference program and data locations in the virtual memory area. Process isolation
    can be achieved by giving each process a unique, nonoverlapping virtual memory.
    Memory sharing can be achieved by overlapping portions of two virtual memory
    spaces. Files are maintained in a long-term store. Files and portions of files may be
    copied into the virtual memory for manipulation by programs.
    Figure 2.10 highlights the addressing concerns in a virtual memory scheme.
    Storage consists of directly addressable (by machine instructions) main memory
    and lower-speed auxiliary memory that is accessed indirectly by loading blocks
    into main memory. Address translation hardware (memory management unit) is
    interposed between the processor and memory. Programs reference locations using
    virtual addresses, which are mapped into real main memory addresses. If a refer-
    ence is made to a virtual address not in real memory, then a portion of the contents
    of real memory is swapped out to auxiliary memory and the desired block of data
    is swapped in. During this activity, the process that generated the address reference
    must be suspended. The OS designer needs to develop an address translation mech-
    anism that generates little overhead and a storage allocation policy that minimizes
    the traffic between memory levels.
    Information Protection and Security
    The growth in the use of time-sharing systems and, more recently, computer net-
    works has brought with it a growth in concern for the protection of information.
    The nature of the threat that concerns an organization will vary greatly depending
    on the circumstances. However, there are some general-purpose tools that can be

built into computers and operating systems that support a variety of protection and
security mechanisms. In general, we are concerned with the problem of controlling
access to computer systems and the information stored in them.
   Much of the work in security and protection as it relates to operating systems
can be roughly grouped into four categories:
  Availability: Concerned with protecting the system against interruption.
  Confidentiality: Assures that users cannot read data for which access is
   unauthorized.
  Data integrity: Protection of data from unauthorized modification.
  Authenticity: Concerned with the proper verification of the identity of users
   and the validity of messages or data.
Scheduling and Resource Management
A key responsibility of the OS is to manage the various resources available to it
(main memory space, I/O devices, processors) and to schedule their use by the vari-
ous active processes. Any resource allocation and scheduling policy must consider
three factors:
  Fairness: Typically, we would like all processes that are competing for the use
   of a particular resource to be given approximately equal and fair access to that
   resource. This is especially so for jobs of the same class, that is, jobs of similar
   demands.
  Differential responsiveness: On the other hand, the OS may need to discrimi-
   nate among different classes of jobs with different service requirements. The
   OS should attempt to make allocation and scheduling decisions to meet the
   total set of requirements. The OS should also make these decisions dynami-
   cally. For example, if a process is waiting for the use of an I/O device, the OS
   may wish to schedule that process for execution as soon as possible to free up
   the device for later demands from other processes.
  Efficiency:     The  OS  should  attempt   to  maximize  throughput,  minimize
   response time, and, in the case of time sharing, accommodate as many users
   as possible. These criteria conflict; finding the right balance for a particular
   situation is an ongoing problem for OS research.
   Scheduling and resource management are essentially operations-research
problems and the mathematical results of that discipline can be applied. In addition,
measurement of system activity is important to be able to monitor performance and
make adjustments.
   Figure 2.11 suggests the major elements of the OS involved in the scheduling
of processes and the allocation of resources in a multiprogramming environment.
The OS maintains a number of queues, each of which is simply a list of processes
waiting for some resource. The short-term queue consists of processes that are in
main memory (or at least an essential minimum portion of each is in main memory)
and are ready to run as soon as the processor is made available. Any one of these
processes could use the processor next. It is up to the short-term scheduler, or

                      Operating system
    Service call      Service
    from process      call
                      handler (code)
    Interrupt                              Long-                      Short-        I/O
    from process      Interrupt            term                       term        queues
                      handler (code)       queue                      queue
    Interrupt
    from I/O
                                                                      Short-term
                                                                      scheduler
                                                                      (code)
                                                                      Pass control
                                                                      to process
    Figure 2.11       Key Elements of an Operating System for Multiprogramming
    dispatcher, to pick one. A common strategy is to give each process in the queue
    some time in turn; this is referred to as a round-robin technique. In effect, the
    round-robin technique employs a circular queue. Another strategy is to assign
    priority levels to the various processes, with the scheduler selecting processes in
    priority order.
    The long-term queue is a list of new jobs waiting to use the processor. The
    OS adds jobs to the system by transferring a process from the long-term queue to
    the short-term queue. At that time, a portion of main memory must be allocated
    to the incoming process. Thus, the OS must be sure that it does not overcommit
    memory or processing time by admitting too many processes to the system. There
    is an I/O queue for each I/O device. More than one process may request the use of
    the same I/O device. All processes waiting to use each device are lined up in that
    device's queue. Again, the OS must determine which process to assign to an avail-
    able I/O device.
    The OS receives control of the processor at the interrupt handler if an inter-
    rupt occurs. A process may specifically invoke some OS service, such as an I/O
    device handler by means of a service call. In this case, a service call handler is the
    entry point into the OS. In any case, once the interrupt or service call is handled, the
    short-term scheduler is invoked to pick a process for execution.
    The foregoing is a functional description; details and modular design of this
    portion of the OS will differ in various systems. Much of the research and develop-
    ment effort in operating systems has been directed at picking algorithms and data
    structures for this function that provide fairness, differential responsiveness, and
    efficiency.

2.4  DEVELOPMENTS LEADING TO MODERN OPERATING
     SYSTEMS
     Over the years, there has been a gradual evolution of OS structure and capabilities.
     However, in recent years a number of new design elements have been introduced
     into both new operating systems and new releases of existing operating systems that
     create a major change in the nature of operating systems. These modern operating
     systems respond to new developments in hardware, new applications, and new secu-
     rity threats. Among the key hardware drivers are multiprocessor systems, greatly
     increased processor speed, high-speed network attachments, and increasing size
     and variety of memory storage devices. In the application arena, multimedia appli-
     cations, Internet and Web access, and client/server computing have influenced OS
     design. With respect to security, Internet access to computers has greatly increased
     the potential threat and increasingly sophisticated attacks, such as viruses, worms,
     and hacking techniques, have had a profound impact on OS design.
        The rate of change in the demands on operating systems requires not just
     modifications and enhancements to existing architectures but new ways of organ-
     izing the OS. A wide range of different approaches and design elements has been
     tried in both experimental and commercial operating systems, but much of the work
     fits into the following categories:
       Microkernel architecture
       Multithreading
       Symmetric multiprocessing
       Distributed operating systems
       Object-oriented design
        Most operating systems, until recently, featured a large monolithic kernel.
     Most of what is thought of as OS functionality is provided in these large kernels,
     including scheduling, file system, networking, device drivers, memory management,
     and more. Typically, a monolithic kernel is implemented as a single process, with
     all elements sharing the same address space. A microkernel architecture assigns
     only a few essential functions to the kernel, including address spaces, interproc-
     ess communication (IPC), and basic scheduling. Other OS services are provided by
     processes, sometimes called servers, that run in user mode and are treated like any
     other application by the microkernel. This approach decouples kernel and server
     development. Servers may be customized to specific application or environment
     requirements. The microkernel approach simplifies implementation, provides flex-
     ibility, and is well suited to a distributed environment. In essence, a microkernel
     interacts with local and remote server processes in the same way, facilitating con-
     struction of distributed systems.
        Multithreading is a technique in which a process, executing an application, is
     divided into threads that can run concurrently. We can make the following distinction:
      Thread: A dispatchable unit of work. It includes a processor context (which
        includes the program counter and stack pointer) and its own data area for a

       stack (to enable subroutine branching). A thread executes sequentially and is
       interruptable so that the processor can turn to another thread.
      Process: A collection of one or more threads and associated system resources
       (such as memory containing both code and data, open files, and devices). This
       corresponds closely to the concept of a program in execution. By breaking
       a single application into multiple threads, the programmer has great control
       over the modularity of the application and the timing of application-related
       events.
       Multithreading is useful for applications that perform a number of essentially
    independent tasks that do not need to be serialized. An example is a database server
    that listens for and processes numerous client requests. With multiple threads run-
    ning within the same process, switching back and forth among threads involves
    less processor overhead than a major process switch between different processes.
    Threads are also useful for structuring processes that are part of the OS kernel as
    described in subsequent chapters.
       Symmetric multiprocessing (SMP) is a term that refers to a computer hard-
    ware architecture (described in Chapter 1) and also to the OS behavior that exploits
    that architecture. The OS of an SMP schedules processes or threads across all of the
    processors. SMP has a number of potential advantages over uniprocessor architec-
    ture, including the following:
      Performance: If the work to be done by a computer can be organized so that
       some portions of the work can be done in parallel, then a system with multiple
       processors will yield greater performance than one with a single processor of
       the same type. This is illustrated in Figure 2.12. With multiprogramming, only
       one process can execute at a time; meanwhile all other processes are waiting
       for the processor. With multiprocessing, more than one process can be run-
       ning simultaneously, each on a different processor.
      Availability: In a symmetric multiprocessor, because all processors can per-
       form the same functions, the failure of a single processor does not halt the
       system. Instead, the system can continue to function at reduced performance.
      Incremental growth: A user can enhance the performance of a system by add-
       ing an additional processor.
      Scaling: Vendors can offer a range of products with different price and per-
       formance characteristics based on the number of processors configured in the
       system.
    It is important to note that these are potential, rather than guaranteed, benefits. The
    OS must provide tools and functions to exploit the parallelism in an SMP system.
       Multithreading and SMP are often discussed together, but the two are
    independent facilities. Even on a uniprocessor system, multithreading is useful for
    structuring applications and kernel processes. An SMP system is useful even for
    nonthreaded processes, because several processes can run in parallel. However, the
    two facilities complement each other and can be used effectively together.
       An attractive feature of an SMP is that the existence of multiple processors is
    transparent to the user. The OS takes care of scheduling of threads or processes on

             Time
Process 1
Process 2
Process 3
                      (a) Interleaving (multiprogramming; one processor)
Process 1
Process 2
Process 3
                      (b) Interleaving and overlapping (multiprocessing; two processors)
Blocked               Running
Figure 2.12  Multiprogramming and Multiprocessing
individual processors and of synchronization among processors. This book discusses
the scheduling and synchronization mechanisms used to provide the single-system
appearance to the user. A different problem is to provide the appearance of a sin-
gle system for a cluster of separate computers--a multicomputer system. In this
case, we are dealing with a collection of entities (computers), each with its own
main memory, secondary memory, and other I/O modules. A distributed operating
system provides the illusion of a single main memory space and a single secondary
memory space, plus other unified access facilities, such as a distributed file system.
Although clusters are becoming increasingly popular, and there are many cluster
products on the market, the state of the art for distributed operating systems lags
that of uniprocessor and SMP operating systems. We examine such systems in
Part Eight.
             Another innovation in OS design is the use of object-oriented technologies.
Object-oriented design lends discipline to the process of adding modular extensions
to a small kernel. At the OS level, an object-based structure enables programmers
to customize an OS without disrupting system integrity. Object orientation also
eases the development of distributed tools and full-blown distributed operating
systems.

2.5  VIRTUAL MACHINES
     Virtual Machines and Virtualizing
     Traditionally, applications have run directly on an OS on a PC or a server. Each PC
     or server would run only one OS at a time. Thus, the vendor had to rewrite parts
     of its applications for each OS/platform they would run on. An effective strategy
     for dealing with this problem is known as virtualization. Virtualization technology
     enables a single PC or server to simultaneously run multiple operating systems or
     multiple sessions of a single OS. A machine with virtualization can host numerous
     applications, including those that run on different operating systems, on a single
     platform. In essence, the host operating system can support a number of virtual
     machines (VM), each of which has the characteristics of a particular OS and, in some
     versions of virtualization, the characteristics of a particular hardware platform.
     The VM approach is becoming a common way for businesses and individuals
     to deal with legacy applications and to optimize their hardware usage by maximizing
     the number of kinds of applications that a single computer can handle [GEER09].
     Commercial VM offerings by companies such as VMware and Microsoft are widely
     used, with millions of copies having been sold. In addition to their use in server
     environments, these VM technologies also are used in desktop environments to run
     multiple operating systems, typically Windows and Linux.
     The  specific  architecture  of      the  VM  approach    varies        among  vendors.
     Figure 2.13 shows a typical arrangement. The virtual machine monitor (VMM), or
     hypervisor, runs on top of (or is incorporated into) the host OS. The VMM sup-
     ports VMs, which are emulated hardware devices. Each VM runs a separate OS.
     The VMM handles each operating system's communications with the processor,
     the storage medium, and the network. To execute programs, the VMM hands off
     the processor control to a virtual OS on a VM. Most VMs use virtualized network
          Applications         Applications                    Applications
                    and           and                          and
                    processes  processes                       processes
                    Virtual    Virtual                         Virtual
          machine 1            machine 2                       machine n
                               Virtual machine monitor
                                  Host operating system
                                  Shared hardware
          Figure 2.13          Virtual Memory Concept

connections to communicate with one another, when such communication is needed.
Key to the success of this approach is that the VMM provides a layer between soft-
ware environments and the underlying hardware and host OS that is programma-
ble, transparent to the software above it, and makes efficient use of the hardware
below it.
Virtual Machine Architecture2
Recall from Section 2.1 (see Figure 2.1) the discussion of the application program-
ming interface, the application binary interface, and the instruction set archi-
tecture. Let us use these interface concepts to clarify the meaning of machine in
the term virtual machine. Consider a process executing a compiled application
program. From the perspective of the process, the machine on which it executes
consists of the virtual memory space assigned to the process, the processor reg-
isters it may use, the user-level machine instructions it may execute, and the OS
system calls it may invoke for I/O. Thus the ABI defines the machine as seen by
a process.
From the perspective of an application, the machine characteristics are speci-
fied by high-level language capabilities, and OS and system library calls. Thus, the
API defines the machine for an application.
For the operating system, the machine hardware defines the system that
supports the operation of the OS and the numerous processes that execute con-
currently. These processes share a file system and other I/O resources. The system
allocates real memory and I/O resources to the processes and allows the processes
to interact with their resources. From the OS perspective, therefore, it is the ISA
that provides the interface between the system and machine.
With        these    considerations  in  mind,              we  can  consider  two  architectural
approaches to implementing virtual machines: process VMs and system VMs.
PROCESS     VIRTUAL  MACHINE  In essence, a process VM presents an ABI to an
application process, translates a set of OS and user-level instructions composing one
platform to those of another (Figure 2.14a). A process VM is a virtual platform for
executing a single process. As such, the process VM is created when the process is
created and terminated when the process is terminated.
In order to provide cross-platform portability, a common implementation of
the process VM architecture is as part of an overall HLL application environment.
The resulting ABI does not correspond to any specific machine. Instead, the ABI
specification is designed to easily support a given HLL or set of HLLs and to be eas-
ily portable to a variety of ISAs. The HLL VM includes a front-end compiler that
generates a virtual binary code for execution or interpretation. This code can then
be executed on any machine that has the process VM implemented.
Two widely used examples of this approach are the Java VM architecture and
the Microsoft Common Language Infrastructure, which is the foundation of the
.NET framework.
2Much of the discussion that follows is based on [SMIT05].

                                Virtualizing                  Application
                                architecture                      view
                   Guest        Application                   Application
                                    process                       process
                                              ABI
                   VM           Virtualizing
                   software     software                          Process
                                                                  virtual
                                OS                            machine
                   Host                       ABI
                                Hardware
                                              (a) Process VM
                                Applications                  Applications
                   Guest
                                OS                            OS
                                              API
                   VMM          Virtualizing
                                software                          System
                                              ISA                 virtual
                   Host         Hardware                      machine
                                              (b) System VM
                   Figure 2.14  Process and System Virtual    Machines
    SYSTEM VIRTUAL MACHINE          In a system VM, virtualizing software translates the
    ISA used by one hardware platform to that of another. Note in Figure 2.14a that
    the virtualizing software in the process VM approach makes use of the services of
    the host OS, while in the system VM approach there is logically no separate host
    OS, rather the host system OS incorporates the VM capability. In the system VM
    case, the virtualizing software is host to a number of guest operating systems, with
    each VM including its own OS. The VMM emulates the hardware ISA so that the
    guest software can potentially execute a different ISA from the one implemented
    on the host.
    With the system VM approach, a single hardware platform can support mul-
    tiple, isolated guest OS environments simultaneously. This approach provides a
    number of benefits, including application portability, support of legacy systems
    without the need to maintain legacy hardware, and security by means of isolation of
    each guest OS environment from the other guest environments.
    A variant on the architecture shown in Figure 2.14b is referred to as a hosted
    VM. In this case, the VMM is built on top of an existing host OS. The VMM relies
    on the host OS to provide device drivers and other lower-level services. An example
    of a hosted VM is the VMware GSX server.

2.6  OS DESIGN CONSIDERATIONS FOR MULTIPROCESSOR
     AND MULTICORE
     Symmetric Multiprocessor OS Considerations
     In an SMP system, the kernel can execute on any processor, and typically each
     processor does self-scheduling from the pool of available processes or threads.
     The kernel can be constructed as multiple processes or multiple threads, allowing
     portions of the kernel to execute in parallel. The SMP approach complicates the OS.
     The OS designer must deal with the complexity due to sharing resources (like data
     structures) and coordinating actions (like accessing devices) from multiple parts of
     the OS executing at the same time. Techniques must be employed to resolve and
     synchronize claims to resources.
        An SMP operating system manages processor and other computer resources
     so that the user may view the system in the same fashion as a multiprogramming
     uniprocessor system. A user may construct applications that use multiple processes
     or multiple threads within processes without regard to whether a single processor
     or multiple processors will be available. Thus, a multiprocessor OS must provide all
     the functionality of a multiprogramming system plus additional features to accom-
     modate multiple processors. The key design issues include the following:
       Simultaneous concurrent processes or threads: Kernel routines need to be
        reentrant to allow several processors to execute the same kernel code simulta-
        neously. With multiple processors executing the same or different parts of the
        kernel, kernel tables and management structures must be managed properly
        to avoid data corruption or invalid operations.
       Scheduling: Any processor may perform scheduling, which complicates the
        task of enforcing a scheduling policy and assuring that corruption of the sched-
        uler data structures is avoided. If kernel-level multithreading is used, then the
        opportunity exists to schedule multiple threads from the same process simul-
        taneously on multiple processors. Multiprocessor scheduling is examined in
        Chapter 10.
       Synchronization: With multiple active processes having potential access to
        shared address spaces or shared I/O resources, care must be taken to provide
        effective synchronization. Synchronization is a facility that enforces mutual
        exclusion and event ordering. A common synchronization mechanism used in
        multiprocessor operating systems is locks, described in Chapter 5.
       Memory management: Memory management on a multiprocessor must deal
        with all of the issues found on uniprocessor computers and is discussed in Part
        Three. In addition, the OS needs to exploit the available hardware parallelism
        to achieve the best performance. The paging mechanisms on different proc-
        essors must be coordinated to enforce consistency when several processors
        share a page or segment and to decide on page replacement. The reuse of
        physical pages is the biggest problem of concern; that is, it must be guaranteed
        that a physical page can no longer be accessed with its old contents before the
        page is put to a new use.

      Reliability and fault tolerance: The OS should provide graceful degradation
       in the face of processor failure. The scheduler and other portions of the OS
       must recognize the loss of a processor and restructure management tables
       accordingly.
       Because multiprocessor OS design issues generally involve extensions to
    solutions to multiprogramming uniprocessor design problems, we do not treat
    multiprocessor operating systems separately. Rather, specific multiprocessor issues
    are addressed in the proper context throughout this book.
    Multicore OS Considerations
    The considerations for multicore systems include all the design issues discussed so
    far in this section for SMP systems. But additional concerns arise. The issue is one
    of the scale of the potential parallelism. Current multicore vendors offer systems
    with up to eight cores on a single chip. With each succeeding processor technology
    generation, the number of cores and the amount of shared and dedicated cache
    memory increases, so that we are now entering the era of "many-core" systems.
       The design challenge for a many-core multicore system is to efficiently
    harness the multicore processing power and intelligently manage the substantial
    on-chip resources efficiently. A central concern is how to match the inherent paral-
    lelism of a many-core system with the performance requirements of applications.
    The potential for parallelism in fact exists at three levels in contemporary multicore
    system. First, there is hardware parallelism within each core processor, known as
    instruction level parallelism, which may or may not be exploited by application pro-
    grammers and compilers. Second, there is the potential for multiprogramming and
    multithreaded execution within each processor. Finally, there is the potential for
    a single application to execute in concurrent processes or threads across multiple
    cores. Without strong and effective OS support for the last two types of parallelism
    just mentioned, hardware resources will not be efficiently used.
       In essence, then, since the advent of multicore technology, OS designers have
    been struggling with the problem of how best to extract parallelism from computing
    workloads. A variety of approaches are being explored for next-generation operat-
    ing systems. We introduce two general strategies in this section and consider some
    details in later chapters.
    PARALLELISM  WITHIN         APPLICATIONS  Most applications can, in principle, be
    subdivided into multiple tasks that can execute in parallel, with these tasks then
    being implemented as multiple processes, perhaps each with multiple threads. The
    difficulty is that the developer must decide how to split up the application work into
    independently executable tasks. That is, the developer must decide what pieces can
    or should be executed asynchronously or in parallel. It is primarily the compiler and
    the programming language features that support the parallel programming design
    process. But, the OS can support this design process, at minimum, by efficiently
    allocating resources among parallel tasks as defined by the developer.
       Perhaps the most effective initiative to support developers is implemented in
    the latest release of the UNIX-based Mac OS X operating system. Mac OS X 10.6
    includes a multicore support capability known as Grand Central Dispatch (GCD).

GCD does not help the developer decide how to break up a task or application into
separate concurrent parts. But once a developer has identified something that can
be split off into a separate task, GCD makes it as easy and noninvasive as possible
to actually do so.
In essence, GCD is a thread pool mechanism, in which the OS maps tasks onto
threads representing an available degree of concurrency (plus threads for block-
ing on I/O). Windows also has a thread pool mechanism (since 2000), and thread
pools have been heavily used in server applications for years. What is new in GCD
is the extension to programming languages to allow anonymous functions (called
blocks) as a way of specifying tasks. GCD is hence not a major evolutionary step.
Nevertheless, it is a new and valuable tool for exploiting the available parallelism of
a multicore system.
One of Apple's slogans for GCD is "islands of serialization in a sea of concurrency."
That captures the practical reality of adding more concurrency to run-of-the-mill
desktop applications. Those islands are what isolate developers from the thorny
problems of simultaneous data access, deadlock, and other pitfalls of multithreading.
Developers are encouraged to identify functions of their applications that would be
better executed off the main thread, even if they are made up of several sequential or
otherwise partially interdependent tasks. GCD makes it easy to break off the entire
unit of work while maintaining the existing order and dependencies between subtasks.
In later chapters, we look at some of the details of GCD.
VIRTUAL  MACHINE     APPROACH  An alternative approach is to recognize that
with the ever-increasing number of cores on a chip, the attempt to multiprogram
individual cores to support multiple applications may be a misplaced use of
resources [JACK10]. If instead, we allow one or more cores to be dedicated to a
particular process and then leave the processor alone to devote its efforts to that
process, we avoid much of the overhead of task switching and scheduling decisions.
The multicore OS could then act as a hypervisor that makes a high-level decision
to allocate cores to applications but does little in the way of resource allocation
beyond that.
The reasoning behind this approach is as follows. In the early days of com-
puting, one program was run on a single processor. With multiprogramming,
each application is given the illusion that it is running on a dedicated processor.
Multiprogramming is based on the concept of a process, which is an abstraction of
an execution environment. To manage processes, the OS requires protected space,
free from user and program interference. For this purpose, the distinction between
kernel mode and user mode was developed. In effect, kernel mode and user mode
abstracted the processor into two processors. With all these virtual processors, how-
ever, come struggles over who gets the attention of the real processor. The overhead
of switching between all these processors starts to grow to the point where respon-
siveness suffers, especially when multiple cores are introduced. But with many-core
systems, we can consider dropping the distinction between kernel and user mode.
In this approach, the OS acts more like a hypervisor. The programs themselves take
on many of the duties of resource management. The OS assigns an application a
processor and some memory, and the program itself, using metadata generated by
the compiler, would best know how to use these resources.

2.7  MICROSOFT WINDOWS OVERVIEW
     History
     The story of Windows begins with a very different OS, developed by Microsoft for
     the first IBM personal computer and referred to as MS-DOS. The initial version,
     MS-DOS 1.0, was released in August 1981. It consisted of 4000 lines of assem-
     bly language source code and ran in 8 Kbytes of memory using the Intel 8086
     microprocessor.
     The IBM PC was an important stage in a continuing revolution in computing
     that has expanded computing from the data center of the 1960s, to the departmental
     minicomputer of the 1970s, and to the desktop in the 1980s. The revolution has con-
     tinued with computing moving into the briefcase in the 1990s, and into our pockets
     during the most recent decade.
     Microsoft's initial OS ran a single application at a time, using a command line
     interface to control the system. It took a long time for Microsoft to develop a true
     GUI interface for the PC; on their third try they succeeded. The 16-bit Windows
     3.0 shipped in 1990 and instantly became successful, selling a million copies in six
     months. Windows 3.0 was implemented as a layer on top of MS-DOS and suffered
     from the limitations of that primitive system. Five years later, Microsoft shipped a
     32-bit version, Windows 95, which was also very successful and led to the develop-
     ment of additional versions: Windows 98 and Windows Me.
     Meanwhile, it had become clear to Microsoft that the MS-DOS platform could
     not sustain a truly modern OS. In 1989 Microsoft hired Dave Cutler, who had devel-
     oped the very successful RSX-11M and VAX/VMS operating systems at Digital
     Equipment Corporation. Cutler's charter was to develop a modern OS, which was
     portable to architectures other than the Intel x86 family, and yet compatible with
     the OS/2 system that Microsoft was jointly developing with IBM, as well as the port-
     able UNIX standard, POSIX. This system was christened NT (New Technology).
     The first version of Windows NT (3.1) was released in 1993, with the same GUI
     as Windows 3.1, the follow-on to Windows 3.0. However, NT 3.1 was a new 32-bit
     OS with the ability to support older DOS and Windows applications as well as pro-
     vide OS/2 support. Several versions of NT 3.x followed with support for additional
     hardware platforms. In 1996, Microsoft released NT 4.0 with the same user interface
     as Windows 95. In 2000, Microsoft introduced the next major upgrade of the NT OS:
     Windows 2000. The underlying Executive and Kernel architecture is fundamentally
     the same as in NT 3.1, but new features have been added. The emphasis in Windows
     2000 was the addition of services and functions to support distributed processing.
     The central element of Windows 2000's new features was Active Directory, which
     is a distributed directory service able to map names of arbitrary objects to any kind
     of information about those objects. Windows 2000 also added the plug-and-play
     and power-management facilities that were already in Windows 98, the successor to
     Windows 95. These features are particularly important for laptop computers.
     In 2001, a new desktop version of NT was released, known as Windows XP.
     The goal of Windows XP was to finally replace the versions of Windows based on
     MS-DOS with an OS based on NT. In 2007, Microsoft shipped Windows Vista for
     the desktop and a short time later, Windows Server 2008. In 2009, they shipped

Windows 7 and Windows Server 2008 R2. Despite the difference in naming, the
client and server versions of these systems use many of the same files, but with addi-
tional features and capabilities enabled for servers.
Over the years, NT has attempted to support multiple processor architectures;
the Intel i860 was the original target for NT as well as the x86. Subsequently, NT
added support for the Digital Alpha architecture, the PowerPC, and the MIPS.
Later came the Intel IA64 (Itanium) and the 64-bit version of the x86, based on
the AMD64 processor architecture. Windows 7 supports only x86 and AMD64.
Windows Server 2008 R2 supports only AMD64 and IA64--but Microsoft has
announced that it will end support for IA64 in future releases. All the other proces-
sor architectures have failed in the market, and today only the x86, AMD64, and
ARM architectures are viable. Microsoft's support for ARM is limited to their
Windows CE OS, which runs on phones and handheld devices. Windows CE has
little relationship to the NT-based Windows that runs on slates, netbooks/laptops,
desktops, and servers.
Microsoft has announced that it is developing a version of NT that targets
cloud computing: Windows Azure. Azure includes a number of features that are
specific to the requirements of public and private clouds. Though it is closely related
to Windows Server, it does not share files in the same way that the Windows client
and server versions do.
The Modern OS
Modern operating systems, such as today's Windows and UNIX (with all its flavors
like Solaris, Linux, and MacOS X), must exploit the capabilities of all the billions
of transistors on each silicon chip. They must work with multiple 32-bit and 64-bit
CPUs, with adjunct GPUs, DSPs, and fixed function units. They must provide sup-
port for sophisticated input/output (multiple touch-sensitive displays, cameras,
microphones, biometric and other sensors) and handle a variety of data challenges
(streaming media, photos, scientific number crunching, search queries)--all while
giving a human being a responsive, real-time experience with the computing system.
To handle these requirements, the computer cannot be doing only one thing
at a time. Unlike the early days of the PC, when the OS ran a single application at
a time, hundreds of activities are taking place to provide the modern computing
experience. The OS can no longer just switch to the application and step away until
it is needed; it must aggressively manage the system and coordinate between all the
competing computations that are taking place often simultaneously on the multiple
CPUs, GPUs, and DSPs that may be present in a modern computing environment.
Thus all modern operating systems have multitasking capability, even though they
may be acting on behalf of only a single human being (called the user).
Windows is a sophisticated multitasking OS, designed to manage the com-
plexity of the modern computing environment, provide a rich platform for appli-
cation developers, and support a rich set of experiences for users. Like Solaris,
Windows is designed to have the features that enterprises need, while at the same
time Windows, like MacOS, provides the simplicity and ease-of-use that consumers
require. In the following sections we will present an overview of the fundamental
structure and capabilities of Windows.

    Architecture
    Figure 2.15 illustrates the overall structure of Windows 7; all releases of Windows
    based on NT have essentially the same structure at this level of detail.
    As with virtually all operating systems, Windows separates application-oriented
    software from the core OS software. The latter, which includes the Executive, the
    Kernel, device drivers, and the hardware abstraction layer, runs in kernel mode.
    Kernel mode software has access to system data and to the hardware. The remaining
    software, running in user mode, has limited access to system data.
    OPERATING SYSTEM ORGANIZATION                                                                     Windows has a highly modular architecture.
    Each system function is managed by just one component of the OS. The rest of the
    OS and all applications access that function through the responsible component using
    standard interfaces. Key system data can only be accessed through the appropriate
              System support                                            Service processes                                                                   Applications
                 processes
    Service control                                            SVChost.exe
              manager                                                                                                                              Task manager                                                                      Environment
              Lsass                                            Winmgmt.exe                                                                                                                                                           subsystems
                                                                                                                                                   Windows
    Winlogon                                                   Spooler                                                                             explorer                                                                          POSIX
    Session                                    Services.exe                                                                                        User
    manager                                                                                                                        application
                                                                                                                                   Subsytem DLLs                                                                                     Win32
                                                                                                               Ntdll.dll
    System                                                                             User mode
    threads
                                                                                       Kernel mode
                                                                                       System service dispatcher
                                               (Kernel-mode callable interfaces)
                                                                                                                                                                                                                                     Win32 USER,
    I/O manager             File system cache                                                                  Security reference                                          manager (registry)                                               GDI
    Device                                     Object manager  manager  Plug-and-play  Power manager  monitor                      Virtual memory  threads  Processes and                      Configuration  call  Local procedure
    and file                                                                                                                                                                                                                                Graphics
    system                                                                                                                                                                                                                                  drivers
    drivers
                                                                                                      Kernel
                                                               Hardware abstraction layer (HAL)
    Lsass = local security authentication server                                                                                   Colored area indicates Executive
    POSIX = portable operating system interface
    GDI = graphics device interface
    DLL = dynamic link libraries
    Figure 2.15  Windows and Windows Vista Architecture [RUSS11]

function. In principle, any module can be removed, upgraded, or replaced without
rewriting the entire system or its standard application program interfaces (APIs).
   The kernel-mode components of Windows are the following:
  Executive: Contains the core OS services, such as memory management, pro-
   cess and thread management, security, I/O, and interprocess communication.
  Kernel: Controls execution of the processors. The Kernel manages thread
   scheduling, process switching, exception and interrupt handling, and multi-
   processor synchronization. Unlike the rest of the Executive and the user level,
   the Kernel's own code does not run in threads.
  Hardware abstraction layer (HAL): Maps between generic hardware com-
   mands and responses and those unique to a specific platform. It isolates
   the OS from platform-specific hardware differences. The HAL makes each
   computer's system bus, direct memory access (DMA) controller, inter-
   rupt controller, system timers, and memory controller look the same to the
   Executive and Kernel components. It also delivers the support needed for
   SMP, explained subsequently.
  Device  drivers:  Dynamic    libraries  that  extend  the  functionality  of      the
   Executive. These include hardware device drivers that translate user I/O func-
   tion calls into specific hardware device I/O requests and software components
   for implementing file systems, network protocols, and any other system exten-
   sions that need to run in kernel mode.
  Windowing and graphics system: Implements the GUI functions, such as deal-
   ing with windows, user interface controls, and drawing.
   The Windows Executive includes components for specific system functions
and provides an API for user-mode software. Following is a brief description of
each of the Executive modules:
  I/O manager: Provides a framework through which I/O devices are accessible
   to applications, and is responsible for dispatching to the appropriate device
   drivers for further processing. The I/O manager implements all the Windows
   I/O APIs and enforces security and naming for devices, network protocols,
   and file systems (using the object manager). Windows I/O is discussed in
   Chapter 11.
  Cache manager: Improves the performance of file-based I/O by causing
   recently referenced file data to reside in main memory for quick access, and
   deferring disk writes by holding the updates in memory for a short time before
   sending them to the disk in more efficient batches.
  Object manager: Creates, manages, and deletes Windows Executive objects
   that are used to represent resources such as processes, threads, and synchroni-
   zation objects. It enforces uniform rules for retaining, naming, and setting the
   security of objects. The object manager also creates the entries in each proc-
   esses' handle table, which consist of access control information and a pointer
   to the object. Windows objects are discussed later in this section.
  Plug-and-play manager: Determines which drivers are required to support a
   particular device and loads those drivers.

      Power manager: Coordinates power management among various devices
       and can be configured to reduce power consumption by shutting down idle
       devices, putting the processor to sleep, and even writing all of memory to disk
       and shutting off power to the entire system.
      Security reference monitor: Enforces access-validation and audit-generation
       rules. The Windows object-oriented model allows for a consistent and uni-
       form view of security, right down to the fundamental entities that make up the
       Executive. Thus, Windows uses the same routines for access validation and
       for audit checks for all protected objects, including files, processes, address
       spaces, and I/O devices. Windows security is discussed in Chapter 15.
      Virtual memory manager: Manages virtual addresses, physical memory, and
       the paging files on disk. Controls the memory management hardware and data
       structures which map virtual addresses in the process's address space to physi-
       cal pages in the computer's memory. Windows virtual memory management is
       described in Chapter 8.
      Process/thread manager: Creates, manages, and deletes process and thread
       objects. Windows process and thread management are described in Chapter 4.
      Configuration manager: Responsible for implementing and managing the
       system registry, which is the repository for both system-wide and per-user
       settings of various parameters.
      Advanced local procedure call (ALPC) facility: Implements an efficient cross-
       process procedure call mechanism for communication between local processes
       implementing services and subsystems. Similar to the remote procedure call
       (RPC) facility used for distributed processing.
    USER-MODE PROCESSES  Four basic types of user-mode processes are supported
    by Windows:
      Special system processes: User-mode services needed to manage the system,
       such as the session manager, the authentication subsystem, the service man-
       ager, and the logon process.
      Service processes: The printer spooler, the event logger, user-mode compo-
       nents that cooperate with device drivers, various network services, and many,
       many others. Services are used by both Microsoft and external software devel-
       opers to extend system functionality as they are the only way to run background
       user-mode activity on a Windows system.
      Environment subsystems: Provide different OS personalities (environments).
       The supported subsystems are Win32 and POSIX. Each environment sub-
       system includes a subsystem process shared among all applications using the
       subsystem and dynamic link libraries (DLLs) that convert the user application
       calls to ALPC calls on the subsystem process, and/or native Windows calls.
      User applications: Executables (EXEs) and DLLs that provide the functional-
       ity users run to make use of the system. EXEs and DLLs are generally targeted
       at a specific environment subsystem; although some of the programs that are
       provided as part of the OS use the native system interfaces (NT API). There is
       also support for running 32-bit programs on 64-bit systems.

   Windows is structured to support applications written for multiple OS personali-
ties. Windows provides this support using a common set of kernel mode components
that underlie the OS environment subsystems. The implementation of each environ-
ment subsystem includes a separate process, which contains the shared data structures,
privileges, and Executive object handles needed to implement a particular personal-
ity. The process is started by the Windows Session Manager when the first application
of that type is started. The subsystem process runs as a system user, so the Executive
will protect its address space from processes run by ordinary users.
   An environment subsystem provides a graphical or command-line user inter-
face that defines the look and feel of the OS for a user. In addition, each subsys-
tem provides the API for that particular environment. This means that applications
created for a particular operating environment need only be recompiled to run on
Windows. Because the OS interface that they see is the same as that for which they
were written, the source code does not need to be modified.
Client/Server Model
The Windows OS services, the environment subsystems, and the applications are
structured using the client/server computing model, which is a common model for
distributed computing and which is discussed in Part Six. This same architecture can
be adopted for use internally to a single system, as is the case with Windows.
   The native NT API is a set of kernel-based services which provide the core
abstractions used by the system, such as processes, threads, virtual memory, I/O,
and communication. Windows provides a far richer set of services by using the
client/server model to implement functionality in user-mode processes. Both the
environment subsystems and the Windows user-mode services are implemented as
processes that communicate with clients via RPC. Each server process waits for a
request from a client for one of its services (e.g., memory services, process creation
services, or networking services). A client, which can be an application program
or another server program, requests a service by sending a message. The message
is routed through the Executive to the appropriate server. The server performs
the requested operation and returns the results or status information by means of
another message, which is routed through the Executive back to the client.
   Advantages of a client/server architecture include the following:
  It simplifies the Executive. It is possible to construct a variety of APIs im-
   plemented in user-mode servers without any conflicts or duplications in the
   Executive. New APIs can be added easily.
  It improves reliability. Each new server runs outside of the kernel, with its
   own partition of memory, protected from other servers. A single server can
   fail without crashing or corrupting the rest of the OS.
  It provides a uniform means for applications to communicate with services via
   RPCs without restricting flexibility. The message-passing process is hidden
   from the client applications by function stubs, which are small pieces of code
   which wrap the RPC call. When an application makes an API call to an envi-
   ronment subsystem or a service, the stub in the client application packages the
   parameters for the call and sends them as a message to the server process that
   implements the call.

      It provides a suitable base for distributed computing. Typically, distributed
       computing makes use of a client/server model, with remote procedure calls
       implemented using distributed client and server modules and the exchange of
       messages between clients and servers. With Windows, a local server can pass
       a message on to a remote server for processing on behalf of local client appli-
       cations. Clients need not know whether a request is being serviced locally or
       remotely. Indeed, whether a request is serviced locally or remotely can change
       dynamically based on current load conditions and on dynamic configuration
       changes.
    Threads and SMP
    Two important characteristics of Windows are its support for threads and for
    symmetric multiprocessing (SMP), both of which were introduced in Section 2.4.
    [RUSS11] lists the following features of Windows that support threads and SMP:
      OS routines can run on any available processor, and different routines can
       execute simultaneously on different processors.
      Windows supports the use of multiple threads of execution within a single
       process. Multiple threads within the same process may execute on different
       processors simultaneously.
      Server processes may use multiple threads to process requests from more than
       one client simultaneously.
      Windows provides mechanisms for sharing data and resources between proc-
       esses and flexible interprocess communication capabilities.
    Windows Objects
    Though the core of Windows is written in C, the design principles followed draw
    heavily on the concepts of object-oriented design. This approach facilitates the shar-
    ing of resources and data among processes and the protection of resources from
    unauthorized access. Among the key object-oriented concepts used by Windows are
    the following:
      Encapsulation: An object consists of one or more items of data, called
       attributes, and one or more procedures that may be performed on those data,
       called services. The only way to access the data in an object is by invoking one
       of the object's services. Thus, the data in the object can easily be protected
       from unauthorized use and from incorrect use (e.g., trying to execute a non-
       executable piece of data).
      Object class and instance: An object class is a template that lists the attributes
       and services of an object and defines certain object characteristics. The OS can
       create specific instances of an object class as needed. For example, there is a
       single process object class and one process object for every currently active
       process. This approach simplifies object creation and management.
      Inheritance: Although the implementation is hand coded, the Executive uses
       inheritance to extend object classes by adding new features. Every Executive

   class is based on a base class which specifies virtual methods that support
   creating, naming, securing, and deleting objects. Dispatcher objects are
   Executive objects that inherit the properties of an event object, so they can
   use common synchronization methods. Other specific object types, such as the
   device class, allow classes for specific devices to inherit from the base class,
   and add additional data and methods.
  Polymorphism: Internally, Windows uses a common set of API functions to
   manipulate objects of any type; this is a feature of polymorphism, as defined
   in Appendix D. However, Windows is not completely polymorphic because
   there are many APIs that are specific to a single object type.
The reader unfamiliar with object-oriented concepts should review Appendix D.
   Not all entities in Windows are objects. Objects are used in cases where data
are intended for user mode access or when data access is shared or restricted.
Among the entities represented by objects are files, processes, threads, semaphores,
timers, and graphical windows. Windows creates and manages all types of objects in
a uniform way, via the object manager. The object manager is responsible for creat-
ing and destroying objects on behalf of applications and for granting access to an
object's services and data.
   Each object within the Executive, sometimes referred to as a kernel object
(to distinguish from user-level objects not of concern to the Executive), exists as
a memory block allocated by the kernel and is directly accessible only by kernel
mode components. Some elements of the data structure (e.g., object name, security
parameters, usage count) are common to all object types, while other elements are
specific to a particular object type (e.g., a thread object's priority). Because these
object data structures are in the part of each process's address space accessible only
by the kernel, it is impossible for an application to reference these data structures
and read or write them directly. Instead, applications manipulate objects indirectly
through the set of object manipulation functions supported by the Executive. When
an object is created, the application that requested the creation receives back a
handle for the object. In essence, a handle is an index into a per-process Executive
table containing a pointer to the referenced object. This handle can then be used
by any thread within the same process to invoke Win32 functions that work with
objects, or can be duplicated into other processes.
   Objects may have security information associated with them, in the form
of a Security Descriptor (SD). This security information can be used to restrict
access to the object based on contents of a token object which describes a par-
ticular user. For example, a process may create a named semaphore object with
the intent that only certain users should be able to open and use that semaphore.
The SD for the semaphore object can list those users that are allowed (or denied)
access to the semaphore object along with the sort of access permitted (read,
write, change, etc.).
   In Windows, objects may be either named or unnamed. When a process
creates an unnamed object, the object manager returns a handle to that object, and
the handle is the only way to refer to it. Handles can be inherited by child processes,
or duplicated between processes. Named objects are also given a name that other
unrelated processes can use to obtain a handle to the object. For example, if proc-

Table 2.4  Windows Kernel Control Objects
Asynchronous Procedure Call       Used to break into the execution of a specified thread and to cause a
                                  procedure to be called in a specified processor mode.
Deferred Procedure Call           Used to postpone interrupt processing to avoid delaying hardware inter-
                                  rupts. Also used to implement timers and interprocessor communication.
Interrupt                         Used to connect an interrupt source to an interrupt service routine by
                                  means of an entry in an Interrupt Dispatch Table (IDT). Each processor
                                  has an IDT that is used to dispatch interrupts that occur on that processor.
Process                           Represents the virtual address space and control information necessary
                                  for the execution of a set of thread objects. A process contains a pointer to
                                  an address map, a list of ready threads containing thread objects, a list of
                                  threads belonging to the process, the total accumulated time for all threads
                                  executing within the process, and a base priority.
Thread                            Represents thread objects, including scheduling priority and quantum, and
                                  which processors the thread may run on.
Profile                           Used to measure the distribution of run time within a block of code. Both
                                  user and system code can be profiled.
           ess A wishes to synchronize with process B, it could create a named event object
           and pass the name of the event to B. Process B could then open and use that event
           object. However, if A simply wished to use the event to synchronize two threads
           within itself, it would create an unnamed event object, because there is no need for
           other processes to be able to use that event.
              There are two categories of objects used by Windows for synchronizing the
           use of the processor:
             Dispatcher objects: The subset of Executive objects which threads can wait on
              to control the dispatching and synchronization of thread-based system opera-
              tions. These are described in Chapter 6.
             Control objects: Used by the Kernel component to manage the operation of
              the processor in areas not managed by normal thread scheduling. Table 2.4
              lists the Kernel control objects.
              Windows is not a full-blown object-oriented OS. It is not implemented in
           an object-oriented language. Data structures that reside completely within one
           Executive component are not represented as objects. Nevertheless, Windows illus-
           trates the power of object-oriented technology and represents the increasing trend
           toward the use of this technology in OS design.
           What Is New in Windows 7
           The core architecture of Windows has been very stable; however, at each release
           there are new features and improvements made even at the lower levels of the sys-
           tem. Many of the changes in Windows are not visible in the features themselves,
           but in the performance and stability of the system. These are due to changes in
           the engineering behind Windows. Other improvements are due to new features, or
           improvements to existing features:

  Engineering improvements: The performance of hundreds of key scenarios,
   such as opening a file from the GUI, are tracked and continuously character-
   ized to identify and fix problems. The system is now built in layers which can
   be separately tested, improving modularity and reducing complexity.
  Performance improvements: The amount of memory required has been
   reduced, both for clients and servers. The VMM is more aggressive about
   limiting the memory use of runaway processes (see Section 8.5). Background
   processes can arrange to start upon an event trigger, such as a plugging in a
   camera, rather than running continuously.
  Reliability improvements: The user-mode heap is more tolerant of memory
   allocation errors by C/C++ programmers, such as continuing to use memory
   after it is freed. Programs that make such errors are detected and the heap
   allocation policies are modified for that program to defer freeing memory and
   avoid corruption of the program's data.
  Energy efficiency: Many improvements have been made to the energy effi-
   ciency of Windows. On servers, unused processors can be "parked," reducing
   their energy use. All Windows systems are more efficient in how the timers
   work; avoiding timer interrupts and the associated background activity allows
   the processors to remain idle longer, which allows modern processors to con-
   sume less energy. Windows accomplishes this by coalescing timer interrupts
   into batches.
  Security: Windows 7 builds on the security features in Windows Vista, which
   added integrity levels to the security model, provided BitLocker volume
   encryption (see Section 15.6), and limited privileged actions by ordinary users.
   BitLocker is now easier to set up and use, and privileged actions result in
   many fewer annoying GUI pop-ups.
  Thread improvements: The most interesting Windows 7 changes were in the
   Kernel. The number of logical CPUs available on each system is growing
   dramatically. Previous versions of Windows limited the number of CPUs to
   64, because of the bitmasks used to represent values like processor affinity
   (see Section 4.4). Windows 7 can support hundreds of CPUs. To ensure that
   the performance of the system scaled with the number of CPUs, major
   improvements were made to the Kernel-scheduling code to break apart locks
   and reduce contention. As the number of available CPUs increase, new
   programming environments are being developed to support the finer-grain
   parallelism than is available with threads. Windows 7 supports a form of User-
   Mode Scheduling which separates the user-mode and kernel-mode portions
   of threads, allowing the user-mode portions to yield the CPU without enter-
   ing the Kernel scheduler. Finally, Windows Server 2008 R2 introduced
   Dynamic Fair Share Scheduling (DFSS) to allow multiuser servers to limit
   how much one user can interfere with another. DFSS keeps a user with
   20 running threads from getting twice as much processor time as a user with
   only 10 running threads.

2.8  TRADITIONAL UNIX SYSTEMS
     History
     The history of UNIX is an oft-told tale and will not be repeated in great detail here.
     Instead, we provide a brief summary.
        UNIX was initially developed at Bell Labs and became operational on a
     PDP-7 in 1970. Some of the people involved at Bell Labs had also participated in
     the time-sharing work being done at MIT's Project MAC. That project led to the
     development of first CTSS and then Multics. Although it is common to say that
     the original UNIX was a scaled-down version of Multics, the developers of UNIX
     actually claimed to be more influenced by CTSS [RITC78]. Nevertheless, UNIX
     incorporated many ideas from Multics.
        Work on UNIX at Bell Labs, and later elsewhere, produced a series of versions
     of UNIX. The first notable milestone was porting the UNIX system from the PDP-7 to
     the PDP-11. This was the first hint that UNIX would be an OS for all computers. The
     next important milestone was the rewriting of UNIX in the programming language
     C. This was an unheard-of strategy at the time. It was generally felt that something as
     complex as an OS, which must deal with time-critical events, had to be written exclu-
     sively in assembly language. Reasons for this attitude include the following:
       Memory (both RAM and secondary store) was small and expensive by today's
        standards, so effective use was important. This included various techniques for
        overlaying memory with different code and data segments, and self-modifying
        code.
       Even though compilers had been available since the 1950s, the computer
        industry was generally skeptical of the quality of automatically generated
        code. With resource capacity small, efficient code, both in terms of time and
        space, was essential.
       Processor and bus speeds were relatively slow, so saving clock cycles could
        make a substantial difference in execution time.
        The C implementation demonstrated the advantages of using a high-level
     language for most if not all of the system code. Today, virtually all UNIX imple-
     mentations are written in C.
        These early versions of UNIX were popular within Bell Labs. In 1974, the
     UNIX system was described in a technical journal for the first time [RITC74]. This
     spurred great interest in the system. Licenses for UNIX were provided to commer-
     cial institutions as well as universities. The first widely available version outside Bell
     Labs was Version 6, in 1976. The follow-on Version 7, released in 1978, is the ances-
     tor of most modern UNIX systems. The most important of the non-AT&T systems
     to be developed was done at the University of California at Berkeley, called UNIX
     BSD (Berkeley Software Distribution), running first on PDP and then VAX com-
     puters. AT&T continued to develop and refine the system. By 1982, Bell Labs had
     combined several AT&T variants of UNIX into a single system, marketed com-
     mercially as UNIX System III. A number of features was later added to the OS to
     produce UNIX System V.

Description
Figure 2.16 provides a general description of the classic UNIX architecture. The
underlying hardware is surrounded by the OS software. The OS is often called the
system kernel, or simply the kernel, to emphasize its isolation from the user and appli-
cations. It is the UNIX kernel that we will be concerned with in our use of UNIX as
an example in this book. UNIX also comes equipped with a number of user services
and interfaces that are considered part of the system. These can be grouped into
the shell, other interface software, and the components of the C compiler (compiler,
assembler, loader). The layer outside of this consists of user applications and the user
interface to the C compiler.
A closer look at the kernel is provided in Figure 2.17. User programs can
invoke OS services either directly or through library programs. The system call
interface is the boundary with the user and allows higher-level software to gain
access to specific kernel functions. At the other end, the OS contains primitive rou-
tines that interact directly with the hardware. Between these two interfaces, the
system is divided into two main parts, one concerned with process control and the
other concerned with file management and I/O. The process control subsystem is
responsible for memory management, the scheduling and dispatching of processes,
and the synchronization and interprocess communication of processes. The file sys-
tem exchanges data between memory and external devices either as a stream of
characters or in blocks. To achieve this, a variety of device drivers are used. For
block-oriented transfers, a disk cache approach is used: A system buffer in main
memory is interposed between the user address space and the external device.
The description in this subsection has dealt with what might be termed
traditional UNIX systems; [VAHA96] uses this term to refer to System V Release
3 (SVR3), 4.3BSD, and earlier versions. The following general statements may be
                              UNIX commands
                              and libraries
                              System call
                              interface
                              Kernel
                              Hardware
                              User-written
                              applications
             Figure 2.16      General UNIX Architecture

                                       User programs
                                 Trap
                                                                     Libraries
                   User level
                   Kernel level
                                              System call interface
                                                                                Interprocess
                                                                                communication
                   File subsystem                          Process
                                                           control              Scheduler
                                                           subsystem
                                   Buffer cache                                 Memory
                                                                                management
                   Character           Block
                   Device drivers
                                                 Hardware control
                   Kernel level
                   Hardware level
                                                 Hardware
                   Figure 2.17     Traditional UNIX Kernel
     made about a traditional UNIX system. It is designed to run on a single processor
     and lacks the ability to protect its data structures from concurrent access by multiple
     processors. Its kernel is not very versatile, supporting a single type of file system,
     process scheduling policy, and executable file format. The traditional UNIX kernel
     is not designed to be extensible and has few facilities for code reuse. The result is
     that, as new features were added to the various UNIX versions, much new code had
     to be added, yielding a bloated and unmodular kernel.
2.9  MODERN UNIX SYSTEMS
     As UNIX evolved, the number of different implementations proliferated, each pro-
     viding some useful features. There was a need to produce a new implementation that
     unified many of the important innovations, added other modern OS design features,
     and produced a more modular architecture. Typical of the modern UNIX kernel is
     the architecture depicted in Figure 2.18. There is a small core of facilities, written in

                                          coff
                                 a.out              elf
                                          exec
                                          switch
File mappings                                                      NFS
                                                                              FFS
Device               Virtual                                vnode/vfs
mappings             memory                                 interface
                     framework                                                     s5fs
Anonymous
mappings                                                                   RFS
                                        Common
                                        facilities
Disk driver                                                                   Time-sharing
                     Block                                  Scheduler         processes
                     device                                 framework
                     switch
        Tape driver                                                System
                                          Streams                  processes
                                 Network            tty
                                 driver             driver
Figure 2.18  Modern UNIX Kernel
a modular fashion, that provide functions and services needed by a number of OS
processes. Each of the outer circles represents functions and an interface that may
be implemented in a variety of ways.
             We now turn to some examples of modern UNIX systems.
System V Release 4 (SVR4)
SVR4, developed jointly by AT&T and Sun Microsystems, combines features from
SVR3, 4.3BSD, Microsoft Xenix System V, and SunOS. It was almost a total rewrite
of the System V kernel and produced a clean, if complex, implementation. New fea-
tures in the release include real-time processing support, process scheduling classes,
dynamically allocated data structures, virtual memory management, virtual file sys-
tem, and a preemptive kernel.
             SVR4 draws on the efforts of both commercial and academic designers and
was developed to provide a uniform platform for commercial UNIX deployment. It
has succeeded in this objective and is perhaps the most important UNIX variant. It
incorporates most of the important features ever developed on any UNIX system

    and does so in an integrated, commercially viable fashion. SVR4 runs on processors
    ranging from 32-bit microprocessors up to supercomputers.
    BSD
    The Berkeley Software Distribution (BSD) series of UNIX releases have played
    a key role in the development of OS design theory. 4.xBSD is widely used in aca-
    demic installations and has served as the basis of a number of commercial UNIX
    products. It is probably safe to say that BSD is responsible for much of the popular-
    ity of UNIX and that most enhancements to UNIX first appeared in BSD versions.
         4.4BSD was the final version of BSD to be released by Berkeley, with the
    design and implementation organization subsequently dissolved. It is a major
    upgrade to 4.3BSD and includes a new virtual memory system, changes in the ker-
    nel structure, and a long list of other feature enhancements.
         One of the most widely used and best documented versions of BSD is
    FreeBSD. FreeBSD is popular for Internet-based servers and firewalls and is used
    in a number of embedded systems.
         The latest version of the Macintosh OS, Mac OS X, is based on FreeBSD 5.0
    and the Mach 3.0 microkernel.
    Solaris 10
    Solaris is Sun's SVR4-based UNIX release, with the latest version being 10. Solaris
    provides all of the features of SVR4 plus a number of more advanced features, such
    as a fully preemptable, multithreaded kernel, full support for SMP, and an object-
    oriented interface to file systems. Solaris is the most widely used and most successful
    commercial UNIX implementation.
2.10 LINUX
    History
    Linux started out as a UNIX variant for the IBM PC (Intel 80386) architecture.
    Linus Torvalds, a Finnish student of computer science, wrote the initial version.
    Torvalds posted an early version of Linux on the Internet in 1991. Since then, a
    number of people, collaborating over the Internet, have contributed to the devel-
    opment of Linux, all under the control of Torvalds. Because Linux is free and the
    source code is available, it became an early alternative to other UNIX workstations,
    such as those offered by Sun Microsystems and IBM. Today, Linux is a full-featured
    UNIX system that runs on all of these platforms and more, including Intel Pentium
    and Itanium, and the Motorola/IBM PowerPC.
         Key to the success of Linux has been the availability of free software packages
    under the auspices of the Free Software Foundation (FSF). FSF's goal is stable,
    platform-independent software that is free, high quality, and embraced by the user
    community. FSF's GNU project3 provides tools for software developers, and the
    3GNU is a recursive acronym for GNU's Not Unix. The GNU project is a free software set of packages
    and tools for developing a UNIX-like operating system; it is often used with the Linux kernel.

GNU Public License (GPL) is the FSF seal of approval. Torvalds used GNU tools
in developing his kernel, which he then released under the GPL. Thus, the Linux
distributions that you see today are the product of FSF's GNU project, Torvald's
individual effort, and the efforts of many collaborators all over the world.
   In addition to its use by many individual programmers, Linux has now made
significant penetration into the corporate world. This is not only because of the
free software, but also because of the quality of the Linux kernel. Many talented
programmers have contributed to the current version, resulting in a technically
impressive product. Moreover, Linux is highly modular and easily configured. This
makes it easy to squeeze optimal performance from a variety of hardware platforms.
Plus, with the source code available, vendors can tweak applications and utilities to
meet specific requirements. Throughout this book, we will provide details of Linux
kernel internals based on the most recent version, Linux 2.6.
Modular Structure
Most UNIX kernels are monolithic. Recall from earlier in this chapter that a mono-
lithic kernel is one that includes virtually all of the OS functionality in one large
block of code that runs as a single process with a single address space. All the func-
tional components of the kernel have access to all of its internal data structures
and routines. If changes are made to any portion of a typical monolithic OS, all the
modules and routines must be relinked and reinstalled and the system rebooted
before the changes can take effect. As a result, any modification, such as adding
a new device driver or file system function, is difficult. This problem is especially
acute for Linux, for which development is global and done by a loosely associated
group of independent programmers.
   Although Linux does not use a microkernel approach, it achieves many of
the potential advantages of this approach by means of its particular modular archi-
tecture. Linux is structured as a collection of modules, a number of which can be
automatically loaded and unloaded on demand. These relatively independent blocks
are referred to as loadable modules [GOYE99]. In essence, a module is an object
file whose code can be linked to and unlinked from the kernel at runtime. Typically,
a module implements some specific function, such as a file system, a device driver,
or some other feature of the kernel's upper layer. A module does not execute as its
own process or thread, although it can create kernel threads for various purposes
as necessary. Rather, a module is executed in kernel mode on behalf of the current
process.
   Thus, although Linux may be considered monolithic, its modular structure
overcomes some of the difficulties in developing and evolving the kernel.
   The Linux loadable modules have two important characteristics:
  Dynamic linking: A kernel module can be loaded and linked into the kernel
   while the kernel is already in memory and executing. A module can also be
   unlinked and removed from memory at any time.
  Stackable modules: The modules are arranged in a hierarchy. Individual
   modules serve as libraries when they are referenced by client modules higher
   up in the hierarchy, and as clients when they reference modules further down.

                 Dynamic linking [FRAN97] facilitates configuration and saves kernel memory.
    In Linux, a user program or user can explicitly load and unload kernel modules
    using the insmod and rmmod commands. The kernel itself monitors the need for
    particular functions and can load and unload modules as needed. With stackable
    modules, dependencies between modules can be defined. This has two benefits:
           1.    Code common to a set of similar modules (e.g., drivers for similar hardware)
                 can be moved into a single module, reducing replication.
           2.    The kernel can make sure that needed modules are present, refraining from
                 unloading a module on which other running modules depend, and loading any
                 additional required modules when a new module is loaded.
                 Figure 2.19 is an example that illustrates the structures used by Linux to
    manage modules. The figure shows the list of kernel modules after only two modules
    have been loaded: FAT and VFAT. Each module is defined by two tables, the mod-
    ule table and the symbol table. The module table includes the following elements:
                *next: Pointer to the following module. All modules are organized into a
                 linked list. The list begins with a pseudomodule (not shown in Figure 2.19).
                *name: Pointer to module name
                size: Module size in memory pages
                usecount: Module usage counter. The counter is incremented when an opera-
                 tion involving the module's functions is started and decremented when the
                 operation terminates.
    Module                                             Module
    *next                                              *next
    *name                                              *name
    size                                               size
    usecount                                           usecount
    flags                                              flags
    nysms                                              nysms
    ndeps                                              ndeps
    *syms        FAT                                   *syms               VFAT
    *deps                                              *deps
    *refs                                              *refs
                      Symbol_table                                               Symbol_table
                      value                                                      value
                      *name                                                      *name
                      value                                                      value
                      *name                                                      *name
                      value                                                      value
                      *name                                                      *name
Figure 2.19      Example List of Linux Kernel Modules

            flags: Module flags
            nsyms: Number of exported symbols
            ndeps: Number of referenced modules
            *syms: Pointer to this module's symbol table.
            *deps: Pointer to list of modules that are referenced by this module.
            *refs: Pointer to list of modules that use this module.
             The symbol table defines those symbols controlled by this module that are
used elsewhere.
             Figure 2.19 shows that the VFAT module was loaded after the FAT module
and that the VFAT module is dependent on the FAT module.
Kernel Components
Figure 2.20, taken from [MOSB02], shows the main components of the Linux kernel
as implemented on an IA-64 architecture (e.g., Intel Itanium). The figure shows
several processes running on top of the kernel. Each box indicates a separate pro-
cess, while each squiggly line with an arrowhead represents a thread of execution.4
The kernel itself consists of an interacting collection of components, with arrows
             Processes                                                                                   User level
             Signals                           System calls
                                  Processes
                        & scheduler                          File        Network
             Virtual                                         systems     protocols                       Kernel
             memory
                                  Char device              Block device  Network
                                  drivers                    drivers     device drivers
Traps &      Physical                          Interrupts
faults       memory
CPU          System               Terminal                   Disk        Network interface               Hardware
             memory                                                      controller
Figure 2.20  Linux Kernel Components
4In Linux, there is no distinction between the concepts of processes and threads. However, multiple
threads in Linux can be grouped together in such a way that, effectively, you can have a single process
comprising multiple threads. These matters are discussed in Chapter 4.

Table 2.5  Some Linux Signals
SIGHUP        Terminal hangup                   SIGCONT    Continue
SIGQUIT       Keyboard quit                     SIGTSTP    Keyboard stop
SIGTRAP       Trace trap                        SIGTTOU    Terminal write
SIGBUS        Bus error                         SIGXCPU    CPU limit exceeded
SIGKILL       Kill signal                       SIGVTALRM  Virtual alarm clock
SIGSEGV       Segmentation violation            SIGWINCH   Window size unchanged
SIGPIPT       Broken pipe                       SIGPWR     Power failure
SIGTERM       Termination                       SIGRTMIN   First real-time signal
SIGCHLD       Child status unchanged            SIGRTMAX   Last real-time signal
           indicating the main interactions. The underlying hardware is also depicted as a
           set of components with arrows indicating which kernel components use or control
           which hardware components. All of the kernel components, of course, execute on
           the processor but, for simplicity, these relationships are not shown.
              Briefly, the principal kernel components are the following:
             Signals: The kernel uses signals to call into a process. For example, signals are
              used to notify a process of certain faults, such as division by zero. Table 2.5
              gives a few examples of signals.
             System calls: The system call is the means by which a process requests a specific
              kernel service. There are several hundred system calls, which can be roughly
              grouped into six categories: file system, process, scheduling, interprocess com-
              munication, socket (networking), and miscellaneous. Table 2.6 defines a few
              examples in each category.
             Processes and scheduler: Creates, manages, and schedules processes.
             Virtual memory: Allocates and manages virtual memory for processes.
Table 2.6  Some Linux System Calls
                                      File system Related
close         Close a file descriptor.
link          Make a new name for a file.
open          Open and possibly create a file or device.
read          Read from file descriptor.
write         Write to file descriptor.
                                      Process Related
execve        Execute program.
exit          Terminate the calling process.
getpid        Get process identification.
setuid        Set user identity of the current process.
prtrace       Provides a means by which a parent process may observe and control the execu-
              tion of another process, and examine and change its core image and registers.

Table 2.6    (continued)
                                      Scheduling Related
sched_getparam            Set the scheduling parameters associated with the scheduling policy for the
                          process identified by pid.
sched_get_priority_max    Return the maximum priority value that can be used with the scheduling
                          algorithm identified by policy.
sched_setscheduler        Set both the scheduling policy (e.g., FIFO) and the associated parameters for
                          the process pid.
sched_rr_get_interval     Write into the timespec structure pointed to by the parameter tp the round-
                          robin time quantum for the process pid.
sched_yield               A process can relinquish the processor voluntarily without blocking via this
                          system call. The process will then be moved to the end of the queue for its static
                          priority and a new process gets to run.
                          Interprocess Communication (IPC) Related
msgrcv                    A message buffer structure is allocated to receive a message. The system
                          call then reads a message from the message queue specified by msqid into
                          the newly created message buffer.
semctl                    Perform the control operation specified by cmd on the semaphore set
                          semid.
semop                     Perform operations on selected members of the semaphore set semid.
shmat                     Attach the shared memory segment identified by semid to the data segment
                          of the calling process.
shmctl                    Allow the user to receive information on a shared memory segment; set the
                          owner, group, and permissions of a shared memory segment; or destroy a
                          segment.
                                      Socket (networking) Related
bind                      Assigns the local IP address and port for a socket. Returns 0 for success and 1
                          for error.
connect                   Establish a connection between the given socket and the remote socket
                          associated with sockaddr.
gethostname               Return local host name.
send                      Send the bytes contained in buffer pointed to by *msg over the given
                          socket.
setsockopt                Set the options on a socket
                                            Miscellaneous
create_module             Attempt to create a loadable module entry and reserve the kernel memory that
                          will be needed to hold the module.
fsync                     Copy all in-core parts of a file to disk, and waits until the device reports that all
                          parts are on stable storage.
query_module              Request information related to loadable modules from the kernel.
time                      Return the time in seconds since January 1, 1970.
vhangup                   Simulate a hangup on the current terminal. This call arranges for other users to
                          have a "clean" tty at login time.

       File systems: Provides a global, hierarchical namespace for files, directories,
        and other file related objects and provides file system functions.
       Network protocols: Supports the Sockets interface to users for the TCP/IP
        protocol suite.
       Character device drivers: Manages devices that require the kernel to send or
        receive data one byte at a time, such as terminals, modems, and printers.
       Block device drivers: Manages devices that read and write data in blocks, such
        as various forms of secondary memory (magnetic disks, CD-ROMs, etc.).
       Network device drivers: Manages network interface cards and communica-
        tions ports that connect to network devices, such as bridges and routers.
       Traps and faults: Handles traps and faults generated by the processor, such as
        a memory fault.
       Physical memory: Manages the pool of page frames in real memory and allo-
        cates pages for virtual memory.
       Interrupts: Handles interrupts from peripheral devices.
2.11 LINUX VSERVER VIRTUAL MACHINE ARCHITECTURE
     Linux VServer is an open-source, fast, lightweight approach to implement-
     ing virtual machines on a Linux server [SOLT07, LIGN05]. Only a single copy
     of the Linux kernel is involved. VServer consists of a relatively modest modifi-
     cation to the kernel plus a small set of OS userland5 tools. The VServer Linux
     kernel supports a number of separate virtual servers. The kernel manages all sys-
     tem resources and tasks, including process scheduling, memory, disk space, and
     processor time. This is closer in concept to the process VM rather than the system
     VM of Figure 2.14.
        Each virtual server is isolated from the others using Linux kernel capabilities.
     This provides security and makes it easy to set up multiple virtual machines on a
     single platform. The isolation involves four elements: chroot, chcontext, chbind, and
     capabilities.
        The chroot command is a UNIX or Linux command to make the root directory
     (/) become something other than its default for the lifetime of the current process.
     It can only be run by privileged users and is used to give a process (commonly a net-
     work server such as FTP or HTTP) access to a restricted portion of the file system.
     This command provides file system isolation. All commands executed by the virtual
     server can only affect files that start with the defined root for that server.
        The chcontext Linux utility allocates a new security context and executes
     commands in that context. The usual or hosted security context is the context 0.
     This context has the same privileges as the root user (UID 0): This context can
     see and kill other tasks in the other contexts. Context number 1 is used to view
     5The term userland refers to all application software that runs in user space rather than kernel space. OS
     userland usually refers to the various programs and libraries that the operating system uses to interact
     with the kernel: software that performs input/output, manipulates file system objects, etc.

                                                             Server                    Server
                                                       applications              applications
                                   VMhost                    VM1                       VMn                 Virtual platform
      Hosting platform       VM admin.
                             Remote admin.
                             Core services
                             /dev  /usr  /home  /proc  /dev  /usr  /home  /proc  /dev  /usr  /home  /proc
                                                       Standard OS image
      Figure 2.21                  Linux VServer Architecture
      other contexts but cannot affect them. All other contexts provide complete isola-
      tion: Processes from one context can neither see nor interact with processes from
      another context. This provides the ability to run similar contexts on the same com-
      puter without any interaction possible at the application level. Thus, each virtual
      server has its own execution context that provides process isolation.
      The chbind utility executes a command, and locks the resulting process and
      its children into using a specific IP address. Once called, all packets sent out by this
      virtual server through the system's network interface are assigned the sending IP
      address derived from the argument given to chbind. This system call provides net-
      work isolation: Each virtual server uses a separate and distinct IP address. Incoming
      traffic intended for one virtual server cannot be accessed by other virtual servers.
      Finally, each virtual server is assigned a set of capabilities. The concept of
      capabilities, as used in Linux, refers to a partitioning of the privileges available to
      a root user, such as the ability to read files or to trace processes owned by another
      user. Thus, each virtual server can be assigned a limited subset of the root user's
      privileges. This provides root isolation. VServer can also set resource limits, such as
      limits to the amount of virtual memory a process may use.
      Figure 2.21, based on [SOLT07], shows the general architecture of Linux
      VServer. VServer provides a shared, virtualized OS image, consisting of a root file
      system, and a shared set of system libraries and kernel services. Each VM can be
      booted, shut down, and rebooted independently. Figure 2.21 shows three group-
      ings of software running on the computer system. The hosting platform includes the
      shared OS image and a privileged host VM, whose function is to monitor and man-
      age the other VMs. The virtual platform creates virtual machines and is the view of
      the system seen by the applications running on the individual VMs.
2.12  RECOMMENDED READING AND WEB SITES
      [BRIN01] is an excellent collection of papers covering major advances in OS design
      over the years. [SWAI07] is a provocative and interesting short article on the future
      of operating systems.

     [MUKH96] provides a good discussion of OS design issues for SMPs.
     [CHAP97] contains five articles on recent design directions for multiprocessor
     operating systems. Worthwhile discussions of the principles of microkernel design
     are contained in [LIED95] and [LIED96]; the latter focuses on performance
     issues.
     [LI10] and [SMIT05] provide good treatments of virtual machines.
     An excellent treatment of UNIX internals, which provides a comparative
     analysis of a number of variants, is [VAHA96]. For UNIX SVR4, [GOOD94]
     provides a definitive treatment, with ample technical detail. For the popular open-
     source FreeBSD, [MCKU05] is highly recommended. [MCDO07] provides a good
     treatment of Solaris internals. Good treatments of Linux internals are [LOVE10]
     and [MAUE08].
     Although there are countless books on various versions of Windows, there
     is remarkably little material available on Windows internals. The book to read is
     [RUSS11].
     BRIN01     Brinch Hansen, P. Classic Operating Systems: From Batch Processing to
              Distributed Systems. New York: Springer-Verlag, 2001.
     CHAP97     Chapin, S., and Maccabe, A., eds. "Multiprocessor Operating Systems:
              Harnessing the Power." special issue of IEEE Concurrency, April쵫une 1997.
     GOOD94     Goodheart, B., and Cox, J. The Magic Garden Explained: The Internals of
              UNIX System V Release 4. Englewood Cliffs, NJ: Prentice Hall, 1994.
     LOVE10     Love, R. Linux Kernel Development. Upper Saddle River, NJ: Addison-
              Wesley, 2010.
     LI10     Li, Y.; Li, W.; and Jiang, C. "A Survey of Virtual Machine Systems: Current
              Technology and Future Trends." Proceedings, Third International Symposium on
              Electronic Commerce and Security, 2010.
     LIED95     Liedtke, J. "On -Kernel Construction." Proceedings of the Fifteenth ACM
              Symposium on Operating Systems Principles, December 1995.
     LIED96     Liedtke, J. "Toward Real Microkernels." Communications of the ACM,
              September 1996.
     MAUE08     Mauerer, W. Professional Linux Kernel Architecture. New York: Wiley, 2008.
     MCDO07     McDougall, R., and Mauro, J. Solaris Internals: Solaris 10 and OpenSolaris
              Kernel Architecture. Palo Alto, CA: Sun Microsystems Press, 2007.
     MCKU05     McKusick, M., and Neville-Neil, J. The Design and Implementation of the
              FreeBSD Operating System. Reading, MA: Addison-Wesley, 2005.
     MUKH96     Mukherjee, B., and Karsten, S. "Operating Systems for Parallel Machines."
              In Parallel Computers: Theory and Practice. Edited by T. Casavant, P. Tvrkik, and
              F. Plasil. Los Alamitos, CA: IEEE Computer Society Press, 1996.
     RUSS11     Russinovich, M.; Solomon, D.; and Ionescu, A. Windows Internals: Covering
              Windows 7 and Windows Server 2008 R2. Redmond, WA: Microsoft Press, 2011.
     SMIT05     Smith, J., and Nair, R. "The Architecture of Virtual Machines." Computer,
              May 2005.
     SWAI07     Swaine, M. "Wither Operating Systems?" Dr. Dobb's Journal, March 2007.
     VAHA96     Vahalia, U. UNIX Internals: The New Frontiers. Upper Saddle River, NJ:
              Prentice Hall, 1996.

        Recommended Web sites:
                 The Operating System Resource Center: A useful collection of documents and papers
                  on a wide range of OS topics.
                 Operating System Technical Comparison: Includes a substantial amount of information
                  on a variety of operating systems.
                 ACM Special Interest Group on Operating Systems: Information on SIGOPS publica-
                  tions and conferences.
                 IEEE Technical Committee on Operating Systems and Application Environments:
                  Includes an online newsletter and links to other sites.
                 The comp.os.research FAQ: Lengthy and worthwhile FAQ covering OS design issues.
                 UNIX Guru Universe: Excellent source of UNIX information.
                 Linux Documentation Project: The name describes the site.
                 IBM's Linux Website: Provides a wide range of technical and user information on
                  Linux. Much of it is devoted to IBM products but there is a lot of useful general techni-
                  cal information.
                 Windows Development: Good source of information on Windows internals.
2.13 KEY TERMS, REVIEW QUESTIONS, AND PROBLEMS
Key Terms
batch processing                      multiprogramming                     round robin
batch system                          multitasking                         scheduling
execution context                     multithreading                       serial processing
interrupt                             nucleus                              symmetric multiprocessing
job                                   operating system                     task
job control language                  physical address                     thread
kernel                                privileged instruction               time sharing
memory management                     process                              time-sharing system
microkernel                           process state                        uniprogramming
monitor                               real address                         virtual address
monolithic kernel                     resident monitor                     virtual machine
multiprogrammed batch
     system
        Review Questions
             2.1  What are three objectives of an OS design?
             2.2  What is the kernel of an OS?
             2.3  What is multiprogramming?
             2.4  What is a process?
             2.5  How is the execution context of a process used by the OS?

     2.6   List and briefly explain five storage management responsibilities of a typical OS.
     2.7   Explain the distinction between a real address and a virtual address.
     2.8   Describe the round-robin scheduling technique.
     2.9   Explain the difference between a monolithic kernel and a microkernel.
     2.10  What is multithreading?
     2.11  List the key design issues for an SMP operating system.
     Problems
     2.1   Suppose that we have a multiprogrammed computer in which each job has identical
           characteristics. In one computation period, T, for a job, half the time is spent in I/O
           and the other half in processor activity. Each job runs for a total of N periods. Assume
           that a simple round-robin scheduling is used, and that I/O operations can overlap with
           processor operation. Define the following quantities:
              Turnaround time = actual time to complete a job
              Throughput = average number of jobs completed per time period T
              Processor utilization = percentage of time that the processor is active (not waiting)
           Compute these quantities for one, two, and four simultaneous jobs, assuming that the
           period T is distributed in each of the following ways:
           a.  I/O first half, processor second half
           b.  I/O first and fourth quarters, processor second and third quarter
     2.2   An I/O-bound program is one that, if run alone, would spend more time waiting for
           I/O than using the processor. A processor-bound program is the opposite. Suppose a
           short-term scheduling algorithm favors those programs that have used little processor
           time in the recent past. Explain why this algorithm favors I/O-bound programs and
           yet does not permanently deny processor time to processor-bound programs.
     2.3   Contrast the scheduling policies you might use when trying to optimize a time-sharing
           system with those you would use to optimize a multiprogrammed batch system.
     2.4   What is the purpose of system calls, and how do system calls relate to the OS and to
           the concept of dual-mode (kernel-mode and user-mode) operation?
     2.5   In IBM's mainframe OS, OS/390, one of the major modules in the kernel is the System
           Resource Manager. This module is responsible for the allocation of resources among
           address spaces (processes). The SRM gives OS/390 a degree of sophistication unique
           among operating systems. No other mainframe OS, and certainly no other type of OS,
           can match the functions performed by SRM. The concept of resource includes proces-
           sor, real memory, and I/O channels. SRM accumulates statistics pertaining to utilization
           of processor, channel, and various key data structures. Its purpose is to provide optimum
           performance based on performance monitoring and analysis. The installation sets
           forth various performance objectives, and these serve as guidance to the SRM, which
           dynamically modifies installation and job performance characteristics based on system
           utilization. In turn, the SRM provides reports that enable the trained operator to refine
           the configuration and parameter settings to improve user service.
               This problem concerns one example of SRM activity. Real memory is divided
           into equal-sized blocks called frames, of which there may be many thousands. Each
           frame can hold a block of virtual memory referred to as a page. SRM receives control
           approximately 20 times per second and inspects each and every page frame. If the
           page has not been referenced or changed, a counter is incremented by 1. Over time,
           SRM averages these numbers to determine the average number of seconds that a
           page frame in the system goes untouched. What might be the purpose of this and what
           action might SRM take?
     2.6   A multiprocessor with eight processors has 20 attached tape drives. There is a large
           number of jobs submitted to the system that each require a maximum of four tape

drives to complete execution. Assume that each job starts running with only three
tape drives for a long period before requiring the fourth tape drive for a short period
toward the end of its operation. Also assume an endless supply of such jobs.
a.  Assume the scheduler in the OS will not start a job unless there are four tape
    drives available. When a job is started, four drives are assigned immediately and
    are not released until the job finishes. What is the maximum number of jobs that
    can be in progress at once? What are the maximum and minimum number of tape
    drives that may be left idle as a result of this policy?
b.  Suggest an alternative policy to improve tape drive utilization and at the same
    time avoid system deadlock. What is the maximum number of jobs that can be in
    progress at once? What are the bounds on the number of idling tape drives?

                                       CHAPTER
PROCESS DESCRIPTION
AND CONTROL
     3.1   What Is a Process?
           Background
           Processes and Process Control Blocks
     3.2   Process States
           A Two-State Process Model
           The Creation and Termination of Processes
           A Five-State Model
           Suspended Processes
     3.3   Process Description
           Operating System Control Structures
           Process Control Structures
     3.4   Process Control
           Modes of Execution
           Process Creation
           Process Switching
     3.5   Execution of the Operating System
           Nonprocess Kernel
           Execution within User Processes
           Process-Based Operating System
     3.6   Security Issues
           System Access Threats
           Countermeasures
     3.7   UNIX SVR4 Process Management
           Process States
           Process Description
           Process Control
     3.8   Summary
     3.9   Recommended Reading
     3.10  Key Terms, Review Questions, and Problems
106
