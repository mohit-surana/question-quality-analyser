FILE SYSTEMS


                          4
                          FILE SYSTEMS
All computer applications need to store and retrieve information. While a proc-
ess is running, it can store a limited amount of information within its own address
space. However, the storage capacity is restricted to the size of the virtual address
space. For some applications this size is adequate, but for others, such as airline
reservations, banking, or corporate record keeping, it is far too small.
A second problem with keeping information within a process' address space is
that when the process terminates, the information is lost. For many applications
(e.g., for databases), the information must be retained for weeks, months, or even
forever. Having it vanish when the process using it terminates is unacceptable.
Furthermore, it must not go away when a computer crash kills the process.
A third problem is that it is frequently necessary for multiple processes to ac-
cess (parts of) the information at the same time. If we have an online telephone di-
rectory stored inside the address space of a single process, only that process can
access it. The way to solve this problem is to make the information itself indepen-
dent of any one process.
Thus, we have three essential requirements for long-term information storage:
1.  It must be possible to store a very large amount of information.
2.  The information must survive the termination of the process using it.
3.  Multiple processes must be able to access the information at once.
Magnetic disks have been used for years for this long-term storage.        In recent
years, solid-state drives have become increasingly popular, as they do not have any
                          263



264                           FILE SYSTEMS                                  CHAP. 4
moving parts that may break.  Also, they offer fast random access. Tapes and opti-
cal disks have also been used extensively, but they have much lower performance
and are typically used for backups.    We will study disks more in Chap. 5, but for
the moment, it is sufficient to think of a disk as a linear sequence of fixed-size
blocks and supporting two operations:
     1.  Read block k.
     2.  Write block k
In reality there are more, but with these two operations one could, in principle,
solve the long-term storage problem.
     However, these are very inconvenient operations, especially on large systems
used by many applications and possibly multiple users (e.g., on a server).       Just a
few of the questions that quickly arise are:
     1.  How do you find information?
     2.  How do you keep one user from reading another user's data?
     3.  How do you know which blocks are free?
and there are many more.
     Just as we saw how the operating system abstracted away the concept of the
processor to create the abstraction of a process and how it abstracted away the con-
cept of physical memory to offer processes (virtual) address spaces, we can solve
this problem with a new abstraction: the file. Together, the abstractions of proc-
esses (and threads), address spaces, and files are the most important concepts relat-
ing to operating systems. If you really understand these three concepts from begin-
ning to end, you are well on your way to becoming an operating systems expert.
     Files are logical units of information created by processes. A disk will usually
contain thousands or even millions of them, each one independent of the others.            In
fact, if you think of each file as a kind of address space, you are not that far off, ex-
cept that they are used to model the disk instead of modeling the RAM.
     Processes can read existing files and create new ones if need be. Information
stored in files must be persistent, that is, not be affected by process creation and
termination.  A file should disappear only when its owner explicitly removes it.
Although operations for reading and writing files are the most common ones, there
exist many others, some of which we will examine below.
     Files are managed by the operating system. How they are structured, named,
accessed, used, protected, implemented, and managed are major topics in operating
system design.  As a whole, that part of the operating system dealing with files is
known as the file system and is the subject of this chapter.
     From the user's standpoint, the most important aspect of a file system is how it
appears, in other words, what constitutes a file, how files are named and protected,
what operations are allowed on files, and so on. The details of whether linked lists



SEC. 4.1                                  FILES                                        265
or bitmaps are used to keep track of free storage and how many sectors there are in
a logical disk block are of no interest, although they are of great importance to the
designers of the file system. For this reason, we have structured the chapter as sev-
eral sections.  The first two are concerned with the user interface to files and direc-
tories, respectively. Then comes a detailed discussion of how the file system is im-
plemented and managed. Finally, we give some examples of real file systems.
4.1 FILES
In the following pages we will look at files from the user's point of view, that
is, how they are used and what properties they have.
4.1.1 File Naming
A file is an abstraction mechanism. It provides a way to store information on
the disk and read it back later.  This must be done in such a way as to shield the
user from the details of how and where the information is stored, and how the disks
actually work.
Probably the most important characteristic of any abstraction mechanism is the
way the objects being managed are named, so we will start our examination of file
systems with the subject of file naming. When a process creates a file, it gives the
file a name.    When the process terminates, the file continues to exist and can be ac-
cessed by other processes using its name.
The exact rules for file naming vary somewhat from system to system, but all
current operating systems allow strings of one to eight letters as legal file names.
Thus andrea, bruce, and cathy are possible file names. Frequently digits and spe-
cial characters are also permitted, so names like 2, urgent!, and Fig.2-14 are often
valid as well. Many file systems support names as long as 255 characters.
Some file systems distinguish between upper- and lowercase letters, whereas
others do not.  UNIX falls in the first category; the old MS-DOS falls in the sec-
ond.  (As an aside, while ancient, MS-DOS is still very widely used in embedded
systems, so it is by no means obsolete.)   Thus, a UNIX system can have all of the
following as three distinct files: maria, Maria, and MARIA.  In MS-DOS, all these
names refer to the same file.
An aside on file systems is probably in order here.   Windows 95 and Windows
98 both used the MS-DOS file system, called FAT-16, and thus inherit many of its
properties, such as how file names are constructed.   Windows 98 introduced some
extensions to FAT-16, leading to FAT-32, but these two are quite similar.    In addi-
tion, Windows NT, Windows 2000, Windows XP, Windows Vista, Windows 7, and
Windows 8 all still support both FAT file systems, which are really obsolete now.
However, these newer operating systems also have a much more advanced native
file system (NTFS) that has different properties (such as file names in Unicode). In



266                                FILE SYSTEMS                          CHAP. 4
fact, there is second file system for Windows 8, known as ReFS (or Resilient File
System), but it is targeted at the server version of Windows 8.      In this chapter,
when we refer to the MS-DOS or FAT file systems, we mean FAT-16 and FAT-32
as used on Windows unless specified otherwise. We will discuss the FAT file sys-
tems later in this chapter and NTFS in Chap. 12, where we will examine Windows
8 in detail. Incidentally, there is also a new FAT-like file system, known as exFAT
file system, a Microsoft extension to FAT-32 that is optimized for flash drives and
large file systems.   Exfat is the only modern Microsoft file system that OS X can
both read and write.
       Many operating systems support two-part file names, with the two parts sepa-
rated by a period, as in prog.c.       The part following the period is called the file
extension and usually indicates something about the file.            In MS-DOS, for ex-
ample, file names are 1 to 8 characters, plus an optional extension of 1 to 3 charac-
ters.  In UNIX, the size of the extension, if any, is up to the user, and a file may
even have two or more extensions, as in homepage.html.zip, where .html indicates
a Web page in HTML and .zip indicates that the file (homepage.html) has been
compressed using the zip program. Some of the more common file extensions and
their meanings are shown in Fig. 4-1.
       Extension                               Meaning
       .bak           Backup file
       .c             C source program
       .gif           Compuserve Graphical Interchange Format image
       .hlp           Help file
       .html          World Wide Web HyperText Markup Language document
       .jpg           Still picture encoded with the JPEG standard
       .mp3           Music encoded in MPEG layer 3 audio format
       .mpg           Movie encoded with the MPEG standard
       .o             Object file (compiler output, not yet linked)
       .pdf           Portable Document Format file
       .ps            PostScript file
       .tex           Input for the TEX formatting program
       .txt           General text file
       .zip           Compressed archive
                      Figure 4-1. Some typical file extensions.
       In some systems (e.g., all flavors of UNIX) file extensions are just conventions
and are not enforced by the operating system.  A file named file.txt might be some
kind of text file, but that name is more to remind the owner than to convey any ac-
tual information to the computer.      On the other hand, a C compiler may actually



SEC. 4.1                                     FILES                                         267
insist that files it is to compile end in .c, and it may refuse to compile them if they
do not. However, the operating system does not care.
     Conventions like this are especially useful when the same program can handle
several different kinds of files. The C compiler, for example, can be given a list of
several files to compile and link together, some of them C files and some of them
assembly-language files. The extension then becomes essential for the compiler to
tell which are C files, which are assembly files, and which are other files.
     In contrast, Windows is aware of the extensions and assigns meaning to them.
Users (or processes) can register extensions with the operating system and specify
for each one which program ``owns'' that extension. When a user double clicks on
a file name, the program assigned to its file extension is launched with the file as
parameter. For example, double clicking on file.docx starts Microsoft Word with
file.docx as the initial file to edit.
4.1.2 File Structure
     Files can be structured in any of several ways. Three common possibilities are
depicted in Fig. 4-2.  The file in Fig. 4-2(a) is an unstructured sequence of bytes.
In effect, the operating system does not know or care what is in the file.            All it sees
are bytes. Any meaning must be imposed by user-level programs. Both UNIX and
Windows use this approach.
     1 Byte            1 Record
                                                        Ant   Fox   Pig
                            Cat         Cow  Dog        Goat  Lion  Owl         Pony  Rat  Worm
                                             Hen  Ibis  Lamb
(a)             (b)                                           (c)
     Figure 4-2. Three kinds of files. (a) Byte sequence. (b) Record sequence.
     (c) Tree.
     Having the operating system regard files as nothing more than byte sequences
provides the maximum amount of flexibility. User programs can put anything they
want in their files and name them any way that they find convenient. The operating
system does not help, but it also does not get in the way. For users who want to do



268                          FILE SYSTEMS                                       CHAP. 4
unusual things, the latter can be very important. All versions of UNIX (including
Linux and OS X) and Windows use this file model.
     The first step up in structure isillustrated in Fig. 4-2(b).  In this model, a file is
a sequence of fixed-length records, each with some internal structure. Central to
the idea of a file being a sequence of records is the idea that the read operation re-
turns one record and the write operation overwrites or appends one record.      As a
historical note, in decades gone by, when the 80-column punched card was king of
the mountain, many (mainframe) operating systems based their file systems on
files consisting of 80-character records, in effect, card images. These systems also
supported files of 132-character records, which were intended for the line printer
(which in those days were big chain printers having 132 columns). Programs read
input in units of 80 characters and wrote it in units of 132 characters, although the
final 52 could be spaces, of course.    No current general-purpose system uses this
model as its primary file system any more, but back in the days of 80-column
punched cards and 132-character line printer paper this was a common model on
mainframe computers.
     The third kind of file structure is shown in Fig. 4-2(c).     In this organization, a
file consists of a tree of records, not necessarily all the same length, each con-
taining a key field in a fixed position in the record. The tree is sorted on the key
field, to allow rapid searching for a particular key.
     The basic operation here is not to get the ``next'' record, although that is also
possible, but to get the record with a specific key. For the zoo file of Fig. 4-2(c),
one could ask the system to get the record whose key is pony, for example, without
worrying about its exact position in the file. Furthermore, new records can be add-
ed to the file, with the operating system, and not the user, deciding where to place
them. This type of file is clearly quite different from the unstructured byte streams
used in UNIX and Windows and is used on some large mainframe computers for
commercial data processing.
4.1.3 File Types
     Many operating systems support several types of files. UNIX (again, including
OS X) and Windows, for example, have regular files and directories.             UNIX also
has character and block special files.  Regular files are the ones that contain user
information. All the files of Fig. 4-2 are regular files.  Directories are system files
for maintaining the structure of the file system.        We will study directories below.
Character special files are related to input/output and used to model serial I/O de-
vices, such as terminals, printers, and networks.          Block special files are used to
model disks. In this chapter we will be primarily interested in regular files.
     Regular files are generally either ASCII files or binary files.  ASCII files con-
sist of lines of text.  In some systems each line is terminated by a carriage return
character.  In others, the line feed character is used.    Some systems (e.g., Windows)
use both. Lines need not all be of the same length.



SEC. 4.1                          FILES                                                     269
     The great advantage of ASCII files is that they can be displayed and printed as
is, and they can be edited with any text editor. Furthermore, if large numbers of
programs use ASCII files for input and output, it is easy to connect the output of
one  program  to  the  input  of  another,  as  in  shell  pipelines.  (The  interprocess
plumbing is not any easier, but interpreting the information certainly is if a stan-
dard convention, such as ASCII, is used for expressing it.)
     Other files are binary, which just means that they are not ASCII files. Listing
them on the printer gives an incomprehensible listing full of random junk. Usually,
they have some internal structure known to programs that use them.
     For example, in Fig. 4-3(a) we see a simple executable binary file taken from
an early version of UNIX. Although technically the file is just a sequence of bytes,
the operating system will execute a file only if it has the proper format.   It has five
sections: header, text, data, relocation bits, and symbol table.       The header starts
with a so-called magic number, identifying the file as an executable file (to pre-
vent the accidental execution of a file not in this format).    Then come the sizes of
the various pieces of the file, the address at which execution starts, and some flag
bits. Following the header are the text and data of the program itself.      These are
loaded into memory and relocated using the relocation bits. The symbol table is
used for debugging.
     Our second example of a binary file is an archive, also from UNIX. It consists
of a collection of library procedures (modules) compiled but not linked. Each one
is prefaced by a header telling its name, creation date, owner, protection code, and
size. Just as with the executable file, the module headers are full of binary num-
bers. Copying them to the printer would produce complete gibberish.
     Every operating system must recognize at least one file type: its own executa-
ble file; some recognize more. The old TOPS-20 system (for the DECsystem 20)
went so far as to examine the creation time of any file to be executed. Then it loca-
ted the source file and saw whether the source had been modified since the binary
was made.  If it had been, it automatically recompiled the source.     In UNIX terms,
the make program had been built into the shell. The file extensions were manda-
tory, so it could tell which binary program was derived from which source.
     Having strongly typed files like this causes problems whenever the user does
anything that the system designers did not expect. Consider, as an example, a sys-
tem in which program output files have extension .dat (data files). If a user writes
a program formatter that reads a .c file (C program), transforms it (e.g., by convert-
ing it to a standard indentation layout), and then writes the transformed file as out-
put, the output file will be of type .dat. If the user tries to offer this to the C compi-
ler to compile it, the system will refuse because it has the wrong extension. At-
tempts to copy file.dat to file.c will be rejected by the system as invalid (to protect
the user against mistakes).
     While this kind of ``user friendliness'' may help novices, it drives experienced
users up the wall since they have to devote considerable effort to circumventing the
operating system's idea of what is reasonable and what is not.



270                                 FILE SYSTEMS                                   CHAP. 4
                                                                             Module
         Magic number                                                        name
                                    Header
         Text size
         Data size                                                           Date
Header   BSS size
         Symbol table size          Object                                   Owner
                                    module
         Entry point                                                         Protection
                                                                             Size
         Flags                      Header
         Text
                                    Object
                                    module
         Data                       Header
         Relocation
         bits
                                    Object
         Symbol                     module
         table
         (a)                                      (b)
                       Figure 4-3.  (a) An executable file. (b) An archive.
4.1.4 File Access
        Early operating systems provided only one kind of file access: sequential
access.  In these systems, a process could read all the bytes or records in a file in
order, starting at the beginning, but could not skip around and read them out of
order. Sequential files could be rewound, however, so they could be read as often
as needed. Sequential files were convenient when the storage medium was mag-
netic tape rather than disk.
        When disks came into use for storing files, it became possible to read the bytes
or records of a file out of order, or to access records by key rather than by position.
Files whose bytes or records can be read in any order are called random-access
files. They are required by many applications.



SEC. 4.1                             FILES                                               271
Random access files are essential for many applications, for example, database
systems.  If an airline customer calls up and wants to reserve a seat on a particular
flight, the reservation program must be able to access the record for that flight
without having to read the records for thousands of other flights first.
Two methods can be used for specifying where to start reading.                 In the first
one, every read operation gives the position in the file to start reading at.  In the
second one, a special operation, seek, is provided to set the current position. After
a seek, the file can be read sequentially from the now-current position. The latter
method is used in UNIX and Windows.
4.1.5 File Attributes
Every file has a name and its data.  In addition, all operating systems associate
other information with each file, for example, the date and time the file was last
modified and the file's size.  We will call these extra items the file's attributes.
Some people call them metadata.  The list of attributes varies considerably from
system to system. The table of Fig. 4-4 shows some of the possibilities, but other
ones also exist.   No existing system has all of these, but each one is present in
some system.
The first four attributes relate to the file's protection and tell who may access it
and who may not. All kinds of schemes are possible, some of which we will study
later.  In some systems the user must present a password to access a file, in which
case the password must be one of the attributes.
The flags are bits or short fields that control or enable some specific property.
Hidden files, for example, do not appear in listings of all the files. The archive flag
is a bit that keeps track of whether the file has been backed up recently. The back-
up program clears it, and the operating system sets it whenever a file is changed.
In this way, the backup program can tell which files need backing up.     The tempo-
rary flag allows a file to be marked for automatic deletion when the process that
created it terminates.
The record-length, key-position, and key-length fields are only present in files
whose records can be looked up using a key. They provide the information required
to find the keys.
The various times keep track of when the file was created, most recently ac-
cessed, and most recently modified. These are useful for a variety of purposes. For
example, a source file that has been modified after the creation of the correspond-
ing object file needs to be recompiled. These fields provide the necessary infor-
mation.
The current size tells how big the file is at present. Some old mainframe oper-
ating systems required the maximum size to be specified when the file was created,
in order to let the operating system reserve the maximum amount of storage in ad-
vance. Workstation and personal-computer operating systems are thankfully clever
enough to do without this feature nowadays.



272                         FILE SYSTEMS                                        CHAP. 4
           Attribute                       Meaning
       Protection           Who can access the file and in what way
       Password             Password needed to access the file
       Creator              ID of the person who created the file
       Owner                Current owner
       Read-only flag       0 for read/write; 1 for read only
       Hidden flag          0 for normal; 1 for do not display in listings
       System flag          0 for normal files; 1 for system file
       Archive flag         0 for has been backed up; 1 for needs to be backed  up
       ASCII/binary flag    0 for ASCII file; 1 for binary file
       Random access flag   0 for sequential access only; 1 for random access
       Temporary flag       0 for normal; 1 for delete file on process exit
       Lock flags           0 for unlocked; nonzero for locked
       Record length        Number of bytes in a record
       Key position         Offset of the key within each record
       Key length           Number of bytes in the key field
       Creation time        Date and time the file was created
       Time of last access  Date and time the file was last accessed
       Time of last change  Date and time the file was last changed
       Current size         Number of bytes in the file
       Maximum size         Number of bytes the file may grow to
                            Figure 4-4. Some possible file attributes.
4.1.6  File Operations
     Files exist to store information and allow it to be retrieved later. Different sys-
tems provide different operations to allow storage and retrieval.            Below is a dis-
cussion of the most common system calls relating to files.
       1.  Create.    The file is created with no data. The purpose of the call is to
           announce that the file is coming and to set some of the attributes.
       2.  Delete. When the file is no longer needed, it has to be deleted to free
           up disk space. There is always a system call for this purpose.
       3.  Open. Before using a file, a process must open it. The purpose of the
           open call is to allow the system to fetch the attributes and list of disk
           addresses into main memory for rapid access on later calls.
       4.  Close.    When all the accesses are finished, the attributes and disk ad-
           dresses are no longer needed, so the file should be closed to free up
           internal table space. Many systems encourage this by imposing a



SEC. 4.1                              FILES                                                  273
          maximum number of open files on processes.       A disk is written in
          blocks, and closing a file forces writing of the file's last block, even
          though that block may not be entirely full yet.
5.        Read.   Data are read from file. Usually, the bytes come from the cur-
          rent position. The caller must specify how many data are needed and
          must also provide a buffer to put them in.
6.        Write.  Data are written to the file again, usually at the current posi-
          tion.   If the current position is the end of the file, the file's size in-
          creases.   If the current position is in the middle of the file, existing
          data are overwritten and lost forever.
7.        Append. This call is a restricted form of write. It can add data only to
          the end of the file. Systems that provide a minimal set of system calls
          rarely  have  append,  but  many   systems     provide     multiple  ways      of
          doing the same thing, and these systems sometimes have append.
8.        Seek.   For random-access files, a method is needed to specify from
          where to take the data. One common approach is a system call, seek,
          that repositions the file pointer to a specific place in the file. After this
          call has completed, data can be read from, or written to, that position.
9.        Get attributes.  Processes often need to read file attributes to do their
          work. For example, the UNIX make program is commonly used to
          manage     software  development   projects    consisting  of  many  source
          files. When make is called, it examines the modification times of all
          the source and object files and arranges for the minimum number of
          compilations required to bring everything up to date.         To do its job, it
          must look at the attributes, namely, the modification times.
10.       Set attributes.  Some of the attributes are user settable and can be
          changed after the file has been created. This system call makes that
          possible.  The protection-mode information is an obvious example.
          Most of the flags also fall in this category.
11.       Rename.    It frequently happens that a user needs to change the name
          of an existing file. This system call makes that possible.     It is not al-
          ways strictly necessary, because the file can usually be copied to a
          new file with the new name, and the old file then deleted.
4.1.7 An Example Program Using File-System Calls
In this section we will examine a simple UNIX program that copies one file
from its source file to a destination file.  It is listed in Fig. 4-5.   The program has
minimal functionality and even worse error reporting, but it gives a reasonable idea
of how some of the system calls related to files work.



274                                            FILE SYSTEMS                                   CHAP. 4
/* File copy program. Error checking and reporting is minimal. */
#include <sys/types.h>                                 /* include necessary header files */
#include <fcntl.h>
#include <stdlib.h>
#include <unistd.h>
int main(int argc, char *argv[]);                      /* ANSI prototype */
#define BUF       SIZE 4096                            /* use a buffer size of 4096 bytes */
#define OUTPUT          MODE 0700                      /* protection bits for output file */
int main(int argc, char *argv[])
{
     int in   fd, out       fd, rd  count, wt  count;
     char buffer[BUF        SIZE];
     if (argc != 3) exit(1);                           /* syntax error if argc is not 3 */
     /* Open the input file and create the output file */
     in   fd = open(argv[1], O      RDONLY);           /* open the source file */
     if (in   fd < 0) exit(2);                         /* if it cannot be opened, exit */
     out     fd = creat(argv[2], OUTPUT        MODE);  /* create the destination file */
     if (out  fd < 0) exit(3);                         /* if it cannot be created, exit */
     /* Copy loop */
     while (TRUE) {
              rd  count = read(in    fd, buffer, BUF   SIZE); /* read a block of data */
              if (rd  count <= 0) break;               /* if end of file or error, exit loop  */
              wt  count = write(out  fd, buffer, rd    count); /* write data */
              if (wt  count <= 0) exit(4);             /* wt  count <= 0 is an error */
     }
     /* Close the files */
     close(in     fd);
     close(out        fd);
     if (rd   count == 0)                              /* no error on last read */
              exit(0);
     else
              exit(5);                                 /* error on last read */
}
                                    Figure 4-5. A simple program to copy a file.
     The program, copyfile, can be called, for example, by the command line
     copyfile abc xyz
to copy the file abc to xyz.         If xyz already exists, it will be overwritten. Otherwise,
it will be created.         The program must be called with exactly two arguments, both
legal file names. The first is the source; the second is the output file.



SEC. 4.1                               FILES                                            275
The four #include statements near the top of the program cause a large number
of definitions and function prototypes to be included in the program. These are
needed to make the program conformant to the relevant international standards, but
will not concern us further. The next line is a function prototype for main, some-
thing required by ANSI C, but also not important for our purposes.
The first #define statement is a macro definition that defines the character
string BUF  SIZE as a macro that expands into the number 4096.             The program
will read and write in chunks of 4096 bytes.    It is considered good programming
practice to give names to constants like this and to use the names instead of the
constants. Not only does this convention make programs easier to read, but it also
makes them easier to maintain. The second #define statement determines who can
access the output file.
The main program is called main, and it has two arguments, argc, and argv.
These are supplied by the operating system when the program is called. The first
one tells how many strings were present on the command line that invoked the pro-
gram, including the program name.      It should be 3.  The second one is an array of
pointers to the arguments.  In the example call given above, the elements of this
array would contain pointers to the following values:
argv[0] = "copyfile"
argv[1] = "abc"
argv[2] = "xyz"
It is via this array that the program accesses its arguments.
Five variables are declared. The first two, in         fd and out  fd, will hold the file
descriptors, small integers returned when a file is opened. The next two, rd  count
and wt    count, are the byte counts returned by the read and write system calls, re-
spectively. The last one, buffer, is the buffer used to hold the data read and supply
the data to be written.
The first actual statement checks argc to see if it is 3. If not, it exits with status
code 1.   Any status code other than 0 means that an error has occurred. The status
code is the only error reporting present in this program.          A production version
would normally print error messages as well.
Then we try to open the source file and create the destination file. If the source
file is successfully opened, the system assigns a small integer to in      fd, to identify
the file. Subsequent calls must include this integer so that the system knows which
file it wants. Similarly, if the destination is successfully created, out  fd is given a
value to identify it. The second argument to creat sets the protection mode.  If ei-
ther the open or the create fails, the corresponding file descriptor is set to -1, and
the program exits with an error code.
Now comes the copy loop. It starts by trying to read in 4 KB of data to buffer.
It does this by calling the library procedure read, which actually invokes the read
system call. The first parameter identifies the file, the second gives the buffer, and
the third tells how many bytes to read. The value assigned to rd           count gives the



276                                  FILE SYSTEMS                              CHAP. 4
number of bytes actually read. Normally, this will be 4096, except if fewer bytes
are remaining in the file. When the end of the file has been reached, it will be 0. If
rd  count is ever zero or negative, the copying cannot continue, so the break state-
ment is executed to terminate the (otherwise endless) loop.
     The call to write outputs the buffer to the destination file. The first parameter
identifies the file, the second gives the buffer, and the third tells how many bytes to
write, analogous to read.  Note that the byte count is the number of bytes actually
read, not BUF  SIZE.     This point is important because the last read will not return
4096 unless the file just happens to be a multiple of 4 KB.
     When the entire file has been processed, the first call beyond the end of file
will return 0 to rd  count, which will make it exit the loop.  At this point the two
files are closed and the program exits with a status indicating normal termination.
     Although the Windows system calls are different from those of UNIX, the gen-
eral structure of a command-line Windows program to copy a file is moderately
similar to that of Fig. 4-5. We will examine the Windows 8 calls in Chap. 11.
4.2 DIRECTORIES
     To keep track of files, file systems normally have directories or folders, which
are themselves files. In this section we will discuss directories, their organization,
their properties, and the operations that can be performed on them.
4.2.1 Single-Level Directory Systems
     The simplest form of directory system is having one directory containing all
the files. Sometimes it is called the root directory, but since it is the only one, the
name does not matter much.  On early personal computers, this system was com-
mon, in part because there was only one user. Interestingly enough, the world's
first supercomputer, the CDC 6600, also had only a single directory for all files,
even though it was used by many users at once. This decision was no doubt made
to keep the software design simple.
     An example of a system with one directory is given in Fig. 4-6.          Here the di-
rectory contains four files. The advantages of this scheme are its simplicity and the
ability to locate files quickly--there is only one place to look, after all.  It is some-
times still used on simple embedded devices such as digital cameras and some
portable music players.
4.2.2 Hierarchical Directory Systems
     The single level is adequate for very simple dedicated applications (and was
even used on the first personal computers), but for modern users with thousands of
files, it would be impossible to find anything if all files were in a single directory.



SEC. 4.2                      DIRECTORIES                                              277
                                             Root directory
                        A     B     C        D
          Figure 4-6. A single-level directory system containing four files.
Consequently, a way is needed to group related files together. A professor, for ex-
ample, might have a collection of files that together form a book that he is writing,
a second collection containing student programs submitted for another course, a
third group containing the code of an advanced compiler-writing system he is
building, a fourth group containing grant proposals, as well as other files for elec-
tronic mail, minutes of meetings, papers he is writing, games, and so on.
What is needed is a hierarchy (i.e., a tree of directories).       With this approach,
there can be as many directories as are needed to group the files in natural ways.
Furthermore, if multiple users share a common file server, as is the case on many
company networks, each user can have a private root directory for his or her own
hierarchy. This approach is shown in Fig. 4-7.        Here, the directories A, B, and C
contained in the root directory each belong to a different user, two of whom have
created subdirectories for projects they are working on.
                                          Root directory
          User
          directory  A           B                 C
                     A     B     B        B        C     C
                           B                 C           C
                     User subdirectories
                                          C     C     C      C     User file
                     Figure 4-7. A hierarchical directory system.
The ability for users to create an arbitrary number of subdirectories provides a
powerful structuring tool for users to organize their work. For this reason, nearly
all modern file systems are organized in this manner.
4.2.3 Path Names
When the file system is organized as a directory tree, some way is needed for
specifying file names. Two different methods are commonly used.               In the first
method, each file is given an absolute path name consisting of the path from the



278                                 FILE SYSTEMS                                CHAP. 4
root directory to the file.  As an example, the path /usr/ast/mailbox means that the
root directory contains a subdirectory usr, which in turn contains a subdirectory
ast, which contains the file mailbox.  Absolute path names always start at the root
directory and are unique.    In UNIX the components of the path are separated by /.
In Windows the separator is \ .     In MULTICS it was >.  Thus, the same path name
would be written as follows in these three systems:
     Windows     \usr\ast\mailbox
     UNIX        /usr/ast/mailbox
     MULTICS     >usr>ast>mailbox
No matter which character is used, if the first character of the path name is the sep-
arator, then the path is absolute.
     The other kind of name is the relative path name. This is used in conjunction
with the concept of the working directory (also called the current directory).            A
user can designate one directory as the current working directory, in which case all
path names not beginning at the root directory are taken relative to the working di-
rectory. For example, if the current working directory is /usr/ast, then the file
whose absolute path is /usr/ast/mailbox can be referenced simply as mailbox.              In
other words, the UNIX command
     cp /usr/ast/mailbox /usr/ast/mailbox.bak
and the command
     cp mailbox mailbox.bak
do exactly the same thing if the working directory is /usr/ast.  The relative form is
often more convenient, but it does the same thing as the absolute form.
     Some programs need to access a specific file without regard to what the work-
ing directory is. In that case, they should always use absolute path names. For ex-
ample, a spelling checker might need to read /usr/lib/dictionary to do its work.          It
should use the full, absolute path name in this case because it does not know what
the working directory will be when it is called. The absolute path name will always
work, no matter what the working directory is.
     Of course, if the spelling checker needs a large number of files from /usr/lib,
an alternative approach is for it to issue a system call to change its working direc-
tory to /usr/lib, and then use just dictionary as the first parameter to open.    By ex-
plicitly changing the working directory, it knows for sure where it is in the direc-
tory tree, so it can then use relative paths.
     Each process has its own working directory, so when it changes its working di-
rectory and later exits, no other processes are affected and no traces of the change
are left behind in the file system. In this way, it is always perfectly safe for a proc-
ess to change its working directory whenever it finds that to be convenient. On the
other hand, if a library procedure changes the working directory and does not
change back to where it was when it is finished, the rest of the program may not



SEC. 4.2                     DIRECTORIES                                                 279
work since its assumption about where it is may now suddenly be invalid. For this
reason, library procedures rarely change the working directory, and when they
must, they always change it back again before returning.
Most operating systems that support a hierarchical directory system have two
special entries in every directory, ``.'' and ``..'', generally pronounced ``dot'' and
``dotdot.'' Dot refers to the current directory; dotdot refers to its parent (except in
the root directory, where it refers to itself). To see how these are used, consider the
UNIX file tree of Fig. 4-8.  A certain process has /usr/ast as its working directory.
It can use .. to go higher up the tree.     For example, it can copy the file /usr/lib/dic-
tionary to its own directory using the command
cp ../lib/dictionary .
The first path instructs the system to go upward (to the usr directory), then to go
down to the directory lib to find the file dictionary.
                                         /
                                            bin  Root directory
                                            etc
                                            lib
                                            usr
                                            tmp
          bin           etc                 lib              usr       tmp
                                                             ast
                                                             jim
                                                             lib
                                            ast         lib       jim
                                                 dict.                 /usr/jim
                             Figure 4-8. A UNIX directory tree.
The second argument (dot) names the current directory. When the cp command
gets a directory name (including dot) as its last argument, it copies all the files to



280                                      FILE SYSTEMS                                CHAP. 4
that directory.  Of course, a more normal way       to  do the    copy would    be to use the
full absolute path name of the source file:
     cp /usr/lib/dictionary .
Here the use of dot saves the user the trouble      of  typing    dictionary a  second time.
Nevertheless, typing
     cp /usr/lib/dictionary dictionary
also works fine, as does
     cp /usr/lib/dictionary /usr/ast/dictionary
All of these do exactly the same thing.
4.2.4 Directory Operations
     The allowed system calls for managing directories exhibit more variation from
system to system than system calls for files.       To give an impression of what they
are and how they work, we will give a sample (taken from UNIX).
     1.  Create.      A directory is created.    It is empty except for dot and dotdot,
         which are put there automatically by the system (or in a few cases, by
         the mkdir program).
     2.  Delete.    A directory is deleted. Only an empty directory can be delet-
         ed.     A directory containing only dot and dotdot is considered empty
         as these cannot be deleted.
     3.  Opendir. Directories can be read. For example, to list all the files in a
         directory, a listing program opens the directory to read out the names
         of all the files it contains. Before a directory can be read, it must be
         opened, analogous to opening and reading a file.
     4.  Closedir.    When a directory has been read, it should be closed to free
         up internal table space.
     5.  Readdir.     This call returns the next entry in an open directory. For-
         merly, it was possible to read directories using the usual read system
         call,   but  that     approach  has   the  disadvantage  of  forcing   the  pro-
         grammer to know and deal with the internal structure of directories.
         In contrast, readdir always returns one entry in a standard format, no
         matter which of the possible directory structures is being used.
     6.  Rename.      In many respects, directories are just like files and can be
         renamed the same way files can be.
     7.  Link.   Linking is a technique that allows a file to appear in more than
         one directory. This system call specifies an existing file and a path



SEC. 4.2                  DIRECTORIES                                                    281
          name, and creates a link from the existing file to the name specified
          by the path.  In this way, the same file may appear in multiple direc-
          tories.  A link of this kind, which increments the counter in the file's
          i-node (to keep track of the number of directory entries containing the
          file), is sometimes called a hard link.
8.        Unlink.  A directory entry is removed.    If the file being unlinked is
          only present in one directory (the normal case), it is removed from the
          file system. If it is present in multiple directories, only the path name
          specified is removed. The others remain.  In UNIX, the system call
          for deleting files (discussed earlier) is, in fact, unlink.
The above list gives the most important calls, but there are a few others as well, for
example, for managing the protection information associated with a directory.
A variant on the idea of linking files is the symbolic link.           Instead, of having
two names point to the same internal data structure representing a file, a name can
be created that points to a tiny file naming another file. When the first file is used,
for example, opened, the file system follows the path and finds the name at the end.
Then it starts the lookup process all over using the new name. Symbolic links have
the advantage that they can cross disk boundaries and even name files on remote
computers. Their implementation is somewhat less efficient than hard links though.
4.3 FILE-SYSTEM IMPLEMENTATION
Now it is time to turn from the user's view of the file system to the imple-
mentor's view. Users are concerned with how files are named, what operations are
allowed on them, what the directory tree looks like, and similar interface issues.
Implementors are interested in how files and directories are stored, how disk space
is managed, and how to make everything work efficiently and reliably.  In the fol-
lowing sections we will examine a number of these areas to see what the issues and
trade-offs are.
4.3.1 File-System Layout
File systems are stored on disks. Most disks can be divided up into one or
more partitions, with independent file systems on each partition. Sector 0 of the
disk is called the MBR (Master Boot Record) and is used to boot the computer.
The end of the MBR contains the partition table.    This table gives the starting and
ending addresses of each partition. One of the partitions in the table is marked as
active.  When the computer is booted, the BIOS reads in and executes the MBR.
The first thing the MBR program does is locate the active partition, read in its first
block, which is called the boot block, and execute it. The program in the boot
block loads the operating system contained in that partition. For uniformity, every



282                       FILE SYSTEMS                                     CHAP. 4
partition starts with a boot block, even if it does not contain a bootable operating
system. Besides, it might contain one in the future.
     Other than starting with a boot block, the layout of a disk partition varies a lot
from file system to file system. Often the file system will contain some of the items
shown in Fig. 4-9. The first one is the superblock. It contains all the key parame-
ters about the file system and is read into memory when the computer is booted or
the file system is first touched. Typical information in the superblock includes a
magic number to identify the file-system type, the number of blocks in the file sys-
tem, and other key administrative information.
                          Entire disk
     Partition table                     Disk partition
     MBR
Boot block  Superblock  Free space mgmt  I-nodes         Root dir    Files and directories
                        Figure 4-9. A possible file-system layout.
     Next might come information about free blocks in the file system, for example
in the form of a bitmap or a list of pointers. This might be followed by the i-nodes,
an array of data structures, one per file, telling all about the file. After that might
come the root directory, which contains the top of the file-system tree. Finally, the
remainder of the disk contains all the other directories and files.
4.3.2 Implementing Files
     Probably the most important issue in implementing file storage is keeping
track of which disk blocks go with which file. Various methods are used in dif-
ferent operating systems. In this section, we will examine a few of them.
Contiguous Allocation
     The simplest allocation scheme is to store each file as a contiguous run of disk
blocks. Thus on a disk with 1-KB blocks, a 50-KB file would be allocated 50 con-
secutive blocks. With 2-KB blocks, it would be allocated 25 consecutive blocks.
     We see an example of contiguous storage allocation in Fig. 4-10(a).   Here the
first 40 disk blocks are shown, starting with block 0 on the left. Initially, the disk



SEC. 4.3                      FILE-SYSTEM IMPLEMENTATION                                       283
was empty.  Then a file A, of length four blocks, was written to disk starting at the
beginning (block 0).   After that a six-block file, B, was written starting right after
the end of file A.
Note that each file begins at the start of a new block, so that if file A was really
3 blocks, some space is wasted at the end of the last block.       In the figure, a total
of seven files are shown, each one starting at the block following the end of the
previous one. Shading is used just to make it easier to tell the files apart. It has no
actual significance in terms of storage.
File A              File C                          File E                         File G
(4 blocks)          (6 blocks)                      (12 blocks)                    (3 blocks)
                                                                                               ...
            File B              File D                              File F
        (3 blocks)              (5 blocks)                          (6 blocks)
                                               (a)
(File A)            (File C)                        (File E)                       (File G)
                                                                                               ...
            File B              5 Free blocks                       6 Free blocks
                                               (b)
          Figure 4-10. (a) Contiguous allocation of disk space for seven files.  (b) The
          state of the disk after files D and F have been removed.
Contiguous disk-space allocation has two significant advantages. First, it is
simple to implement because keeping track of where a file's blocks are is reduced
to remembering two numbers: the disk address of the first block and the number of
blocks in the file. Given the number of the first block, the number of any other
block can be found by a simple addition.
Second, the read performance is excellent because the entire file can be read
from the disk in a single operation. Only one seek is needed (to the first block).
After that, no more seeks or rotational delays are needed, so data come in at the
full bandwidth of the disk. Thus contiguous allocation is simple to implement and
has high performance.
Unfortunately, contiguous allocation also has a very serious drawback: over the
course of time, the disk becomes fragmented.        To see how this comes about, exam-
ine Fig. 4-10(b).   Here two files, D and F, have been removed. When a file is re-
moved, its blocks are naturally freed, leaving a run of free blocks on the disk. The
disk is not compacted on the spot to squeeze out the hole, since that would involve
copying all the blocks following the hole, potentially millions of blocks, which



284                                  FILE SYSTEMS                           CHAP. 4
would take hours or even days with large disks.             As a result, the disk ultimately
consists of files and holes, as illustrated in the figure.
     Initially, this fragmentation is not a problem, since each new file can be written
at the end of disk, following the previous one. However, eventually the disk will fill
up and it will become necessary to either compact the disk, which is prohibitively
expensive, or to reuse the free space in the holes. Reusing the space requires main-
taining a list of holes, which is doable.  However, when a new file is to be created,
it is necessary to know its final size in order to choose a hole of the correct size to
place it in.
     Imagine the consequences of such a design. The user starts a word processor in
order to create a document. The first thing the program asks is how many bytes the
final document will be. The question must be answered or the program will not
continue.     If the number given ultimately proves too small, the program has to ter-
minate prematurely because the disk hole is full and there is no place to put the rest
of the file. If the user tries to avoid this problem by giving an unrealistically large
number as the final size, say, 1 GB, the editor may be unable to find such a large
hole and announce that the file cannot be created.          Of course, the user would be
free to start the program again and say 500 MB this time, and so on until a suitable
hole was located. Still, this scheme is not likely to lead to happy users.
     However, there is one situation in which contiguous allocation is feasible and,
in fact, still used: on CD-ROMs.     Here all the file sizes are known in advance and
will never change during subsequent use of the CD-ROM file system.
     The situation with DVDs is a bit more complicated.            In principle, a 90-min
movie could be encoded as a single file of length about 4.5 GB, but the file system
used,  UDF    (Universal  Disk  Format),   uses   a  30-bit  number  to     represent    file
length, which limits files to 1 GB.  As a consequence, DVD movies are generally
stored as three or four 1-GB files, each of which is contiguous. These physical
pieces of the single logical file (the movie) are called extents.
     As we mentioned in Chap. 1, history often repeats itself in computer science as
new generations of technology occur. Contiguous allocation was actually used on
magnetic-disk file systems years ago due to its simplicity and high performance
(user friendliness did not count for much then).     Then the idea was dropped due to
the nuisance of having to specify final file size at file-creation time. But with the
advent of CD-ROMs, DVDs, Blu-rays, and other write-once optical media, sud-
denly contiguous files were a good idea again.       It is thus important to study old
systems and ideas that were conceptually clean and simple because they may be
applicable to future systems in surprising ways.
Linked-List Allocation
     The second method for storing files is to keep each one as a linked list of disk
blocks, as shown in Fig. 4-11. The first word of each block is used as a pointer to
the next one. The rest of the block is for data.



SEC. 4.3                  FILE-SYSTEM IMPLEMENTATION                                     285
                                                     File A
                                                                                  0
                          File         File          File           File          File
                    block              block         block          block         block
                          0            1             2              3             4
          Physical        4            7             2              10            12
          block
                                                     File B
                                                                           0
                                File          File           File          File
                                block         block          block         block
                                0             1              2             3
          Physical              6             3              11            14
                    block
          Figure 4-11. Storing a file as a linked list of disk blocks.
Unlike contiguous allocation, every disk block can be used in this method.               No
space is lost to disk fragmentation (except for internal fragmentation in the last
block). Also, it is sufficient for the directory entry to merely store the disk address
of the first block. The rest can be found starting there.
On the other hand, although reading a file sequentially is straightforward, ran-
dom access is extremely slow.         To get to block n, the operating system has to start
at the beginning and read the n - 1 blocks prior to it, one at a time. Clearly, doing
so many reads will be painfully slow.
Also, the amount of data storage in a block is no longer a power of two be-
cause the pointer takes up a few bytes. While not fatal, having a peculiar size is
less efficient because many programs read and write in blocks whose size is a pow-
er of two. With the first few bytes of each block occupied by a pointer to the next
block, reads of the full block size require acquiring and concatenating information
from two disk blocks, which generates extra overhead due to the copying.
Linked-List Allocation Using a Table in Memory
Both disadvantages of the linked-list allocation can be eliminated by taking the
pointer word from each disk block and putting it in a table in memory. Figure 4-12
shows what the table looks like for the example of Fig. 4-11.                     In both figures, we
have two files. File A uses disk blocks 4, 7, 2, 10, and 12, in that order, and file B
uses disk blocks 6, 3, 11, and 14, in that order. Using the table of Fig. 4-12, we can
start with block 4 and follow the chain all the way to the end. The same can be
done starting with block 6. Both chains are terminated with a special marker (e.g.,
-1) that is not a valid block number. Such a table in main memory is called a FAT
(File Allocation Table).



286                         FILE  SYSTEMS                                                   CHAP. 4
               Physical
               block
               0
               1
               2            10
               3            11
               4            7                 File A starts here
               5
               6            3                 File B starts here
               7            2
               8
               9
               10           12
               11           14
               12           -1
               13
               14           -1
               15                             Unused block
         Figure 4-12. Linked-list allocation using a file-allocation table in main memory.
     Using this organization, the entire block is available for data. Furthermore, ran-
dom access is much easier.  Although the chain must still be followed to find a
given offset within the file, the chain is entirely in memory, so it can be followed
without making any disk references. Like the previous method, it is sufficient for
the directory entry to keep a single integer (the starting block number) and still be
able to locate all the blocks, no matter how large the file is.
     The primary disadvantage of this method is that the entire table must be in
memory all the time to make it work. With a 1-TB disk and a 1-KB block size, the
table needs 1 billion entries, one for each of the 1 billion disk blocks. Each entry
has to be a minimum of 3 bytes. For speed in lookup, they should be 4 bytes. Thus
the table will take up 3 GB or 2.4 GB of main memory all the time, depending on
whether the system is optimized for space or time. Not wildly practical. Clearly the
FAT idea does not scale well to large disks.  It was the original MS-DOS file sys-
tem and is still fully supported by all versions of Windows though.
I-nodes
     Our last method for keeping track of which blocks belong to which file is to
associate with each file a data structure called an i-node (index-node), which lists
the attributes and disk addresses of the file's blocks. A simple example is depicted
in Fig. 4-13.  Given the i-node, it is then possible to find all the blocks of the file.



SEC. 4.3  FILE-SYSTEM IMPLEMENTATION                                                        287
The big advantage of this scheme over linked files using an in-memory table is that
the i-node need be in memory only when the corresponding file is open.    If each i-
node occupies n bytes and a maximum of k files may be open at once, the total
memory occupied by the array holding the i-nodes for the open files is only kn
bytes. Only this much space need be reserved in advance.
          File Attributes
          Address of disk block 0
          Address of disk block 1
          Address of disk block 2
          Address of disk block 3
          Address of disk block 4
          Address of disk block 5
          Address of disk block 6
          Address of disk block 7
          Address of block of pointers
                                                          Disk block
                                                          containing
                                                          additional
                                                          disk addresses
          Figure 4-13. An example i-node.
This array is usually far smaller than the space occupied by the file table de-
scribed in the previous section. The reason is simple.    The table for holding the
linked list of all disk blocks is proportional in size to the disk itself. If the disk has
n blocks, the table needs n entries.  As disks grow larger, this table grows linearly
with them. In contrast, the i-node scheme requires an array in memory whose size
is proportional to the maximum number of files that may be open at once. It does
not matter if the disk is 100 GB, 1000 GB, or 10,000 GB.
One problem with i-nodes is that if each one has room for a fixed number of
disk addresses, what happens when a file grows beyond this limit? One solution is
to reserve the last disk address not for a data block, but instead for the address of a
block containing more disk-block addresses, as shown in Fig. 4-13. Even more ad-
vanced would be two or more such blocks containing disk addresses or even disk
blocks pointing to other disk blocks full of addresses.   We will come back to i-
nodes when studying UNIX in Chap. 10.   Similarly, the Windows NTFS file sys-
tem uses a similar idea, only with bigger i-nodes that can also contain small files.



288                                       FILE SYSTEMS                                      CHAP. 4
4.3.3 Implementing Directories
     Before a file can be read, it must be opened. When a file is opened, the operat-
ing system uses the path name supplied by the user to locate the directory entry on
the disk. The directory entry provides the information needed to find the disk
blocks.    Depending on the system, this information may be the disk address of the
entire file (with contiguous allocation), the number of the first block (both link-
ed-list schemes), or the number of the i-node. In all cases, the main function of the
directory system is to map the ASCII name of the file onto the information needed
to locate the data.
     A closely related issue is where the attributes should be stored. Every file sys-
tem maintains various file attributes, such as each file's owner and creation time,
and they must be stored somewhere. One obvious possibility is to store them di-
rectly in the directory entry.       Some systems do precisely that. This option is shown
in Fig. 4-14(a).  In this simple design, a directory consists of a list of fixed-size en-
tries, one per file, containing a (fixed-length) file name, a structure of the file at-
tributes, and one or more disk addresses (up to some maximum) telling where the
disk blocks are.
     games       attributes               games
     mail        attributes               mail
     news        attributes               news
     work        attributes               work
            (a)                                   (b)                 Data structure
                                                                      containing the
                                                                      attributes
           Figure 4-14. (a) A simple directory containing fixed-size entries with the disk addresses
           and attributes in the directory entry. (b) A directory in which each entry just
           refers to an i-node.
     For systems that use i-nodes, another possibility for storing the attributes is in
the i-nodes, rather than in the directory entries. In that case, the directory entry can
be shorter: just a file name and an i-node number. This approach is illustrated in
Fig. 4-14(b).    As we shall see later, this method has some advantages over putting
them in the directory entry.
     So far we have made the assumption that files have short, fixed-length names.
In MS-DOS files have a 18 character base name and an optional extension of 13
characters. In UNIX Version 7, file names were 114 characters, including any ex-
tensions.   However,         nearly  all  modern  operating  systems  support  longer,                vari-
able-length file names. How can these be implemented?



SEC. 4.3                         FILE-SYSTEM IMPLEMENTATION                                 289
The simplest approach is to set a limit on file-name length, typically 255 char-
acters, and then use one of the designs of Fig. 4-14 with 255 characters reserved
for each file name. This approach is simple, but wastes a great deal of directory
space, since few files have such long names. For efficiency reasons, a different
structure is desirable.
One alternative is to give up the idea that all directory entries are the same size.
With this method, each directory entry contains a fixed portion, typically starting
with the length of the entry, and then followed by data with a fixed format, usually
including the owner, creation time, protection information, and other attributes.
This fixed-length header is followed by the actual file name, however long it may
be, as shown in Fig. 4-15(a) in big-endian format (e.g., SPARC).             In this example
we have three files, project-budget, personnel, and foo.     Each file name is termi-
nated by a special character (usually 0), which is represented in the figure by a box
with a cross in it.      To allow each directory entry to begin on a word boundary, each
file name is filled out to an integral number of words, shown by shaded boxes in
the figure.
                File 1 entry length     Pointer to file 1's name                            Entry
                                                                                            for one
                File 1 attributes                         File 1 attributes                 file
Entry        p           r       o   j  Pointer to file 2's name
for one      e           c       t   -
file                                                      File 2 attributes
             b           u       d   g
             e           t              Pointer to file 3's name
                File 2 entry length
                                                          File 3 attributes
                File 2 attributes
             p           e       r   s
             o           n       n   e
             l                          p                 r       o                      j
                File 3 entry length     e                 c       t                      -
                                        b                 u       d                      g
                File 3 attributes       e                 t                              p
                                        e                 r       s                      o  Heap
             f           o       o
                                        n                 n       e                      l
                                                          f       o                      o
                            (a)                              (b)
         Figure 4-15. Two ways of handling long file names in a directory. (a) In-line.
         (b) In a heap.
A disadvantage of this method is that when a file is removed, a variable-sized
gap is introduced into the directory into which the next file to be entered may not
fit. This problem is essentially the same one we saw with contiguous disk files,



290                               FILE SYSTEMS                               CHAP. 4
only now compacting the directory is feasible because it is entirely in memory. An-
other problem is that a single directory entry may span multiple pages, so a page
fault may occur while reading a file name.
     Another way to handle variable-length names is to make the directory entries
themselves all fixed length and keep the file names together in a heap at the end of
the directory, as shown in Fig. 4-15(b).    This method has the advantage that when
an entry is removed, the next file entered will always fit there. Of course, the heap
must be managed and page faults can still occur while processing file names. One
minor win here is that there is no longer any real need for file names to begin at
word boundaries, so no filler characters are needed after file names in Fig. 4-15(b)
as they are in Fig. 4-15(a).
     In all of the designs so far, directories are searched linearly from beginning to
end when a file name has to be looked up. For extremely long directories, linear
searching can be slow.  One way to speed up the search is to use a hash table in
each directory. Call the size of the table n. To enter a file name, the name is hashed
onto a value between 0 and n - 1, for example, by dividing it by n and taking the
remainder. Alternatively, the words comprising the file name can be added up and
this quantity divided by n, or something similar.
     Either way, the table entry corresponding to the hash code is inspected. If it is
unused, a pointer is placed there to the file entry. File entries follow the hash table.
If that slot is already in use, a linked list is constructed, headed at the table entry
and threading through all entries with the same hash value.
     Looking up a file follows the same procedure. The file name is hashed to select
a hash-table entry.    All the entries on the chain headed at that slot are checked to
see if the file name is present.  If the name is not on the chain, the file is not pres-
ent in the directory.
     Using a hash table has the advantage of much faster lookup, but the disadvan-
tage of more complex administration.      It is only really a serious candidate in sys-
tems where it is expected that directories will routinely contain hundreds or thou-
sands of files.
     A different way to speed up searching large directories is to cache the results
of searches. Before starting a search, a check is first made to see if the file name is
in the cache.    If so, it can be located immediately.  Of course, caching only works
if a relatively small number of files comprise the majority of the lookups.
4.3.4 Shared Files
     When several users are working together on a project, they often need to share
files.  As a result, it is often convenient for a shared file to appear simultaneously
in different directories belonging to different users.  Figure 4-16 shows the file sys-
tem of Fig. 4-7 again, only with one of C's files now present in one of B's direc-
tories as well. The connection between B's directory and the shared file is called a



SEC. 4.3               FILE-SYSTEM IMPLEMENTATION                                           291
link. The file system itself is now a Directed Acyclic Graph, or DAG, rather than
a tree. Having the file system be a DAG complicates maintenance, but such is life.
                                           Root directory
                    A                   B                    C
                    A             B     B     B           C     C
                                  B           C                 C
                                           ?         C       C     C
                                        Shared file
                    Figure 4-16.  File system containing a shared file.
Sharing files is convenient, but it also introduces some problems.          To start
with, if directories really do contain disk addresses, then a copy of the disk ad-
dresses will have to be made in B's directory when the file is linked.      If either B or
C subsequently appends to the file, the new blocks will be listed only in the direc-
tory of the user doing the append. The changes will not be visible to the other user,
thus defeating the purpose of sharing.
This problem can be solved in two ways.              In the first solution, disk blocks are
not listed in directories, but in a little data structure associated with the file itself.
The directories would then point just to the little data structure.         This is the ap-
proach used in UNIX (where the little data structure is the i-node).
In the second solution, B links to one of C's files by having the system create a
new file, of type LINK, and entering that file in B's directory. The new file con-
tains just the path name of the file to which it is linked.        When B reads from the
linked file, the operating system sees that the file being read from is of type LINK,
looks up the name of the file, and reads that file. This approach is called symbolic
linking, to contrast it with traditional (hard) linking.
Each of these methods has its drawbacks.             In the first method, at the moment
that B links to the shared file, the i-node records the file's owner as C.  Creating a
link does not change the ownership (see Fig. 4-17), but it does increase the link
count in the i-node, so the system knows how many directory entries currently
point to the file.
If C subsequently tries to remove the file, the system is faced with a problem.
If it removes the file and clears the i-node, B will have a directory entry pointing to



292                      FILE SYSTEMS                                                       CHAP. 4
     C's directory       B's directory     C's directory       B's directory
     Owner = C                             Owner = C           Owner = C
     Count = 1                             Count = 2              Count = 1
                    (a)                    (b)                    (c)
     Figure 4-17. (a) Situation prior to linking. (b) After the link is created. (c) After
     the original owner removes the file.
an invalid i-node.    If the i-node is later reassigned to another file, B's link will
point to the wrong file. The system can see from the count in the i-node that the
file is still in use, but there is no easy way for it to find all the directory entries for
the file, in order to erase them. Pointers to the directories cannot be stored in the i-
node because there can be an unlimited number of directories.
     The only thing to do is remove C's directory entry, but leave the i-node intact,
with count set to 1, as shown in Fig. 4-17(c).        We now have a situation in which B
is the only user having a directory entry for a file owned by C.  If the system does
accounting or has quotas, C will continue to be billed for the file until B decides to
remove it, if ever, at which time the count goes to 0 and the file is deleted.
     With symbolic links this problem does not arise because only the true owner
has a pointer to the i-node. Users who have linked to the file just have path names,
not i-node pointers.     When the owner removes the file, it is destroyed. Subsequent
attempts to use the file via a symbolic link will fail when the system is unable to
locate the file. Removing a symbolic link does not affect the file at all.
     The problem with symbolic links is the extra overhead required. The file con-
taining the path must be read, then the path must be parsed and followed, compo-
nent by component, until the i-node is reached. All of this activity may require a
considerable number of extra disk accesses. Furthermore, an extra i-node is needed
for each symbolic link, as is an extra disk block to store the path, although if the
path name is short, the system could store it in the i-node itself, as a kind of opti-
mization. Symbolic links have the advantage that they can be used to link to files
on machines anywhere in the world, by simply providing the network address of
the machine where the file resides in addition to its path on that machine.
     There is also another problem introduced by links, symbolic or otherwise.
When links are allowed, files can have two or more paths.      Programs that start at a
given directory and find all the files in that directory and its subdirectories will
locate a linked file multiple times. For example, a program that dumps all the files



SEC. 4.3                  FILE-SYSTEM IMPLEMENTATION                                      293
in a directory and its subdirectories onto a tape may make multiple copies of a
linked file. Furthermore, if the tape is then read into another machine, unless the
dump program is clever, the linked file will be copied twice onto the disk, instead
of being linked.
4.3.5 Log-Structured File Systems
Changes in technology are putting pressure on current file systems. In particu-
lar, CPUs keep getting faster, disks are becoming much bigger and cheaper (but not
much faster), and memories are growing exponentially in size. The one parameter
that is not improving by leaps and bounds is disk seek time (except for solid-state
disks, which have no seek time).
The combination of these factors means that a performance bottleneck is aris-
ing in many file systems. Research done at Berkeley attempted to alleviate this
problem by designing a completely new kind of file system, LFS (the Log-struc-
tured File System). In this section we will briefly describe how LFS works. For a
more complete treatment, see the original paper on LFS (Rosenblum and Ouster-
hout, 1991).
The idea that drove the LFS design is that as CPUs get faster and RAM memo-
ries get larger, disk caches are also increasing rapidly. Consequently, it is now pos-
sible to satisfy a very substantial fraction of all read requests directly from the
file-system cache, with no disk access needed.  It follows from this observation
that in the future, most disk accesses will be writes, so the read-ahead mechanism
used in some file systems to fetch blocks before they are needed no longer gains
much performance.
To make matters worse, in most file systems, writes are done in very small
chunks. Small writes are highly inefficient, since a 50-sec disk write is often pre-
ceded by a 10-msec seek and a 4-msec rotational delay. With these parameters,
disk efficiency drops to a fraction of 1%.
To see where all the small writes come from, consider creating a new file on a
UNIX system.      To write this file, the i-node for the directory, the directory block,
the i-node for the file, and the file itself must all be written. While these writes can
be delayed, doing so exposes the file system to serious consistency problems if a
crash occurs before the writes are done. For this reason, the i-node writes are gen-
erally done immediately.
From this reasoning, the LFS designers decided to reimplement the UNIX file
system in such a way as to achieve the full bandwidth of the disk, even in the face
of a workload consisting in large part of small random writes. The basic idea is to
structure the entire disk as a great big log.
Periodically, and when there is a special need for it, all the pending writes
being buffered in memory are collected into a single segment and written to the
disk as a single contiguous segment at the end of the log.  A single segment may



294                                       FILE SYSTEMS                               CHAP. 4
thus contain i-nodes, directory blocks, and data blocks, all mixed together.         At the
start of each segment is a segment summary, telling what can be found in the seg-
ment. If the average segment can be made to be about 1 MB, almost the full band-
width of the disk can be utilized.
      In this design, i-nodes still exist and even have the same structure as in UNIX,
but they are now scattered all over the log, instead of being at a fixed position on
the disk. Nevertheless, when an i-node is located, locating the blocks is done in the
usual way.  Of course, finding an i-node is now much harder, since its address can-
not simply be calculated from its i-number, as in UNIX.              To make it possible to
find i-nodes, an i-node map, indexed by i-number, is maintained. Entry i in this
map points to i-node i on the disk.       The map is kept on disk, but it is also cached,
so the most heavily used parts will be in memory most of the time.
      To summarize what we have said so far, all writes are initially buffered in
memory, and periodically all the buffered writes are written to the disk in a single
segment, at the end of the log. Opening a file now consists of using the map to
locate the i-node for the file.      Once the i-node has been located, the addresses of
the blocks can be found from it. All of the blocks will themselves be in segments,
somewhere in the log.
      If disks were infinitely large, the above description would be the entire story.
However, real disks are finite, so eventually the log will occupy the entire disk, at
which time no new segments can be written to the log. Fortunately, many existing
segments may have blocks that are no longer needed. For example, if a file is over-
written, its i-node will now point to the new blocks, but the old ones will still be
occupying space in previously written segments.
      To deal with this problem, LFS has a cleaner thread that spends its time scan-
ning the log circularly to compact it.          It starts out by reading the summary of the
first segment in the log to see which i-nodes and files are there.         It then checks the
current i-node map to see if the i-nodes are still current and file blocks are still in
use.  If not, that information is discarded.    The i-nodes and blocks that are still in
use go into memory to be written out in the next segment. The original segment is
then marked as free, so that the log can use it for new data.        In this manner, the
cleaner moves along the log, removing old segments from the back and putting any
live data into memory for rewriting in the next segment. Consequently, the disk is a
big circular buffer, with the writer thread adding new segments to the front and the
cleaner thread removing old ones from the back.
      The bookkeeping here is nontrivial, since when a file block is written back to a
new   segment,  the  i-node    of    the  file  (somewhere  in  the  log)  must  be  located,
updated, and put into memory to be written out in the next segment. The i-node
map must then be updated to point to the new copy.          Nevertheless, it is possible to
do the administration, and the performance results show that all this complexity is
worthwhile. Measurements given in the papers cited above show that LFS outper-
forms  UNIX     by  an  order    of  magnitude  on  small   writes,  while  having   a   per-
formance that is as good as or better than UNIX for reads and large writes.



SEC. 4.3                  FILE-SYSTEM IMPLEMENTATION                                      295
4.3.6 Journaling File Systems
While log-structured file systems are an interesting idea, they are not widely
used, in part due to their being highly incompatible with existing file systems.
Nevertheless, one of the ideas inherent in them, robustness in the face of failure,
can be easily applied to more conventional file systems. The basic idea here is to
keep a log of what the file system is going to do before it does it, so that if the sys-
tem crashes before it can do its planned work, upon rebooting the system can look
in the log to see what was going on at the time of the crash and finish the job. Such
file systems, called journaling file systems, are actually in use. Microsoft's NTFS
file system and the Linux ext3 and ReiserFS file systems all use journaling. OS X
offers journaling file systems as an option. Below we will give a brief introduction
to this topic.
To see the nature of the problem, consider a simple garden-variety operation
that happens all the time: removing a file. This operation (in UNIX) requires three
steps:
1.        Remove the file from its directory.
2.        Release the i-node to the pool of free i-nodes.
3.        Return all the disk blocks to the pool of free disk blocks.
In Windows analogous steps are required.       In the absence of system crashes, the
order in which these steps are taken does not matter; in the presence of crashes, it
does. Suppose that the first step is completed and then the system crashes.           The i-
node and file blocks will not be accessible from any file, but will also not be avail-
able for reassignment; they are just off in limbo somewhere, decreasing the avail-
able resources. If the crash occurs after the second step, only the blocks are lost.
If the order of operations is changed and the i-node is released first, then after
rebooting, the i-node may be reassigned, but the old directory entry will continue
to point to it, hence to the wrong file.  If the blocks are released first, then a crash
before the i-node is cleared will mean that a valid directory entry points to an i-
node listing blocks now in the free storage pool and which are likely to be reused
shortly, leading to two or more files randomly sharing the same blocks. None of
these outcomes are good.
What the journaling file system does is first write a log entry listing the three
actions to be completed. The log entry is then written to disk (and for good meas-
ure, possibly read back from the disk to verify that it was, in fact, written cor-
rectly).  Only after the log entry has been written, do the various operations begin.
After the operations complete successfully, the log entry is erased.   If the system
now crashes, upon recovery the file system can check the log to see if any opera-
tions were pending.  If so, all of them can be rerun (multiple times in the event of
repeated crashes) until the file is correctly removed.



296                           FILE SYSTEMS                           CHAP. 4
     To make journaling work, the logged operations must be idempotent, which
means they can be repeated as often as necessary without harm. Operations such as
``Update the bitmap to mark i-node k or block n as free'' can be repeated until the
cows come home with no danger. Similarly, searching a directory and removing
any entry called foobar is also idempotent.     On the other hand, adding the newly
freed blocks from i-node K to the end of the free list is not idempotent since they
may already be there. The more-expensive operation ``Search the list of free blocks
and add block n to it if it is not already present'' is idempotent. Journaling file sys-
tems have to arrange their data structures and loggable operations so they all are
idempotent. Under these conditions, crash recovery can be made fast and secure.
     For added reliability, a file system can introduce the database concept of an
atomic transaction.  When this concept is used, a group of actions can be brack-
eted by the begin transaction and end transaction operations. The file system then
knows it must complete either all the bracketed operations or none of them, but not
any other combinations.
     NTFS has an extensive journaling system and its structure is rarely corrupted
by system crashes. It has been in development since its first release with Windows
NT in 1993. The first Linux file system to do journaling was ReiserFS, but its pop-
ularity was impeded by the fact that it was incompatible with the then-standard
ext2 file system.  In contrast, ext3, which is a less ambitious project than ReiserFS,
also does journaling while maintaining compatibility with the previous ext2 sys-
tem.
4.3.7 Virtual File Systems
     Many different file systems are in use--often on the same computer--even for
the same operating system.  A Windows system may have a main NTFS file sys-
tem, but also a legacy FAT-32 or FAT-16 drive or partition that contains old, but
still needed, data, and from time to time a flash drive, an old CD-ROM or a DVD
(each with its own unique file system) may be required as well.      Windows handles
these disparate file systems by identifying each one with a different drive letter, as
in C:, D:, etc. When a process opens a file, the drive letter is explicitly or implicitly
present so Windows knows which file system to pass the request to. There is no at-
tempt to integrate heterogeneous file systems into a unified whole.
     In contrast, all modern UNIX systems make a very serious attempt to integrate
multiple file systems into a single structure.  A Linux system could have ext2 as
the root file system, with an ext3 partition mounted on /usr and a second hard disk
with a ReiserFS file system mounted on /home as well as an ISO 9660 CD-ROM
temporarily mounted on /mnt.  From the user's point of view, there is a single
file-system hierarchy. That it happens to encompass multiple (incompatible) file
systems is not visible to users or processes.
     However, the presence of multiple file systems is very definitely visible to the
implementation, and since the pioneering work of Sun Microsystems (Kleiman,



SEC. 4.3             FILE-SYSTEM IMPLEMENTATION                                          297
1986), most UNIX systems have used the concept of a VFS (virtual file system)
to try to integrate multiple file systems into an orderly structure. The key idea is to
abstract out that part of the file system that is common to all file systems and put
that code in a separate layer that calls the underlying concrete file systems to ac-
tually manage the data. The overall structure is illustrated in Fig. 4-18.      The dis-
cussion below is not specific to Linux or FreeBSD or any other version of UNIX,
but gives the general flavor of how virtual file systems work in UNIX systems.
          User
          process
                     POSIX
                                     Virtual file system
                                                          VFS interface
          File              FS 1        FS 2              FS 3
          system
                                        Buffer cache
                     Figure 4-18. Position of the virtual file system.
All system calls relating to files are directed to the virtual file system for initial
processing. These calls, coming from user processes, are the standard POSIX calls,
such as open, read, write, lseek, and so on. Thus the VFS has an ``upper'' interface
to user processes and it is the well-known POSIX interface.
The VFS also has a ``lower'' interface to the concrete file systems, which is
labeled VFS interface in Fig. 4-18.     This interface consists of several dozen func-
tion calls that the VFS can make to each file system to get work done. Thus to cre-
ate a new file system that works with the VFS, the designers of the new file system
must make sure that it supplies the function calls the VFS requires.        An obvious
example of such a function is one that reads a specific block from disk, puts it in
the file system's buffer cache, and returns a pointer to it. Thus the VFS has two dis-
tinct interfaces: the upper one to the user processes and the lower one to the con-
crete file systems.
While most of the file systems under the VFS represent partitions on a local
disk, this is not always the case.   In fact, the original motivation for Sun to build
the VFS was to support remote file systems using the NFS (Network File System)
protocol. The VFS design is such that as long as the concrete file system supplies
the functions the VFS requires, the VFS does not know or care where the data are
stored or what the underlying file system is like.
Internally, most VFS implementations are essentially object oriented, even if
they are written in C rather than C++.  There are several key object types that are



298                         FILE SYSTEMS                                   CHAP. 4
normally supported. These include the superblock (which describes a file system),
the v-node (which describes a file), and the directory (which describes a file sys-
tem directory). Each of these has associated operations (methods) that the concrete
file systems must support.  In addition, the VFS has some internal data structures
for its own use, including the mount table and an array of file descriptors to keep
track of all the open files in the user processes.
     To understand how the VFS works, let us run through an example chronologi-
cally. When the system is booted, the root file system is registered with the VFS.
In addition, when other file systems are mounted, either at boot time or during op-
eration, they, too must register with the VFS. When a file system registers, what it
basically does is provide a list of the addresses of the functions the VFS requires,
either as one long call vector (table) or as several of them, one per VFS object, as
the VFS demands. Thus once a file system has registered with the VFS, the VFS
knows how to, say, read a block from it--it simply calls the fourth (or whatever)
function in the vector supplied by the file system. Similarly, the VFS then also
knows how to carry out every other function the concrete file system must supply:
it just calls the function whose address was supplied when the file system regis-
tered.
     After a file system has been mounted, it can be used. For example, if a file sys-
tem has been mounted on /usr and a process makes the call
open("/usr/include/unistd.h", O RDONLY)
while parsing the path, the VFS sees that a new file system has been mounted on
/usr and locates its superblock by searching the list of superblocks of mounted file
systems. Having done this, it can find the root directory of the mounted file system
and look up the path include/unistd.h there. The VFS then creates a v-node and
makes a call to the concrete file system to return all the information in the file's i-
node. This information is copied into the v-node (in RAM), along with other infor-
mation, most importantly the pointer to the table of functions to call for operations
on v-nodes, such as read, write, close, and so on.
     After the v-node has been created, the VFS makes an entry in the file-descrip-
tor table for the calling process and sets it to point to the new v-node.  (For the
purists, the file descriptor actually points to another data structure that contains the
current file position and a pointer to the v-node, but this detail is not important for
our purposes here.)  Finally, the VFS returns the file descriptor to the caller so it
can use it to read, write, and close the file.
     Later when the process does a read using the file descriptor, the VFS locates
the v-node from the process and file descriptor tables and follows the pointer to the
table of functions, all of which are addresses within the concrete file system on
which the requested file resides. The function that handles read is now called and
code within the concrete file system goes and gets the requested block. The VFS
has no idea whether the data are coming from the local disk, a remote file system
over the network, a USB stick, or something different. The data structures involved



SEC. 4.3                 FILE-SYSTEM IMPLEMENTATION                                     299
are shown in Fig. 4-19.  Starting with the caller's process number and the file de-
scriptor, successively the v-node, read function pointer, and access function within
the concrete file system are located.
VFS
                         File
   Process               descriptors
   table
                                                     V-nodes
          ...
                                                              Function
4                              ...                            pointers
2
0                                                    ...
                                                              write
                                                              read
                                                              open
                                                                                   Call from
                                                                                   VFS into
                                                                                   FS 1
                                           Read                                    FS 1
                                           function
   Figure 4-19. A simplified view of the data structures and code used by the VFS
   and concrete file system to do a read.
In this manner, it becomes relatively straightforward to add new file systems.
To make one, the designers first get a list of function calls the VFS expects and
then write their file system to provide all of them. Alternatively, if the file system
already exists, then they have to provide wrapper functions that do what the VFS
needs, usually by making one or more native calls to the concrete file system.
4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION
Making the file system work is one thing; making it work efficiently and
robustly in real life is something quite different.  In the following sections we will
look at some of the issues involved in managing disks.



300                              FILE SYSTEMS                               CHAP. 4
4.4.1 Disk-Space Management
     Files are normally stored on disk, so management of disk space is a major con-
cern to file-system designers. Two general strategies are possible for storing an n
byte file: n consecutive bytes of disk space are allocated, or the file is split up into
a number of (not necessarily) contiguous blocks. The same trade-off is present in
memory-management systems between pure segmentation and paging.
     As we have seen, storing a file as a contiguous sequence of bytes has the ob-
vious problem that if a file grows, it may have to be moved on the disk. The same
problem holds for segments in memory, except that moving a segment in memory
is a relatively fast operation compared to moving a file from one disk position to
another. For this reason, nearly all file systems chop files up into fixed-size blocks
that need not be adjacent.
Block Size
     Once it has been decided to store files in fixed-size blocks, the question arises
how big the block should be. Given the way disks are organized, the sector, the
track, and the cylinder are obvious candidates for the unit of allocation (although
these are all device dependent, which is a minus).  In a paging system, the page
size is also a major contender.
     Having a large block size means that every file, even a 1-byte file, ties up an
entire cylinder.  It also means that small files waste a large amount of disk space.
On the other hand, a small block size means that most files will span multiple
blocks and thus need multiple seeks and rotational delays to read them, reducing
performance. Thus if the allocation unit is too large, we waste space; if it is too
small, we waste time.
     Making a good choice requires having some information about the file-size
distribution.  Tanenbaum et al. (2006) studied the file-size distribution in the Com-
puter Science Department of a large research university (the VU) in 1984 and then
again in 2005, as well as on a commercial Web server hosting a political Website
(www.electoral-vote.com).   The results are shown in Fig. 4-20, where for each
power-of-two file size, the percentage of all files smaller or equal to it is listed for
each of the three data sets. For example, in 2005, 59.13% of all files at the VU
were 4 KB or smaller and 90.84% of all files were 64 KB or smaller. The median
file size was 2475 bytes. Some people may find this small size surprising.
     What conclusions can we draw from these data?  For one thing, with a block
size of 1 KB, only about 3050% of all files fit in a single block, whereas with a
4-KB block, the percentage of files that fit in one block goes up to the 6070%
range. Other data in the paper show that with a 4-KB block, 93% of the disk blocks
are used by the 10% largest files. This means that wasting some space at the end of
each small file hardly matters because the disk is filled up by a small number of



SEC. 4.4  FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                        301
Length    VU 1984      VU 2005         Web   Length  VU 1984     VU 2005                 Web
1         1.79         1.38            6.67  16 KB       92.53                    78.92  86.79
2         1.88         1.53            7.67  32 KB       97.21                    85.87  91.65
4         2.01         1.65            8.33  64 KB       99.18                    90.84  94.80
8         2.31         1.80     11.30        128 KB      99.84                    93.73  96.93
16        3.32         2.15     11.46        256 KB      99.96                    96.12  98.48
32        5.13         3.15     12.33        512 KB      100.00                   97.73  98.99
64        8.71         4.98     26.10        1 MB        100.00                   98.87  99.62
128       14.73        8.03     28.49        2 MB        100.00                   99.44  99.80
256       23.09        13.29    32.10        4 MB        100.00                   99.71  99.87
512       34.44        20.62    39.94        8 MB        100.00                   99.86  99.94
1 KB      48.05        30.91    47.82        16 MB       100.00                   99.94  99.97
2 KB      60.87        46.09    59.44        32 MB       100.00                   99.97  99.99
4 KB      75.31        59.13    70.64        64 MB       100.00                   99.99  99.99
8 KB      84.97        69.96    79.69        128 MB      100.00                   99.99  100.00
          Figure 4-20. Percentage of files smaller than a given size (in bytes).
large files (videos) and the total amount of space taken up by the small files hardly
matters at all. Even doubling the space the smallest 90% of the files take up would
be barely noticeable.
On the other hand, using a small block means that each file will consist of
many blocks. Reading each block normally requires a seek and a rotational delay
(except on a solid-state disk), so reading a file consisting of many small blocks will
be slow.
As an example, consider a disk with 1 MB per track, a rotation time of 8.33
msec, and an average seek time of 5 msec. The time in milliseconds to read a block
of k bytes is then the sum of the seek, rotational delay, and transfer times:
                       5 + 4. 165 + (k/1000000)  8. 33
The dashed curve of Fig. 4-21 shows the data rate for such a disk as a function of
block size. To compute the space efficiency, we need to make an assumption about
the mean file size. For simplicity, let us assume that all files are 4 KB.        Although
this number is slightly larger than the data measured at the VU, students probably
have more small files than would be present in a corporate data center, so it might
be a better guess on the whole. The solid curve of Fig. 4-21 shows the space ef-
ficiency as a function of block size.
The two curves can be understood as follows. The access time for a block is
completely dominated by the seek time and rotational delay, so given that it is
going to cost 9 msec to access a block, the more data that are fetched, the better.



302                                      FILE SYSTEMS                                             CHAP. 4
                         60                                         100%
     Data rate (MB/sec)  50                                         80%   Disk space utilization
                         40
                                                                    60%
                         30
                                                                    40%
                         20
                         10                                         20%
                         0   1 KB  4 KB  16 KB  64 KB  256 KB  1MB  0%
     Figure 4-21. The dashed curve (left-hand scale) gives the data rate of a disk. The
     solid curve (right-hand scale) gives the disk-space efficiency. All files are 4 KB.
Hence the data rate goes up almost linearly with block size (until the transfers take
so long that the transfer time begins to matter).
     Now consider space efficiency. With 4-KB files and 1-KB, 2-KB, or 4-KB
blocks, files use 4, 2, and 1 block, respectively, with no wastage. With an 8-KB
block and 4-KB files, the space efficiency drops to 50%, and with a 16-KB block it
is down to 25%. In reality, few files are an exact multiple of the disk block size, so
some space is always wasted in the last block of a file.
     What the curves show, however, is that performance and space utilization are
inherently in conflict. Small blocks are bad for performance but good for disk-
space utilization.           For these data, no reasonable compromise is available. The size
closest to where the two curves cross is 64 KB, but the data rate is only 6.6 MB/sec
and the space efficiency is about 7%, neither of which is very good. Historically,
file systems have chosen sizes in the 1-KB to 4-KB range, but with disks now
exceeding 1 TB, it might be better to increase the block size to 64 KB and accept
the wasted disk space. Disk space is hardly in short supply any more.
     In an experiment to see if Windows NT file usage was appreciably different
from UNIX file usage, Vogels made measurements on files at Cornell University
(Vogels, 1999).          He observed that NT file usage is more complicated than on
UNIX. He wrote:
     When we type a few characters in the Notepad text editor, saving this to a
     file will trigger 26 system calls, including 3 failed open attempts, 1 file
     overwrite and 4 additional open and close sequences.
Nevertheless, Vogels observed a median size (weighted by usage) of files just read
as 1 KB, files just written as 2.3 KB, and files read and written as 4.2 KB.                      Given
the different data sets measurement techniques, and the year, these results are cer-
tainly compatible with the VU results.



SEC. 4.4         FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                    303
Keeping Track of Free Blocks
Once a block size has been chosen, the next issue is how to keep track of free
blocks. Two methods are widely used, as shown in Fig. 4-22.        The first one con-
sists of using a linked list of disk blocks, with each block holding as many free
disk block numbers as will fit. With a 1-KB block and a 32-bit disk block number,
each block on the free list holds the numbers of 255 free blocks.                 (One slot is re-
quired for the pointer to the next block.)               Consider a 1-TB disk, which has about 1
billion disk blocks.  To store all these addresses at 255 per block requires about 4
million blocks.  Generally, free blocks are used to hold the free list, so the storage
is essentially free.
                      Free  disk  blocks:  16,  17,  18
          42                      230                    86        1001101101101100
          136                     162                    234       0110110111110111
          210                     612                    897       1010110110110110
          97                      342                    422       0110110110111011
          41                      214                    140       1110111011101111
          63                      160                    223       1101101010001111
          21                      664                    223       0000111011010111
          48                      216                    160       1011101101101111
          262                     320                    126       1100100011101111
          310                     180                    142       0111011101110111
          516                     482                    141       1101111101110111
A 1-KB disk block can hold 256                                                    A bitmap
32-bit disk block numbers
                                  (a)                                             (b)
          Figure 4-22. (a) Storing the free list on a linked list. (b) A bitmap.
The other free-space management technique is the bitmap.                          A disk with n
blocks requires a bitmap with n bits. Free blocks are represented by 1s in the map,
allocated blocks by 0s (or vice versa).              For our example 1-TB disk, we need 1 bil-
lion bits for the map, which requires around 130,000 1-KB blocks to store.                  It is
not surprising that the bitmap requires less space, since it uses 1 bit per block, vs.
32 bits in the linked-list model. Only if the disk is nearly full (i.e., has few free
blocks) will the linked-list scheme require fewer blocks than the bitmap.
If free blocks tend to come in long runs of consecutive blocks, the free-list sys-
tem can be modified to keep track of runs of blocks rather than single blocks.              An
8-, 16-, or 32-bit count could be associated with each block giving the number of



304                              FILE SYSTEMS                                 CHAP. 4
consecutive free blocks.  In the best case, a basically empty disk could be repres-
ented by two numbers: the address of the first free block followed by the count of
free blocks.  On the other hand, if the disk becomes severely fragmented, keeping
track of runs is less efficient than keeping track of individual blocks because not
only must the address be stored, but also the count.
     This issue illustrates a problem operating system designers often have. There
are multiple data structures and algorithms that can be used to solve a problem, but
choosing the best one requires data that the designers do not have and will not have
until the system is deployed and heavily used. And even then, the data may not be
available. For example, our own measurements of file sizes at the VU in 1984 and
1995, the Website data, and the Cornell data are only four samples. While a lot bet-
ter than nothing, we have little idea if they are also representative of home com-
puters, corporate computers, government computers, and others. With some effort
we might have been able to get a couple of samples from other kinds of computers,
but even then it would be foolish to extrapolate to all computers of the kind meas-
ured.
     Getting back to the free list method for a moment, only one block of pointers
need be kept in main memory. When a file is created, the needed blocks are taken
from the block of pointers.      When it runs out, a new block of pointers is read in
from the disk. Similarly, when a file is deleted, its blocks are freed and added to
the block of pointers in main memory. When this block fills up, it is written to
disk.
     Under certain circumstances, this method leads to unnecessary disk I/O.        Con-
sider the situation of Fig. 4-23(a), in which the block of pointers in memory has
room for only two more entries.  If a three-block file is freed, the pointer block
overflows and has to be written to disk, leading to the situation of Fig. 4-23(b).          If
a three-block file is now written, the full block of pointers has to be read in again,
taking us back to Fig. 4-23(a).  If the three-block file just written was a temporary
file, when it is freed, another disk write is needed to write the full block of pointers
back to the disk.  In short, when the block of pointers is almost empty, a series of
short-lived temporary files can cause a lot of disk I/O.
     An alternative approach that avoids most of this disk I/O is to split the full
block of pointers. Thus instead of going from Fig. 4-23(a) to Fig. 4-23(b), we go
from Fig. 4-23(a) to Fig. 4-23(c) when three blocks are freed. Now the system can
handle a series of temporary files without doing any disk I/O. If the block in mem-
ory fills up, it is written to the disk, and the half-full block from the disk is read in.
The idea here is to keep most of the pointer blocks on disk full (to minimize disk
usage), but keep the one in memory about half full, so it can handle both file crea-
tion and file removal without disk I/O on the free list.
     With a bitmap, it is also possible to keep just one block in memory, going to
disk for another only when it becomes completely full or empty.  An additional
benefit of this approach is that by doing all the allocation from a single block of the
bitmap, the disk blocks will be close together, thus minimizing disk-arm motion.



SEC. 4.4       FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                        305
Main           Disk
memory
          (a)                                    (b)                                     (c)
        Figure 4-23. (a) An almost-full block of pointers to free disk blocks in memory
        and three blocks of pointers on disk. (b) Result of freeing a three-block file.
        (c) An alternative strategy for handling the three free blocks. The shaded entries
        represent pointers to free disk blocks.
Since the bitmap is a fixed-size data structure, if the kernel is (partially) paged, the
bitmap can be put in virtual memory and have pages of it paged in as needed.
Disk Quotas
To prevent people from hogging too much disk space, multiuser operating sys-
tems often provide a mechanism for enforcing disk quotas. The idea is that the sys-
tem administrator assigns each user a maximum allotment of files and blocks, and
the operating system makes sure that the users do not exceed their quotas. A typi-
cal mechanism is described below.
When a user opens a file, the attributes and disk addresses are located and put
into an open-file table in main memory. Among the attributes is an entry telling
who the owner is.  Any increases in the file's size will be charged to the owner's
quota.
A second table contains the quota record for every user with a currently open
file, even if the file was opened by someone else. This table is shown in Fig. 4-24.
It is an extract from a quota file on disk for the users whose files are currently
open. When all the files are closed, the record is written back to the quota file.
When a new entry is made in the open-file table, a pointer to the owner's quota
record is entered into it, to make it easy to find the various limits. Every time a
block is added to a file, the total number of blocks charged to the owner is incre-
mented, and a check is made against both the hard and soft limits. The soft limit
may be exceeded, but the hard limit may not.          An attempt to append to a file when
the hard block limit has been reached will result in an error. Analogous checks also
exist for the number of files to prevent a user from hogging all the i-nodes.
When a user attempts to log in, the system examines the quota file to see if the
user has exceeded the soft limit for either number of files or number of disk blocks.



306                            FILE       SYSTEMS                                       CHAP.  4
              Open file table                      Quota table
           Attributes                     Soft block limit
           disk addresses                 Hard block limit
           User = 8                       Current # of blocks
           Quota pointer                  # Block warnings left     Quota
                                                                    record
                                                   Soft file limit  for user 8
                                                   Hard file limit
                                          Current # of files
                                          # File warnings left
           Figure 4-24. Quotas are kept track of on a per-user basis in a quota table.
If either limit has been violated, a warning is displayed, and the count of warnings
remaining is reduced by one.   If the count ever gets to zero, the user has ignored
the warning one time too many, and is not permitted to log in. Getting permission
to log in again will require some discussion with the system administrator.
     This method has the property that users may go above their soft limits during a
login session, provided they remove the excess before logging out. The hard limits
may never be exceeded.
4.4.2 File-System Backups
     Destruction of a file system is often a far greater disaster than destruction of a
computer.  If a computer is destroyed by fire, lightning surges, or a cup of coffee
poured onto the keyboard, it is annoying and will cost money, but generally a re-
placement can be purchased with a minimum of fuss. Inexpensive personal com-
puters can even be replaced within an hour by just going to a computer store (ex-
cept at universities, where issuing a purchase order takes three committees, five
signatures, and 90 days).
     If a computer's file system is irrevocably lost, whether due to hardware or soft-
ware, restoring all the information will be difficult, time consuming, and in many
cases, impossible. For the people whose programs, documents, tax records, cus-
tomer files, databases, marketing plans, or other data are gone forever, the conse-
quences can be catastrophic. While the file system cannot offer any protection
against physical destruction of the equipment and media, it can help protect the
information.  It is pretty straightforward: make backups.           But that is not quite as
simple as it sounds. Let us take a look.



SEC. 4.4       FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                    307
     Most people do not think making backups of their files is worth the time and
effort--until one fine day their disk abruptly dies, at which time most of them
undergo a deathbed conversion. Companies, however, (usually) well understand the
value of their data and generally do a backup at least once a day, often to tape.
Modern tapes hold hundreds of gigabytes and cost pennies per gigabyte. Neverthe-
less, making backups is not quite as trivial as it sounds, so we will examine some
of the related issues below.
     Backups to tape are generally made to handle one of two potential problems:
     1.   Recover from disaster.
     2.   Recover from stupidity.
The first one covers getting the computer running again after a disk crash, fire,
flood, or other natural catastrophe.       In practice, these things do not happen very
often, which is why many people do not bother with backups. These people also
tend not to have fire insurance on their houses for the same reason.
     The second reason is that users often accidentally remove files that they later
need again. This problem occurs so often that when a file is ``removed'' in Win-
dows, it is not deleted at all, but just moved to a special directory, the recycle bin,
so it can be fished out and restored easily later. Backups take this principle further
and allow files that were removed days, even weeks, ago to be restored from old
backup tapes.
     Making a backup takes a long time and occupies a large amount of space, so
doing it efficiently and conveniently is important. These considerations raise the
following issues.  First, should the entire file system be backed up or only part of
it?  At many installations, the executable (binary) programs are kept in a limited
part of the file-system tree. It is not necessary to back up these files if they can all
be reinstalled from the manufacturer's Website or the installation DVD.           Also,
most systems have a directory for temporary files. There is usually no reason to
back it up either. In UNIX, all the special files (I/O devices) are kept in a directory
/dev. Not only is backing up this directory not necessary, it is downright dangerous
because the backup program would hang forever if it tried to read each of these to
completion. In short, it is usually desirable to back up only specific directories and
everything in them rather than the entire file system.
     Second, it is wasteful to back up files that have not changed since the previous
backup, which leads to the idea of incremental dumps.      The simplest form of
incremental    dumping  is    to  make  a  complete  dump  (backup)   periodically,       say
weekly or monthly, and to make a daily dump of only those files that have been
modified since the last full dump. Even better is to dump only those files that have
changed since they were last dumped. While this scheme minimizes dumping time,
it makes recovery more complicated, because first the most recent full dump has to
be restored, followed by all the incremental dumps in reverse order. To ease recov-
ery, more sophisticated incremental dumping schemes are often used.



308                                 FILE SYSTEMS                            CHAP. 4
     Third, since immense amounts of data are typically dumped, it may be desir-
able to compress the data before writing them to tape. However, with many com-
pression algorithms, a single bad spot on the backup tape can foil the decompres-
sion algorithm and make an entire file or even an entire tape unreadable. Thus the
decision to compress the backup stream must be carefully considered.
     Fourth, it is difficult to perform a backup on an active file system.  If files and
directories are being added, deleted, and modified during the dumping process, the
resulting dump may be inconsistent. However, since making a dump may take
hours, it may be necessary to take the system offline for much of the night to make
the backup, something that is not always acceptable. For this reason, algorithms
have been devised for making rapid snapshots of the file-system state by copying
critical data structures, and then requiring future changes to files and directories to
copy the blocks instead of updating them in place (Hutchinson et al., 1999). In this
way, the file system is effectively frozen at the moment of the snapshot, so it can
be backed up at leisure afterward.
     Fifth and last, making backups introduces many nontechnical problems into an
organization. The best online security system in the world may be useless if the
system administrator keeps all the backup disks or tapes in his office and leaves it
open and unguarded whenever he walks down the hall to get coffee. All a spy has
to do is pop in for a second, put one tiny disk or tape in his pocket, and saunter off
jauntily. Goodbye security.  Also, making a daily backup has little use if the fire
that burns down the computers also burns up all the backup disks. For this reason,
backup disks should be kept off-site, but that introduces more security risks (be-
cause now two sites must be secured).     For a thorough discussion of these and
other practical administration issues, see Nemeth et al. (2013). Below we will dis-
cuss only the technical issues involved in making file-system backups.
     Two strategies can be used for dumping a disk to a backup disk: a physical
dump or a logical dump.      A physical dump starts at block 0 of the disk, writes all
the disk blocks onto the output disk in order, and stops when it has copied the last
one. Such a program is so simple that it can probably be made 100% bug free,
something that can probably not be said about any other useful program.
     Nevertheless, it is worth making several comments about physical dumping.
For one thing, there is no value in backing up unused disk blocks.          If the dumping
program can obtain access to the free-block data structure, it can avoid dumping
unused blocks. However, skipping unused blocks requires writing the number of
each block in front of the block (or the equivalent), since it is no longer true that
block k on the backup was block k on the disk.
     A second concern is dumping bad blocks.    It is nearly impossible to manufac-
ture large disks without any defects.     Some bad blocks are always present. Some-
times when a low-level format is done, the bad blocks are detected, marked as bad,
and replaced by spare blocks reserved at the end of each track for just such emer-
gencies.  In  many  cases,   the    disk  controller  handles  bad-block    replacement
transparently without the operating system even knowing about it.



SEC. 4.4          FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                  309
However, sometimes blocks go bad after formatting, in which case the operat-
ing system will eventually detect them. Usually, it solves the problem by creating a
``file'' consisting of all the bad blocks--just to make sure they never appear in the
free-block pool and are never assigned. Needless to say, this file is completely
unreadable.
If all bad blocks are remapped by the disk controller and hidden from the oper-
ating system as just described, physical dumping works fine. On the other hand, if
they are visible to the operating system and maintained in one or more bad-block
files or bitmaps, it is absolutely essential that the physical dumping program get
access to this information and avoid dumping them to prevent endless disk read er-
rors while trying to back up the bad-block file.
Windows systems have paging and hibernation files that are not needed in the
event of a restore and should not be backed up in the first place. Specific systems
may also have other internal files that should not be backed up, so the dumping
program needs to be aware of them.
The main advantages of physical dumping are simplicity and great speed (basi-
cally, it can run at the speed of the disk). The main disadvantages are the inability
to skip selected directories, make incremental dumps, and restore individual files
upon request. For these reasons, most installations make logical dumps.
A logical dump starts at one or more specified directories and recursively
dumps all files and directories found there that have changed since some given
base date (e.g., the last backup for an incremental dump or system installation for a
full dump).  Thus, in a logical dump, the dump disk gets a series of carefully iden-
tified directories and files, which makes it easy to restore a specific file or directory
upon request.
Since logical dumping is the most common form, let us examine a common al-
gorithm in detail using the example of Fig. 4-25 to guide us. Most UNIX systems
use this algorithm.   In the figure we see a file tree with directories (squares) and
files (circles).  The shaded items have been modified since the base date and thus
need to be dumped. The unshaded ones do not need to be dumped.
This algorithm also dumps all directories (even unmodified ones) that lie on
the path to a modified file or directory for two reasons. The first reason is to make
it possible to restore the dumped files and directories to a fresh file system on a dif-
ferent computer.  In this way, the dump and restore programs can be used to tran-
sport entire file systems between computers.
The second reason for dumping unmodified directories above modified files is
to make it possible to incrementally restore a single file (possibly to handle recov-
ery from stupidity).  Suppose that a full file-system dump is done Sunday evening
and an incremental dump is done on Monday evening.  On Tuesday the directory
/usr/jhs/proj/nr3 is removed, along with all the directories and files under it.           On
Wednesday morning bright and early suppose the user wants to restore the file
/usr/jhs/proj/nr3/plans/summary. However, it is not possible to just restore the file
summary because there is no place to put it. The directories nr3 and plans must be



310                                       FILE SYSTEMS                                         CHAP. 4
                                           1               Root directory
     2                    5                16                  18                          27
3        4                6                17                  19                  28          29
Directory
that has not        7             10                       20              22                  30
changed
                 8     9      11      14                   21              23              31      32
                                               File that                           File that has
                          12      13  15       has changed     24          25  26  not changed
        Figure 4-25. A file system to be dumped. The squares are directories and the cir-
        cles are files. The shaded items have been modified since the last dump. Each di-
        rectory and file is labeled by its i-node number.
restored first.  To get their owners, modes, times, and whatever, correct, these di-
rectories must be present on the dump disk even though they themselves were not
modified since the previous full dump.
     The dump algorithm maintains a bitmap indexed by i-node number with sever-
al bits per i-node. Bits will be set and cleared in this map as the algorithm pro-
ceeds.  The algorithm operates in four phases. Phase 1 begins at the starting direc-
tory (the root in this example) and examines all the entries in it. For each modified
file, its i-node is marked in the bitmap.      Each directory is also marked (whether or
not it has been modified) and then recursively inspected.
     At the end of phase 1, all modified files and all directories have been marked in
the bitmap, as shown (by shading) in Fig. 4-26(a).             Phase 2 conceptually recur-
sively walks the tree again, unmarking any directories that have no modified files
or directories in them or under them. This phase leaves the bitmap as shown in
Fig. 4-26(b). Note that directories 10, 11, 14, 27, 29, and 30 are now unmarked be-
cause they contain nothing under them that has been modified. They will not be
dumped.     By way of contrast, directories 5 and 6 will be dumped even though they
themselves have not been modified because they will be needed to restore today's
changes to a fresh machine. For efficiency, phases 1 and 2 can be combined in one
tree walk.
     At this point it is known which directories and files must be dumped. These are
the ones that are marked in Fig. 4-26(b).      Phase 3 consists of scanning the i-nodes
in numerical order and dumping all the directories that are marked for dumping.



SEC. 4.4         FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                                                311
(a)  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
(b)  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
(c)  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
(d)  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
                 Figure 4-26. Bitmaps used by the logical dumping algorithm.
These are shown in Fig. 4-26(c).            Each directory is prefixed by the directory's at-
tributes (owner, times, etc.) so that they can be restored. Finally, in phase 4, the
files marked in Fig. 4-26(d) are also dumped, again prefixed by their attributes.
This completes the dump.
     Restoring a file system from the dump disk is straightforward.                                     To start with,
an empty file system is created on the disk.                    Then the most recent full dump is re-
stored. Since the directories appear first on the dump disk, they are all restored
first, giving a skeleton of the file system. Then the files themselves are restored.
This process is then repeated with the first incremental dump made after the full
dump, then the next one, and so on.
     Although logical dumping is straightforward, there are a few tricky issues. For
one, since the free block list is not a file, it is not dumped and hence it must be
reconstructed from scratch after all the dumps have been restored. Doing so is al-
ways possible since the set of free blocks is just the complement of the set of
blocks contained in all the files combined.
     Another issue is links. If a file is linked to two or more directories, it is impor-
tant that the file is restored only one time and that all the directories that are sup-
posed to point to it do so.
     Still another issue is the fact that UNIX files may contain holes.                                 It is legal to
open a file, write a few bytes, then seek to a distant file offset and write a few more
bytes. The blocks in between are not part of the file and should not be dumped and
must not be restored. Core files often have a hole of hundreds of megabytes be-
tween the data segment and the stack.               If not handled properly, each restored core
file will fill this area with zeros and thus be the same size as the virtual address
space (e.g., 232 bytes, or worse yet, 264 bytes).
     Finally, special files, named pipes, and the like (anything that is not a real file)
should never be dumped, no matter in which directory they may occur (they need
not be confined to /dev).       For more information about file-system backups, see
Chervenak et al., (1998) and Zwicky (1991).



312                                    FILE SYSTEMS                               CHAP. 4
4.4.3 File-System Consistency
     Another area where reliability is an issue is file-system consistency. Many file
systems read blocks, modify them, and write them out later.         If the system crashes
before all the modified blocks have been written out, the file system can be left in
an inconsistent state. This problem is especially critical if some of the blocks that
have not been written out are i-node blocks, directory blocks, or blocks containing
the free list.
     To deal with inconsistent file systems, most computers have a utility program
that checks file-system consistency. For example, UNIX has fsck; Windows has sfc
(and others).   This utility can be run whenever the system is booted, especially
after a crash. The description below tells how fsck works.          Sfc is somewhat dif-
ferent because it works on a different file system, but the general principle of using
the file system's inherent redundancy to repair it is still valid. All file-system
checkers verify each file system (disk partition) independently of the other ones.
     Two kinds of consistency checks can be made: blocks and files.  To check for
block consistency, the program builds two tables, each one containing a counter for
each block, initially set to 0.  The counters in the first table keep track of how
many times each block is present in a file; the counters in the second table record
how often each block is present in the free list (or the bitmap of free blocks).
     The program then reads all the i-nodes using a raw device, which ignores the
file structure and just returns all the disk blocks starting at 0.  Starting from an i-
node, it is possible to build a list of all the block numbers used in the correspond-
ing file. As each block number is read, its counter in the first table is incremented.
The program then examines the free list or bitmap to find all the blocks that are not
in use. Each occurrence of a block in the free list results in its counter in the sec-
ond table being incremented.
     If the file system is consistent, each block will have a 1 either in the first table
or in the second table, as illustrated in Fig. 4-27(a).  However, as a result of a
crash, the tables might look like Fig. 4-27(b), in which block 2 does not occur in
either table.   It will be reported as being a missing block.       While missing blocks
do no real harm, they waste space and thus reduce the capacity of the disk. The
solution to missing blocks is straightforward: the file system checker just adds
them to the free list.
     Another situation that might occur is that of Fig. 4-27(c). Here we see a block,
number 4, that occurs twice in the free list.    (Duplicates can occur only if the free
list is really a list; with a bitmap it is impossible.) The solution here is also simple:
rebuild the free list.
     The worst thing that can happen is that the same data block is present in two or
more files, as shown in Fig. 4-27(d) with block 5.       If either of these files is re-
moved, block 5 will be put on the free list, leading to a situation in which the same
block is both in use and free at the same time.  If both files are removed, the block
will be put onto the free list twice.



SEC. 4.4                FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                                    313
               Block number                                                     Block number
0  1  2  3  4  5  6  7  8    9 101112131415                      0  1  2  3  4  5  6  7  8    9 101112131415
1  1  0  1  0  1  1  1  1    0  0  1  1  1  0  0  Blocks in use  1  1  0  1  0  1  1  1  1    0  0  1  1  1  0  0  Blocks in use
0  0  1  0  1  0  0  0  0    1  1  0  0  0  1  1  Free blocks    0  0  0  0  1  0  0  0  0    1  1  0  0  0  1  1  Free blocks
                        (a)                                                              (b)
               Block number                                                        Block number
0  1  2  3  4  5  6  7  8    9 101112131415                      0  1  2  3  4  5  6  7  8    9 101112131415
1  1  0  1  0  1  1  1  1    0  0  1  1  1  0  0  Blocks in use  1  1  0  1  0  2  1  1  1    0  0  1  1  1  0  0  Blocks in use
0  0  1  0  2  0  0  0  0    1  1  0  0  0  1  1  Free blocks    0  0  1  0  1  0  0  0  0    1  1  0  0  0  1  1  Free blocks
                        (c)                                                              (d)
            Figure 4-27.        File-system states.  (a) Consistent.      (b) Missing block.           (c) Dupli-
            cate block in free list. (d) Duplicate data block.
      The appropriate action for the file-system checker to take is to allocate a free
block, copy the contents of block 5 into it, and insert the copy into one of the files.
In this way, the information content of the files is unchanged (although almost
assuredly one is garbled), but the file-system structure is at least made consistent.
The error should be reported, to allow the user to inspect the damage.
      In addition to checking to see that each block is properly accounted for, the
file-system checker also checks the directory system. It, too, uses a table of count-
ers, but these are per file, rather than per block.                          It starts at the root directory and
recursively descends the tree, inspecting each directory in the file system. For
every i-node in every directory, it increments a counter for that file's usage count.
Remember that due to hard links, a file may appear in two or more directories.
Symbolic links do not count and do not cause the counter for the target file to be
incremented.
      When the checker is all done, it has a list, indexed by i-node number, telling
how many directories contain each file.                          It then compares these numbers with the
link counts stored in the i-nodes themselves. These counts start at 1 when a file is
created and are incremented each time a (hard) link is made to the file. In a consis-
tent file system, both counts will agree.                        However, two kinds of errors can occur:
the link count in the i-node can be too high or it can be too low.
      If the link count is higher than the number of directory entries, then even if all
the files are removed from the directories, the count will still be nonzero and the i-
node will not be removed. This error is not serious, but it wastes space on the disk
with files that are not in any directory.                        It should be fixed by setting the link count
in the i-node to the correct value.
      The other error is potentially catastrophic.                        If two directory entries are linked
to a file, but the i-node says that there is only one, when either directory entry is re-
moved, the i-node count will go to zero. When an i-node count goes to zero, the



314                                   FILE SYSTEMS                  CHAP. 4
file system marks it as unused and releases all of its blocks. This action will result
in one of the directories now pointing to an unused i-node, whose blocks may soon
be assigned to other files. Again, the solution is just to force the link count in the i-
node to the actual number of directory entries.
     These two operations, checking blocks and checking directories, are often inte-
grated for efficiency reasons (i.e., only one pass over the i-nodes is required).
Other checks are also possible. For example, directories have a definite format,
with i-node numbers and ASCII names.        If an i-node number is larger than the
number of i-nodes on the disk, the directory has been damaged.
     Furthermore, each i-node has a mode, some of which are legal but strange,
such as 0007, which allows the owner and his group no access at all, but allows
outsiders to read, write, and execute the file.  It might be useful to at least report
files that give outsiders more rights than the owner. Directories with more than,
say, 1000 entries are also suspicious.  Files located in user directories, but which
are owned by the superuser and have the SETUID bit on, are potential security
problems because such files acquire the powers of the superuser when executed by
any user. With a little effort, one can put together a fairly long list of technically
legal but still peculiar situations that might be worth reporting.
     The previous paragraphs have discussed the problem of protecting the user
against crashes. Some file systems also worry about protecting the user against
himself. If the user intends to type
     rm *.o
to remove all the files ending with .o (compiler-generated object files), but accide-
ntally types
     rm * .o
(note the space after the asterisk), rm will remove all the files in the current direc-
tory and then complain that it cannot find .o.   In Windows, files that are removed
are placed in the recycle bin (a special directory), from which they can later be
retrieved if need be.  Of course, no storage is reclaimed until they are actually
deleted from this directory.
4.4.4 File-System Performance
     Access to disk is much slower than access to memory. Reading a 32-bit memo-
ry word might take 10 nsec.   Reading from a hard disk might proceed at 100
MB/sec, which is four times slower per 32-bit word, but to this must be added
510 msec to seek to the track and then wait for the desired sector to arrive under
the read head.  If only a single word is needed, the memory access is on the order
of a million times as fast as disk access.       As a result of this difference in access
time, many file systems have been designed with various optimizations to improve
performance. In this section we will cover three of them.



SEC. 4.4    FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                       315
Caching
The most common technique used to reduce disk accesses is the block cache
or buffer cache.    (Cache is pronounced ``cash'' and is derived from the French
cacher, meaning to hide.) In this context, a cache is a collection of blocks that log-
ically belong on the disk but are being kept in memory for performance reasons.
Various algorithms can be used to manage the cache, but a common one is to
check all read requests to see if the needed block is in the cache.  If it is, the read
request can be satisfied without a disk access.  If the block is not in the cache, it is
first read into the cache and then copied to wherever it is needed. Subsequent re-
quests for the same block can be satisfied from the cache.
Operation of the cache is illustrated in Fig. 4-28.         Since there are many (often
thousands of) blocks in the cache, some way is needed to determine quickly if a
given block is present. The usual way is to hash the device and disk address and
look up the result in a hash table. All the blocks with the same hash value are
chained together on a linked list so that the collision chain can be followed.
Hash table          Front (LRU)                                      Rear (MRU)
                    Figure 4-28. The buffer cache data structures.
When a block has to be loaded into a full cache, some block has to be removed
(and rewritten to the disk if it has been modified since being brought in).      This
situation is very much like paging, and all the usual page-replacement algorithms
described in Chap. 3, such as FIFO, second chance, and LRU, are applicable. One
pleasant difference between paging and caching is that cache references are rel-
atively infrequent, so that it is feasible to keep all the blocks in exact LRU order
with linked lists.
In Fig. 4-28, we see that in addition to the collision chains starting at the hash
table, there is also a bidirectional list running through all the blocks in the order of
usage, with the least recently used block on the front of this list and the most
recently used block at the end. When a block is referenced, it can be removed from
its position on the bidirectional list and put at the end.          In this way, exact LRU
order can be maintained.
Unfortunately, there is a catch. Now that we have a situation in which exact
LRU is possible, it turns out that LRU is undesirable. The problem has to do with



316                               FILE SYSTEMS                          CHAP. 4
the crashes and file-system consistency discussed in the previous section. If a criti-
cal block, such as an i-node block, is read into the cache and modified, but not
rewritten to the disk, a crash will leave the file system in an inconsistent state.        If
the i-node block is put at the end of the LRU chain, it may be quite a while before
it reaches the front and is rewritten to the disk.
     Furthermore, some blocks, such as i-node blocks, are rarely referenced two
times within a short interval. These considerations lead to a modified LRU scheme,
taking two factors into account:
     1.  Is the block likely to be needed again soon?
     2.  Is the block essential to the consistency of the file system?
For both questions, blocks can be divided into categories such as i-node blocks,
indirect blocks, directory blocks, full data blocks, and partially full data blocks.
Blocks that will probably not be needed again soon go on the front, rather than the
rear of the LRU list, so their buffers will be reused quickly. Blocks that might be
needed again soon, such as a partly full block that is being written, go on the end
of the list, so they will stay around for a long time.
     The second question is independent of the first one.  If the block is essential to
the file-system consistency (basically, everything except data blocks), and it has
been modified, it should be written to disk immediately, regardless of which end of
the LRU list it is put on.  By writing critical blocks quickly, we greatly reduce the
probability that a crash will wreck the file system. While a user may be unhappy if
one of his files is ruined in a crash, he is likely to be far more unhappy if the whole
file system is lost.
     Even with this measure to keep the file-system integrity intact, it is undesirable
to keep data blocks in the cache too long before writing them out. Consider the
plight of someone who is using a personal computer to write a book. Even if our
writer periodically tells the editor to write the file being edited to the disk, there is
a good chance that everything will still be in the cache and nothing on the disk.          If
the system crashes, the file-system structure will not be corrupted, but a whole
day's work will be lost.
     This situation need not happen very often before we have a fairly unhappy
user. Systems take two approaches to dealing with it.      The UNIX way is to have a
system call, sync, which forces all the modified blocks out onto the disk im-
mediately. When the system is started up, a program, usually called update, is
started up in the background to sit in an endless loop issuing sync calls, sleeping
for 30 sec between calls. As a result, no more than 30 seconds of work is lost due
to a crash.
     Although Windows now has a system call equivalent to sync, called FlushFile-
Buffers, in the past it did not. Instead, it had a different strategy that was in some
ways better than the UNIX approach (and in some ways worse).            What it did was
to write every modified block to disk as soon as it was written to the cache. Caches



SEC. 4.4  FILE-SYSTEM MANAGEMENT AND OPTIMIZATION                                          317
in which all modified blocks are written back to the disk immediately are called
write-through caches.   They require more disk I/O than nonwrite-through caches.
       The difference between these two approaches can be seen when a program
writes a 1-KB block full, one character at a time. UNIX will collect all the charac-
ters in the cache and write the block out once every 30 seconds, or whenever the
block is removed from the cache. With a write-through cache, there is a disk access
for every character written.  Of course, most programs do internal buffering, so
they normally write not a character, but a line or a larger unit on each write system
call.
       A consequence of this difference in caching strategy is that just removing a
disk from a UNIX system without doing a sync will almost always result in lost
data, and frequently in a corrupted file system as well. With write-through caching
no problem arises.  These differing strategies were chosen because UNIX was de-
veloped in an environment in which all disks were hard disks and not removable,
whereas the first Windows file system was inherited from MS-DOS, which started
out in the floppy-disk world. As hard disks became the norm, the UNIX approach,
with its better efficiency (but worse reliability), became the norm, and it is also
used now on Windows for hard disks. However, NTFS takes other measures (e.g.,
journaling) to improve reliability, as discussed earlier.
       Some operating systems integrate the buffer cache with the page cache. This is
especially attractive when memory-mapped files are supported.  If a file is mapped
onto memory, then some of its pages may be in memory because they were de-
mand paged in. Such pages are hardly different from file blocks in the buffer
cache.  In this case, they can be treated the same way, with a single cache for both
file blocks and pages.
Block Read Ahead
       A second technique for improving perceived file-system performance is to try
to get blocks into the cache before they are needed to increase the hit rate.  In par-
ticular, many files are read sequentially.  When the file system is asked to produce
block k in a file, it does that, but when it is finished, it makes a sneaky check in the
cache to see if block k + 1 is already there. If it is not, it schedules a read for block
k + 1 in the hope that when it is needed, it will have already arrived in the cache.
At the very least, it will be on the way.
       Of course, this read-ahead strategy works only for files that are actually being
read sequentially.  If a file is being randomly accessed, read ahead does not help.
In fact, it hurts by tying up disk bandwidth reading in useless blocks and removing
potentially useful blocks from the cache (and possibly tying up more disk band-
width writing them back to disk if they are dirty).        To see whether read ahead is
worth doing, the file system can keep track of the access patterns to each open file.
For example, a bit associated with each file can keep track of whether the file is in
``sequential-access mode'' or ``random-access mode.'' Initially, the file is given the



318                              FILE SYSTEMS                                    CHAP. 4
benefit of the doubt and put in sequential-access mode. However, whenever a seek
is done, the bit is cleared.     If sequential reads start happening again, the bit is set
once again.  In this way, the file system can make a reasonable guess about wheth-
er it should read ahead or not.  If it gets it wrong once in a while, it is not a disas-
ter, just a little bit of wasted disk bandwidth.
Reducing Disk-Arm Motion
     Caching and read ahead are not the only ways to increase file-system perfor-
mance. Another important technique is to reduce the amount of disk-arm motion
by putting blocks that are likely to be accessed in sequence close to each other,
preferably in the same cylinder. When an output file is written, the file system has
to allocate the blocks one at a time, on demand. If the free blocks are recorded in a
bitmap, and the whole bitmap is in main memory, it is easy enough to choose a free
block as close as possible to the previous block. With a free list, part of which is on
disk, it is much harder to allocate blocks close together.
     However, even with a free list, some block clustering can be done. The trick is
to keep track of disk storage not in blocks, but in groups of consecutive blocks.           If
all sectors consist of 512 bytes, the system could use 1-KB blocks (2 sectors) but
allocate disk storage in units of 2 blocks (4 sectors). This is not the same as having
2-KB disk blocks, since the cache would still use 1-KB blocks and disk transfers
would still be 1 KB, but reading a file sequentially on an otherwise idle system
would reduce the number of seeks by a factor of two, considerably improving per-
formance.    A variation on the same theme is to take account of rotational posi-
tioning.  When allocating blocks, the system attempts to place consecutive blocks
in a file in the same cylinder.
     Another performance bottleneck in systems that use i-nodes or anything like
them is that reading even a short file requires two disk accesses: one for the i-node
and one for the block. The usual i-node placement is shown in Fig. 4-29(a).        Here
all the i-nodes are near the start of the disk, so the average distance between an i-
node and its blocks will be half the number of cylinders, requiring long seeks.
     One easy performance improvement is to put the i-nodes in the middle of the
disk, rather than at the start, thus reducing the average seek between the i-node and
the first block by a factor of two. Another idea, shown in Fig. 4-29(b), is to divide
the disk into cylinder groups, each with its own i-nodes, blocks, and free list
(McKusick et al., 1984).      When creating a new file, any i-node can be chosen, but
an attempt is made to find a block in the same cylinder group as the i-node.                If
none is available, then a block in a nearby cylinder group is used.
     Of course, disk-arm movement and rotation time are relevant only if the disk
has them.    More and more computers come equipped with solid-state disks (SSD)
which have no moving parts whatsoever. For these disks, built on the same technol-
ogy as flash cards, random accesses are just as fast as sequential ones and many of
the problems of traditional disks go away. Unfortunately, new problems emerge.



SEC. 4.4  FILE-SYSTEM MANAGEMENT           AND       OPTIMIZATION                            319
                             I-nodes are                    Disk is divided into
                             located near                   cylinder groups, each
                             the start                      with its own i-nodes
                             of the disk
                                                            Cylinder                  group
          (a)                                        (b)
Figure 4-29. (a) I-nodes placed at the start of the disk. (b) Disk divided into cyl-
inder groups, each with its own blocks and i-nodes.
For instance, SSDs have peculiar properties when it comes to reading, writing, and
deleting. In particular, each block can be written only a limited number of times, so
great care is taken to spread the wear on the disk evenly.
4.4.5 Defragmenting Disks
When the operating system is initially installed, the programs and files it needs
are installed consecutively starting at the beginning of the disk, each one directly
following the previous one. All free disk space is in a single contiguous unit fol-
lowing the installed files.  However, as time goes on, files are created and removed
and typically the disk becomes badly fragmented, with files and holes all over the
place. As a consequence, when a new file is created, the blocks used for it may be
spread all over the disk, giving poor performance.
The performance can be restored by moving files around to make them contig-
uous and to put all (or at least most) of the free space in one or more large contigu-
ous regions on the disk. Windows has a program, defrag, that does precisely this.
Windows users should run it regularly, except on SSDs.
Defragmentation works better on file systems that have a lot of free space in a
contiguous region at the end of the partition. This space allows the defragmentation
program to select fragmented files near the start of the partition and copy all their
blocks to the free space.    Doing so frees up a contiguous block of space near the
start of the partition into which the original or other files can be placed contigu-
ously. The process can then be repeated with the next chunk of disk space, etc.
Some files cannot be moved, including the paging file, the hibernation file, and
the journaling log, because the administration that would be required to do this is



320                                  FILE SYSTEMS                          CHAP. 4
more trouble than it is worth.       In some systems, these are fixed-size contiguous
areas anyway, so they do not have to be defragmented. The one time when their
lack of mobility is a problem is when they happen to be near the end of the parti-
tion and the user wants to reduce the partition size. The only way to solve this
problem is to remove them altogether, resize the partition, and then recreate them
afterward.
     Linux file systems (especially ext2 and ext3) generally suffer less from defrag-
mentation than Windows systems due to the way disk blocks are selected, so man-
ual defragmentation is rarely required. Also, SSDs do not really suffer from frag-
mentation at all.  In fact, defragmenting an SSD is counterproductive. Not only is
there no gain in performance, but SSDs wear out, so defragmenting them merely
shortens their lifetimes.
4.5 EXAMPLE FILE SYSTEMS
     In the following sections we will discuss several example file systems, ranging
from quite simple to more sophisticated. Since modern UNIX file systems and
Windows 8's native file system are covered in the chapter on UNIX (Chap. 10) and
the chapter on Windows 8 (Chap. 11) we will not cover those systems here.              We
will, however, examine their predecessors below.
4.5.1 The MS-DOS File System
     The MS-DOS file system is the one the first IBM PCs came with.   It was the
main file system up through Windows 98 and Windows ME.  It is still supported
on Windows 2000, Windows XP, and Windows Vista, although it is no longer stan-
dard on new PCs now except for floppy disks. However, it and an extension of it
(FAT-32) have become widely used for many embedded systems.           Most digital
cameras use it. Many MP3 players use it exclusively. The popular Apple iPod uses
it as the default file system, although knowledgeable hackers can reformat the iPod
and install a different file system. Thus the number of electronic devices using the
MS-DOS file system is vastly larger now than at any time in the past, and certainly
much larger than the number using the more modern NTFS file system. For that
reason alone, it is worth looking at in some detail.
     To read a file, an MS-DOS program must first make an open system call to get
a handle for it. The open system call specifies a path, which may be either absolute
or relative to the current working directory. The path is looked up component by
component until the final directory is located and read into memory.  It is then
searched for the file to be opened.
     Although MS-DOS directories are variable sized, they use a fixed-size 32-byte
directory entry. The format of an MS-DOS directory entry is shown in Fig. 4-30. It
contains the file name, attributes, creation date and time, starting block, and exact



SEC. 4.5                       EXAMPLE FILE SYSTEMS                                        321
file size. File names shorter than 8 + 3 characters are left justified and padded with
spaces on the right, in each field separately. The Attributes field is new and con-
tains bits to indicate that a file is read-only, needs to be archived, is hidden, or is a
system file. Read-only files cannot be written.      This is to protect them from acci-
dental damage. The archived bit has no actual operating system function (i.e., MS-
DOS does not examine or set it).  The intention is to allow user-level archive pro-
grams to clear it upon archiving a file and to have other programs set it when modi-
fying a file.     In this way, a backup program can just examine this attribute bit on
every file to see which files to back up. The hidden bit can be set to prevent a file
from appearing in directory listings. Its main use is to avoid confusing novice users
with files they might not understand. Finally, the system bit also hides files.     In ad-
dition, system files cannot accidentally be deleted using the del command. The
main components of MS-DOS have this bit set.
           Bytes    8          3  1              10     2        2     2         4
                    File name                                             Size
                       Extension  Attributes  Reserved  Time     Date     First
                                                                          block
                                                                       number
                       Figure 4-30. The MS-DOS directory entry.
The directory entry also contains the date and time the file was created or last
modified. The time is accurate only to 2 sec because it is stored in a 2-byte field,
which can store only 65,536 unique values (a day contains 86,400 seconds).                 The
time field is subdivided into seconds (5 bits), minutes (6 bits), and hours (5 bits).
The date counts in days using three subfields: day (5 bits), month (4 bits), and year
- 1980 (7 bits).    With a 7-bit number for the year and time beginning in 1980, the
highest expressible year is 2107. Thus MS-DOS has a built-in Y2108 problem. To
avoid catastrophe, MS-DOS users should begin with Y2108 compliance as early as
possible.  If MS-DOS had used the combined date and time fields as a 32-bit sec-
onds counter, it could have represented every second exactly and delayed the catas-
trophe until 2116.
MS-DOS stores the file size as a 32-bit number, so in theory files can be as
large as 4 GB.    However, other limits (described below) restrict the maximum file
size to 2 GB or less. A surprisingly large part of the entry (10 bytes) is unused.
MS-DOS keeps track of file blocks via a file allocation table in main memory.
The directory entry contains the number of the first file block. This number is used
as an index into a 64K entry FAT in main memory. By following the chain, all the
blocks can be found. The operation of the FAT is illustrated in Fig. 4-12.
The FAT file system comes in three versions: FAT-12, FAT-16, and FAT-32, de-
pending on how many bits a disk address contains. Actually, FAT-32 is something



322                             FILE SYSTEMS                               CHAP. 4
of a misnomer, since only the low-order 28 bits of the disk addresses are used.           It
should have been called FAT-28, but powers of two sound so much neater.
     Another variant of the FAT file system is exFAT, which Microsoft introduced
for large removable devices. Apple licensed exFAT, so that there is one modern file
system that can be used to transfer files both ways between Windows and OS X
computers. Since exFAT is proprietary and Microsoft has not released the specif-
ication, we will not discuss it further here.
     For all FATs, the disk block can be set to some multiple of 512 bytes (possibly
different for each partition), with the set of allowed block sizes (called cluster
sizes by Microsoft) being different for each variant. The first version of MS-DOS
used FAT-12 with 512-byte blocks, giving a maximum partition size of 212  512
bytes (actually only 4086  512 bytes because 10 of the disk addresses were used
as special markers, such as end of file, bad block, etc.). With these parameters, the
maximum disk partition size was about 2 MB and the size of the FAT table in
memory was 4096 entries of 2 bytes each. Using a 12-bit table entry would have
been too slow.
     This system worked well for floppy disks, but when hard disks came out, it
became a problem. Microsoft solved the problem by allowing additional block
sizes of 1 KB, 2 KB, and 4 KB.  This change preserved the structure and size of
the FAT-12 table, but allowed disk partitions of up to 16 MB.
     Since MS-DOS supported four disk partitions per disk drive, the new FAT-12
file system worked up to 64-MB disks. Beyond that, something had to give.        What
happened was the introduction of FAT-16, with 16-bit disk pointers. Additionally,
block sizes of 8 KB, 16 KB, and 32 KB were permitted.          (32,768 is the largest
power of two that can be represented in 16 bits.)  The FAT-16 table now occupied
128 KB of main memory all the time, but with the larger memories by then avail-
able, it was widely used and rapidly replaced the FAT-12 file system. The largest
disk partition that can be supported by FAT-16 is 2 GB (64K entries of 32 KB
each) and the largest disk, 8 GB, namely four partitions of 2 GB each. For quite a
while, that was good enough.
     But not forever. For business letters, this limit is not a problem, but for storing
digital video using the DV standard, a 2-GB file holds just over 9 minutes of video.
As a consequence of the fact that a PC disk can support only four partitions, the
largest video that can be stored on a disk is about 38 minutes, no matter how large
the disk is. This limit also means that the largest video that can be edited on line is
less than 19 minutes, since both input and output files are needed.
     Starting with the second release of Windows 95, the FAT-32 file system, with
its 28-bit disk addresses, was introduced and the version of MS-DOS underlying
Windows 95 was adapted to support FAT-32.      In this system, partitions could theo-
retically be 228  215 bytes, but they are actually limited to 2 TB (2048 GB) be-
cause internally the system keeps track of partition sizes in 512-byte sectors using
a 32-bit number, and 29  232 is 2 TB.         The maximum partition size for various
block sizes and all three FAT types is shown in Fig. 4-31.



SEC. 4.5                EXAMPLE FILE SYSTEMS                                           323
                        Block size  FAT-12  FAT-16     FAT-32
                        0.5 KB        2 MB
                        1 KB          4 MB
                        2 KB          8 MB  128 MB
                        4 KB        16 MB   256 MB     1 TB
                        8 KB                512 MB     2 TB
                        16 KB               1024 MB    2 TB
                        32 KB               2048 MB    2 TB
Figure 4-31. Maximum partition size for     different block sizes. The  empty  boxes
represent forbidden combinations.
In addition to supporting larger disks, the FAT-32 file system has two other ad-
vantages over FAT-16.   First, an 8-GB disk using FAT-32 can be a single partition.
Using FAT-16 it has to be four partitions, which appears to the Windows user as the
C:, D:, E:, and F: logical disk drives.  It is up to the user to decide which file to
place on which drive and keep track of what is where.
The other advantage of FAT-32 over FAT-16 is that for a given size disk parti-
tion, a smaller block size can be used. For example, for a 2-GB disk partition,
FAT-16 must use 32-KB blocks; otherwise with only 64K available disk addresses,
it cannot cover the whole partition.     In contrast, FAT-32 can use, for example,
4-KB blocks for a 2-GB disk partition. The advantage of the smaller block size is
that most files are much shorter than 32 KB. If the block size is 32 KB, a file of 10
bytes ties up 32 KB of disk space.       If the average file is, say, 8 KB, then with a
32-KB block, three quarters of the disk will be wasted, not a terribly efficient way
to use the disk. With an 8-KB file and a 4-KB block, there is no disk wastage, but
the price paid is more RAM eaten up by the FAT.        With a 4-KB block and a 2-GB
disk partition, there are 512K blocks, so the FAT must have 512K entries in memo-
ry (occupying 2 MB of RAM).
MS-DOS uses the FAT to keep track of free disk blocks. Any block that is not
currently allocated is marked with a special code.     When MS-DOS needs a new
disk block, it searches the FAT for an entry containing this code. Thus no bitmap or
free list is required.
4.5.2 The UNIX V7 File System
Even early versions of UNIX had a fairly sophisticated multiuser file system
since it was derived from MULTICS.       Below we will discuss the V7 file system,
the one for the PDP-11 that made UNIX famous.          We will examine a modern
UNIX file system in the context of Linux in Chap. 10.
The file system is in the form of a tree starting at the root directory, with the
addition of links, forming a directed acyclic graph. File names can be up to 14



324                                 FILE SYSTEMS                                 CHAP. 4
characters and can contain any ASCII characters except / (because that is the sepa-
rator between components in a path) and NUL (because that is used to pad out
names shorter than 14 characters). NUL has the numerical value of 0.
     A UNIX directory entry contains one entry for each file in that directory. Each
entry is extremely simple because UNIX uses the i-node scheme illustrated in
Fig. 4-13.  A directory entry contains only two fields: the file name (14 bytes) and
the number of the i-node for that file (2 bytes), as shown in Fig. 4-32.      These pa-
rameters limit the number of files per file system to 64K.
                     Bytes  2                   14
                                                File name
                            I-node
                     number
                     Figure 4-32. A UNIX V7 directory entry.
     Like the i-node of Fig. 4-13, the UNIX i-node contains some attributes. The at-
tributes contain the file size, three times (creation, last access, and last modifica-
tion), owner, group, protection information, and a count of the number of directory
entries that point to the i-node. The latter field is needed due to links. Whenever a
new link is made to an i-node, the count in the i-node is increased. When a link is
removed, the count is decremented.  When it gets to 0, the i-node is reclaimed and
the disk blocks are put back in the free list.
     Keeping track of disk blocks is done using a generalization of Fig. 4-13 in
order to handle very large files. The first 10 disk addresses are stored in the i-node
itself, so for small files, all the necessary information is right in the i-node, which
is fetched from disk to main memory when the file is opened. For somewhat larger
files, one of the addresses in the i-node is the address of a disk block called a sin-
gle indirect block.  This block contains additional disk addresses.           If this still is
not enough, another address in the i-node, called a double indirect block, contains
the address of a block that contains a list of single indirect blocks. Each of these
single indirect blocks points to a few hundred data blocks.   If even this is not
enough, a triple indirect block can also be used. The complete picture is given in
Fig. 4-33.
     When a file is opened, the file system must take the file name supplied and
locate its disk blocks. Let us consider how the path name /usr/ast/mbox is looked
up.  We will use UNIX as an example, but the algorithm is basically the same for
all hierarchical directory systems. First the file system locates the root directory.
In UNIX its i-node is located at a fixed place on the disk. From this i-node, it
locates the root directory, which can be anywhere on the disk, but say block 1.
     After that it reads the root directory and looks up the first component of the
path, usr, in the root directory to find the i-node number of the file /usr.     Locating



SEC.            4.5             EXAMPLE FILE        SYSTEMS                             325
                I-node
                Attributes      Single
                                indirect
Disk addresses                  block
                                          Double                                 Addresses of
                                          indirect                               data blocks
                                          block
                                                                       Triple
                                                                       indirect
                                                                       block
                                          Figure 4-33. A UNIX i-node.
an i-node from its number is straightforward, since each one has a fixed location on
the disk. From this i-node, the system locates the directory for /usr and looks up
the next component, ast, in it. When it has found the entry for ast, it has the i-node
for the directory /usr/ast.     From this i-node it can find the directory itself and look
up mbox.             The i-node for this file is then read into memory and kept there until the
file is closed. The lookup process is illustrated in Fig. 4-34.
                Relative path names are looked up the same way as absolute ones, only starting
from the working directory instead of from the root directory. Every directory has
entries for .           and ..  which are put there when the directory is created. The entry .
has the i-node number for the current directory, and the entry for ..            has the i-node
number for the parent directory. Thus, a procedure looking up ../dick/prog.c simply
looks up ..          in the working directory, finds the i-node number for the parent direc-
tory, and searches that directory for dick.         No special mechanism is needed to
handle these names.             As far as the directory system is concerned, they are just or-
dinary ASCII strings, just the same as any other names.                        The only bit of trickery
here is that .. in the root directory points to itself.
4.5.3 CD-ROM File Systems
                As our last example of a file system, let us consider the file systems used on
CD-ROMs. These systems are particularly simple because they were designed for
write-once media. Among other things, for example, they have no provision for



326                               FILE SYSTEMS                                    CHAP. 4
                                  Block 132         I-node 26                 Block 406
                     I-node 6         is /usr          is for                 is /usr/ast
     Root directory  is for /usr  directory            /usr/ast               directory
     1   .           Mode         6                    Mode               26
     1   ..          size         1                    size               6
     4   bin         times        19  dick             times              64  grants
     7   dev         132          30  erik             406                92  books
     14  lib                      51  jim                                 60  mbox
     9   etc                      26  ast                                 81  minix
     6   usr                      45  bal                                 17  src
     8   tmp         I-node 6                       I-node 26
     Looking up      says that    /usr/ast          says that             /usr/ast/mbox
     usr yields      /usr is in   is i-node     /usr/ast is in                is i-node
     i-node 6        block 132        26            block 406                 60
                     Figure 4-34. The steps in looking up /usr/ast/mbox.
keeping track of free blocks because on a CD-ROM files cannot be freed or added
after the disk has been manufactured. Below we will take a look at the main CD-
ROM file system type and two extensions to it. While CD-ROMs are now old, they
are also simple, and the file systems used on DVDs and Blu-ray are based on the
one for CD-ROMS.
     Some years after the CD-ROM made its debut, the CD-R (CD Recordable) was
introduced. Unlike the CD-ROM, it is possible to add files after the initial burning,
but these are simply appended to the end of the CD-R.  Files are never removed
(although the directory can be updated to hide existing files). As a consequence of
this ``append-only'' file system, the fundamental properties are not altered. In par-
ticular, all the free space is in one contiguous chunk at the end of the CD.
The ISO 9660 File System
     The most common standard for CD-ROM file systems was adopted as an Inter-
national Standard in 1988 under the name ISO 9660.     Virtually every CD-ROM
currently on the market is compatible with this standard, sometimes with the exten-
sions to be discussed below. One goal of this standard was to make every CD-
ROM readable on every computer, independent of the byte ordering and the operat-
ing system used.     As a consequence, some limitations were placed on the file sys-
tem to make it possible for the weakest operating systems then in use (such as MS-
DOS) to read it.
     CD-ROMs do not have concentric cylinders the way magnetic disks do. In-
stead there is a single continuous spiral containing the bits in a linear sequence



SEC. 4.5                         EXAMPLE FILE SYSTEMS                                   327
(although seeks across the spiral are possible).     The bits along the spiral are divid-
ed into logical blocks (also called logical sectors) of 2352 bytes. Some of these are
for preambles, error correction, and other overhead.      The payload portion of each
logical block is 2048 bytes. When used for music, CDs have leadins, leadouts, and
intertrack gaps, but these are not used for data CD-ROMs.   Often the position of a
block along the spiral is quoted in minutes and seconds.    It can be converted to a
linear block number using the conversion factor of 1 sec = 75 blocks.
     ISO 9660 supports CD-ROM sets with as many as 216 - 1 CDs in the set. The
individual CD-ROMs may also be partitioned into logical volumes (partitions).
However, below we will concentrate on ISO 9660 for a single unpartitioned CD-
ROM.
     Every CD-ROM begins with 16 blocks whose function is not defined by the
ISO 9660 standard.    A CD-ROM manufacturer could use this area for providing a
bootstrap program to allow the computer to be booted from the CD-ROM, or for
some nefarious purpose. Next comes one block containing the primary volume
descriptor, which contains some general information about the CD-ROM.                  This
information includes the system identifier (32 bytes), volume identifier (32 bytes),
publisher identifier (128 bytes), and data preparer identifier (128 bytes).  The man-
ufacturer can fill in these fields in any desired way, except that only uppercase let-
ters, digits, and a very small number of punctuation marks may be used to ensure
cross-platform compatibility.
     The primary volume descriptor also contains the names of three files, which
may     contain  the  abstract,  copyright  notice,  and  bibliographic  information,    re-
spectively.  In addition, certain key numbers are also present, including the logical
block size (normally 2048, but 4096, 8192, and larger powers of 2 are allowed in
certain cases), the number of blocks on the CD-ROM, and the creation and expira-
tion dates of the CD-ROM.        Finally, the primary volume descriptor also contains a
directory entry for the root directory, telling where to find it on the CD-ROM (i.e.,
which block it starts at).  From this directory, the rest of the file system can be lo-
cated.
     In addition to the primary volume descriptor, a CD-ROM may contain a sup-
plementary volume descriptor.    It contains similar information to the primary, but
that will not concern us here.
     The root directory, and every other directory for that matter, consists of a vari-
able number of entries, the last of which contains a bit marking it as the final one.
The directory entries themselves are also variable length.  Each directory entry
consists of 10 to 12 fields, of which some are in ASCII and others are numerical
fields in binary. The binary fields are encoded twice, once in little-endian format
(used on Pentiums, for example) and once in big-endian format (used on SPARCs,
for example).    Thus, a 16-bit number uses 4 bytes and a 32-bit number uses 8
bytes.
     The use of this redundant coding was necessary to avoid hurting anyone's feel-
ings when the standard was developed.       If the standard had dictated little endian,



328                                          FILE SYSTEMS                                     CHAP. 4
then people from companies whose products were big endian would have felt like
second-class citizens and would not have accepted the standard. The emotional
content   of  a  CD-ROM           can  thus     be  quantified     and  measured   exactly    in  kilo-
bytes/hour of wasted space.
     The format of an ISO 9660 directory entry is illustrated in Fig. 4-35. Since di-
rectory entries have variable lengths, the first field is a byte telling how long the
entry is. This byte is defined to have the high-order bit on the left to avoid any
ambiguity.
                                                                                              Padding
Bytes  1  1      8                     8            7           1  2         4  1  4-15
              Location of file    File size     Date and time           CD #    L  File name      Sys
              Extended attribute record length      Flags
          Directory entry length                       Interleave       Base name       Ext  ;   Ver
                                  Figure 4-35. The ISO 9660 directory enty.
     Directory entries may optionally have extended attributes.                    If this feature is
used, the second byte tells how long the extended attributes are.
     Next comes the starting block of the file itself. Files are stored as contiguous
runs of blocks, so a file's location is completely specified by the starting block and
the size, which is contained in the next field.
     The date and time that the CD-ROM was recorded is stored in the next field,
with separate bytes for the year, month, day, hour, minute, second, and time zone.
Years begin to count at 1900, which means that CD-ROMs will suffer from a
Y2156 problem because the year following 2155 will be 1900. This problem could
have been delayed by defining the origin of time to be 1988 (the year the standard
was adopted).    Had that been done, the problem would have been postponed until
2244. Every 88 extra years helps.
     The Flags field contains a few miscellaneous bits, including one to hide the
entry in listings (a feature copied from MS-DOS), one to distinguish an entry that
is a file from an entry that is a directory, one to enable the use of the extended at-
tributes, and one to mark the last entry in a directory.                A few other bits are also
present in this field but they will not concern us here. The next field deals with
interleaving pieces of files in a way that is not used in the simplest version of ISO
9660, so we will not consider it further.
     The next field tells which CD-ROM the file is located on. It is permitted that a
directory entry on one CD-ROM refers to a file located on another CD-ROM in the
set. In this way, it is possible to build a master directory on the first CD-ROM that
lists all the files on all the CD-ROMs in the complete set.
     The field marked L in Fig. 4-35 gives the size of the file name in bytes.                    It is
followed by the file name itself.            A file name consists of a base name, a dot, an



SEC. 4.5               EXAMPLE FILE SYSTEMS                                            329
extension, a semicolon, and a binary version number (1 or 2 bytes).          The base
name and extension may use uppercase letters, the digits 09, and the underscore
character. All other characters are forbidden to make sure that every computer can
handle every file name. The base name can be up to eight characters; the extension
can be up to three characters. These choices were dictated by the need to be MS-
DOS compatible.  A file name may be present in a directory multiple times, as
long as each one has a different version number.
      The last two fields are not always present. The Padding field is used to force
every directory entry to be an even number of bytes, to align the numeric fields of
subsequent entries on 2-byte boundaries.       If padding is needed, a 0 byte is used.
Finally, we have the System use field. Its function and size are undefined, except
that it must be an even number of bytes. Different systems use it in different ways.
The Macintosh keeps Finder flags here, for example.
      Entries within a directory are listed in alphabetical order except for the first
two entries. The first entry is for the directory itself.  The second one is for its par-
ent.  In this respect, these entries are similar to the UNIX . and .. directory entries.
The files themselves need not be in directory order.
      There is no explicit limit to the number of entries in a directory. However,
there is a limit to the depth of nesting.  The maximum depth of directory nesting is
eight. This limit was arbitrarily set to make some implementations simpler.
      ISO 9660 defines what are called three levels. Level 1 is the most restrictive
and specifies that file names are limited to 8 + 3 characters as we have described,
and also requires all files to be contiguous as we have described. Furthermore, it
specifies that directory names be limited to eight characters with no extensions.
Use of this level maximizes the chances that a CD-ROM can be read on every
computer.
      Level 2 relaxes the length restriction.  It allows files and directories to have
names of up to 31 characters, but still from the same set of characters.
      Level 3 uses the same name limits as level 2, but partially relaxes the assump-
tion that files have to be contiguous. With this level, a file may consist of several
sections (extents), each of which is a contiguous run of blocks. The same run may
occur multiple times in a file and may also occur in two or more files.      If large
chunks of data are repeated in several files, level 3 provides some space optimiza-
tion by not requiring the data to be present multiple times.
Rock Ridge Extensions
      As we have seen, ISO 9660 is highly restrictive in several ways. Shortly after it
came out, people in the UNIX community began working on an extension to make
it possible to represent UNIX file systems on a CD-ROM.       These extensions were
named Rock Ridge, after a town in the Mel Brooks movie Blazing Saddles, proba-
bly because one of the committee members liked the film.



330                                 FILE SYSTEMS                                 CHAP. 4
     The extensions use the System use field in order to make Rock Ridge CD-
ROMs readable on any computer. All the other fields retain their normal ISO 9660
meaning.  Any system not aware of the Rock Ridge extensions just ignores them
and sees a normal CD-ROM.
     The extensions are divided up into the following fields:
     1.   PX - POSIX attributes.
     2.   PN - Major and minor device numbers.
     3.   SL - Symbolic link.
     4.   NM - Alternative name.
     5.   CL - Child location.
     6.   PL - Parent location.
     7.   RE - Relocation.
     8.   TF - Time stamps.
The  PX   field  contains  the   standard  UNIX  rwxrwxrwx     permission  bits  for        the
owner, group, and others.       It also contains the other bits contained in the mode
word, such as the SETUID and SETGID bits, and so on.
     To allow raw devices to be represented on a CD-ROM, the PN field is present.
It contains the major and minor device numbers associated with the file.         In this
way, the contents of the /dev directory can be written to a CD-ROM and later
reconstructed correctly on the target system.
     The SL field is for symbolic links. It allows a file on one file system to refer to
a file on a different file system.
     The most important field is NM. It allows a second name to be associated with
the file. This name is not subject to the character set or length restrictions of ISO
9660, making it possible to express arbitrary UNIX file names on a CD-ROM.
     The next three fields are used together to get around the ISO 9660 limit of di-
rectories that may be nested only eight deep. Using them it is possible to specify
that a directory is to be relocated, and to tell where it goes in the hierarchy. It is ef-
fectively a way to work around the artificial depth limit.
     Finally, the TF field contains the three timestamps included in each UNIX i-
node, namely the time the file was created, the time it was last modified, and the
time it was last accessed. Together, these extensions make it possible to copy a
UNIX file system to a CD-ROM and then restore it correctly to a different system.
Joliet Extensions
     The UNIX community was not the only group that did not like ISO 9660 and
wanted a way to extend it.      Microsoft also found it too restrictive (although it was
Microsoft's own MS-DOS that caused most of the restrictions in the first place).



SEC. 4.5                     EXAMPLE FILE SYSTEMS                                       331
Therefore Microsoft invented some extensions that were called Joliet.  They were
designed to allow Windows file systems to be copied to CD-ROM and then restor-
ed, in precisely the same way that Rock Ridge was designed for UNIX.   Virtually
all programs that run under Windows and use CD-ROMs support Joliet, including
programs that burn CD-recordables. Usually, these programs offer a choice be-
tween the various ISO 9660 levels and Joliet.
The major extensions provided by Joliet are:
1.         Long file names.
2.         Unicode character set.
3.         Directory nesting deeper than eight levels.
4.         Directory names with extensions
The first extension allows file names up to 64 characters. The second extension
enables the use of the Unicode character set for file names. This extension is im-
portant for software intended for use in countries that do not use the Latin alpha-
bet, such as Japan, Israel, and Greece. Since Unicode characters are 2 bytes, the
maximum file name in Joliet occupies 128 bytes.
Like Rock Ridge, the limitation on directory nesting is removed by Joliet. Di-
rectories can be nested as deeply as needed.   Finally, directory names can have ex-
tensions.  It is not clear why this extension was included, since Windows direc-
tories virtually never use extensions, but maybe some day they will.
4.6 RESEARCH ON FILE SYSTEMS
File systems have always attracted more research than other parts of the oper-
ating system and that is still the case.    Entire conferences such as FAST, MSST,
and NAS, are devoted largely to file and storage systems.  While standard file sys-
tems are fairly well understood, there is still quite a bit of research going on about
backups (Smaldone et al., 2013; and Wallace et al., 2012) caching (Koller et al.;
Oh, 2012; and Zhang et al., 2013a), erasing data securely (Wei et al., 2011), file
compression (Harnik et al., 2013), flash file systems (No, 2012; Park and Shen,
2012; and Narayanan, 2009), performance (Leventhal, 2013; and Schindler et al.,
2011), RAID (Moon and Reddy, 2013), reliability and recovery from errors (Chi-
dambaram et al., 2013; Ma et. al, 2013; McKusick, 2012; and Van Moolenbroek et
al., 2012), user-level file systems (Rajgarhia and Gehani, 2010), verifying consis-
tency (Fryer et al., 2012), and versioning file systems (Mashtizadeh et al., 2013).
Just measuring what is actually going in a file system is also a research topic (Har-
ter et al., 2012).
Security is a perennial topic (Botelho et al., 2013; Li et al., 2013c; and Lorch
et al., 2013).  In contrast, a hot new topic is cloud file systems (Mazurek et al.,



332                                           FILE SYSTEMS                             CHAP. 4
2012;  and  Vrable     et  al.,       2012).  Another  area  that  has  been  getting  attention
recently is provenance--keeping track of the history of the data, including where
they came from, who owns them, and how they have been transformed (Ghoshal
and Plale, 2013; and Sultana and Bertino, 2013).             Keeping data safe and useful for
decades is also of interest to companies that have a legal requirement to do so
(Baker et al., 2006).      Finally, other researchers are rethinking the file system stack
(Appuswamy et al., 2011).
4.7 SUMMARY
     When seen from the outside, a file system is a collection of files and direc-
tories, plus operations on them. Files can be read and written, directories can be
created and destroyed, and files can be moved from directory to directory. Most
modern file systems support a hierarchical directory system in which directories
may have subdirectories and these may have subsubdirectories ad infinitum.
     When seen from the inside, a file system looks quite different. The file system
designers have to be concerned with how storage is allocated, and how the system
keeps track of which block goes with which file. Possibilities include contiguous
files, linked lists, file-allocation tables, and i-nodes. Different systems have dif-
ferent directory structures.          Attributes can go in the directories or somewhere else
(e.g., an i-node).  Disk space can be managed using free lists or bitmaps. File-sys-
tem reliability is enhanced by making incremental dumps and by having a program
that can repair sick file systems.            File-system performance is important and can be
enhanced in several ways, including caching, read ahead, and carefully placing the
blocks of a file close to each other. Log-structured file systems also improve per-
formance by doing writes in large units.
     Examples of file systems include ISO 9660, -DOS, and UNIX.               These differ in
many ways, including how they keep track of which blocks go with which file, di-
rectory structure, and management of free disk space.
                                              PROBLEMS
1.   Give five different path names for the file /etc/passwd.      (Hint: Think about the direc-
     tory entries ``.'' and ``..''.)
2.   In Windows, when a user double clicks on a file listed by Windows Explorer, a pro-
     gram is run and given that file as a parameter. List two different ways the operating
     system could know which program to run.



CHAP. 4                               PROBLEMS                                        333
3.   In early UNIX systems, executable files (a.out files) began with a very specific magic
     number, not one chosen at random. These files began with a header, followed by the
     text and data segments.  Why do you think a very specific number was chosen for ex-
     ecutable files, whereas other file types had a more-or-less random magic number as the
     first word?
4.   Is the open system call in UNIX  absolutely essential?  What   would the  consequences
     be of not having it?
5.   Systems that support sequential files always have an operation to rewind files.  Do sys-
     tems that support random-access files need this, too?
6.   Some operating systems provide a system call rename to give a file a new name.                Is
     there any difference at all between using this call to rename a file and just copying the
     file to a new file with the new name, followed by deleting the old one?
7.   In some systems it is possible to map part of a file into memory. What restrictions must
     such systems impose? How is this partial mapping implemented?
8.   A simple operating system supports only a single directory but allows it to have arbi-
     trarily many files with arbitrarily long file names. Can something approximating a hier-
     archical file system be simulated? How?
9.   In UNIX and Windows, random access is done by having a special system call that
     moves the ``current position'' pointer associated with a file to a given byte in the file.
     Propose an alternative way to do random access without having this system call.
10.  Consider the directory tree of Fig. 4-8.   If /usr/jim is the working directory, what is the
     absolute path name for the file whose relative path name is ../ast/x?
11.  Contiguous allocation of files leads to disk fragmentation, as mentioned in the text, be-
     cause some space in the last disk block will be wasted in files whose length is not an
     integral number of blocks.  Is this internal fragmentation or external fragmentation?
     Make an analogy with something discussed in the previous chapter.
12.  Describe the effects of a corrupted data block for a given file for: (a) contiguous, (b)
     linked, and (c) indexed (or table based).
13.  One way to use contiguous allocation of the disk and not suffer from holes is to com-
     pact the disk every time a file is removed. Since all files are contiguous, copying a file
     requires a seek and rotational delay to read the file, followed by the transfer at full
     speed. Writing the file back requires the same work.   Assuming a seek time of 5 msec,
     a rotational delay of 4 msec, a transfer rate of 80 MB/sec, and an average file size of 8
     KB, how long does it take to read a file into main memory and then write it back to the
     disk at a new location?  Using these numbers, how long would it take to compact half
     of a 16-GB disk?
14.  In light of the answer to the previous question, does compacting the disk ever make
     any sense?
15.  Some digital consumer devices need to store data, for example as files. Name a modern
     device that requires file storage and for which contiguous allocation would be a fine
     idea.



334                               FILE SYSTEMS                                            CHAP. 4
16.  Consider the i-node shown in Fig. 4-13.     If it contains 10 direct addresses and these
     were 8 bytes each and all disk blocks were 1024 KB, what would the largest possible
     file be?
17.  For a given class, the student records are stored in a file. The records are randomly ac-
     cessed and updated. Assume that each student's record is of fixed size. Which of the
     three  allocation  schemes   (contiguous,   linked  and  table/indexed)   will  be   most    ap-
     propriate?
18.  Consider a file whose size varies between 4 KB and 4 MB during its lifetime. Which
     of the three allocation schemes (contiguous, linked and table/indexed) will be most ap-
     propriate?
19.  It has been suggested that efficiency could be improved and disk space saved by stor-
     ing the data of a short file within the i-node. For the i-node of Fig. 4-13, how many
     bytes of data could be stored inside the i-node?
20.  Two computer science students, Carolyn and Elinor, are having a discussion about i-
     nodes. Carolyn maintains that memories have gotten so large and so cheap that when a
     file is opened, it is simpler and faster just to fetch a new copy of the i-node into the i-
     node table, rather than search the entire table to see if it is already there. Elinor dis-
     agrees. Who is right?
21.  Name one advantage of hard links over symbolic links and one advantage of symbolic
     links over hard links.
22.  Explain how hard links and soft links differ with respective to i-node allocations.
23.  Consider a 4-TB disk that uses 4-KB blocks and the free-list method. How many block
     addresses can be stored in one block?
24.  Free disk space can be kept track of using a free list or a bitmap. Disk addresses re-
     quire D bits. For a disk with B blocks, F of which are free, state the condition under
     which the free list uses less space than the bitmap. For D having the value 16 bits,
     express your answer as a percentage of the disk space that must be free.
25.  The beginning of a free-space bitmap looks like this after the disk partition is first for-
     matted: 1000 0000 0000 0000 (the first block is used by the root directory). The sys-
     tem always searches for free blocks starting at the lowest-numbered block, so after
     writing file A, which uses six blocks, the bitmap looks like this: 1111 1110 0000 0000.
     Show the bitmap after each of the following additional actions:
     (a) File B is written, using five blocks.
     (b) File A is deleted.
     (c) File C is written, using eight blocks.
     (d) File B is deleted.
26.  What would happen if the bitmap or free list containing the information about free disk
     blocks was completely lost due to a crash?  Is there any way to recover from this disas-
     ter, or is it bye-bye disk?  Discuss your answers for UNIX and the FAT-16 file system
     separately.



CHAP. 4                                  PROBLEMS                                                 335
27.  Oliver Owl's night job at the university computing center is to change the tapes used
     for overnight data backups. While waiting for each tape to complete, he works on writ-
     ing his thesis that proves Shakespeare's plays were written by extraterrestrial visitors.
     His text processor runs on the system being backed up since that is the only one they
     have. Is there a problem with this arrangement?
28.  We discussed making incremental dumps in some detail in the text.        In Windows it is
     easy to tell when to dump a file because every file has an archive bit. This bit is miss-
     ing in UNIX. How do UNIX backup programs know which files to dump?
29.  Suppose that file 21 in Fig. 4-25 was not modified since the last dump.     In what way
     would the four bitmaps of Fig. 4-26 be different?
30.  It has been suggested that the first part of each UNIX file be kept in the same disk
     block as its i-node. What good would this do?
31.  Consider Fig. 4-27. Is it possible that for some particular block number the counters in
     both lists have the value 2? How should this problem be corrected?
32.  The performance of a file system depends upon the cache hit rate (fraction of blocks
     found in the cache). If it takes 1 msec to satisfy a request from the cache, but 40 msec
     to satisfy a request if a disk read is needed, give a formula for the mean time required
     to satisfy a request if the hit rate is h.  Plot this function for values of h varying from 0
     to 1.0.
33.  For an external USB hard drive attached to a computer, which is more suitable: a write-
     through cache or a block cache?
34.  Consider an application where students' records are stored in a file. The application
     takes a student ID as input and subsequently reads, updates, and writes the correspond-
     ing student record; this is repeated till the application quits. Would the "block read-
     ahead" technique be useful here?
35.  Consider a disk that has 10 data blocks starting from block 14 through 23. Let there be
     2 files on the disk: f1 and f2. The directory structure lists that the first data blocks of f1
     and f2 are respectively 22 and 16. Given the FAT table entries as below, what are the
     data blocks allotted to f1 and f2?
     (14,18); (15,17); (16,23); (17,21); (18,20); (19,15); (20, -1); (21, -1); (22,19); (23,14).
     In the above notation, (x, y) indicates that the value stored in table entry x points to data
     block y.
36.  Consider the idea behind Fig. 4-21, but now for a disk with a mean seek time of 6
     msec, a rotational rate of 15,000 rpm, and 1,048,576 bytes per track. What are the data
     rates for block sizes of 1 KB, 2 KB, and 4 KB, respectively?
37.  A certain file system uses 4-KB disk blocks. The median file size is 1 KB.  If all files
     were exactly 1 KB, what fraction of the disk space would be wasted? Do you think the
     wastage for a real file system will be higher than this number or lower than it? Explain
     your answer.



336                                      FILE SYSTEMS                                CHAP. 4
38.  Given a disk-block size of 4 KB and block-pointer address value of 4 bytes, what is the
     largest file size (in bytes) that can be accessed using 10 direct addresses and one indi-
     rect block?
39.  Files in MS-DOS have to compete for space in the FAT-16 table in memory. If one file
     uses k entries, that is k entries that are not available to any other file, what constraint
     does this place on the total length of all files combined?
40.  A UNIX file system has 4-KB blocks and 4-byte disk addresses. What is the maximum
     file size if i-nodes contain 10 direct entries, and one single, double, and triple indirect
     entry each?
41.  How many disk operations are needed to fetch the i-node for afile with the path name
     /usr/ast/courses/os/handout.t? Assume that the i-node for the root directory is in mem-
     ory, but nothing else along the path is in memory. Also assume that all directories fit in
     one disk block.
42.  In many UNIX systems, the i-nodes are kept at the start of the disk. An alternative de-
     sign is to allocate an i-node when a file is created and put the i-node at the start of the
     first block of the file. Discuss the pros and cons of this alternative.
43.  Write a program that reverses the bytes of a file, so that the last byte is now first and
     the first byte is now last.      It must work with an arbitrarily long file, but try to make it
     reasonably efficient.
44.  Write a program that starts at a given directory and descends the file tree from that
     point recording the sizes of all the files it finds. When it is all done, it should print a
     histogram of the file sizes using a bin width specified as a parameter (e.g., with 1024,
     file sizes of 0 to 1023 go in one bin, 1024 to 2047 go in the next bin, etc.).
45.  Write a program that scans all directories in a UNIX file system and finds and locates
     all i-nodes with a hard link count of two or more. For each such file, it lists together all
     file names that point to the file.
46.  Write a new version of the UNIX ls program. This version takes as an argument one or
     more directory names and for each directory lists all the files in that directory, one line
     per file. Each field should be formatted in a reasonable way given its type.    List only
     the first disk address, if any.
47.  Implement a program to measure the impact of application-level buffer sizes on read
     time. This involves writing to and reading from a large file (say, 2 GB). Vary the appli-
     cation buffer size (say, from 64 bytes to 4 KB). Use timing measurement routines (such
     as gettimeofday and getitimer on UNIX) to measure the time taken for different buffer
     sizes. Analyze the results and report your findings: does buffer size make a difference
     to the overall write time and per-write time?
48.  Implement a simulated file system that will be fully contained in a single regular file
     stored on the disk.    This disk file will contain directories, i-nodes, free-block infor-
     mation, file data blocks, etc. Choose appropriate algorithms for maintaining free-block
     information and for allocating data blocks (contiguous, indexed, linked).       Your pro-
     gram will accept system commands from the user to create/delete directories, cre-
     ate/delete/open files, read/write from/to a selected file, and to list directory contents.
