The Linux System


The Linux                                                      18C H A P T E R
System
      Updated by Robert Love
      This chapter presents an in-depth examination of the Linux operating system.
      By examining a complete, real system, we can see how the concepts we have
      discussed relate both to one another and to practice.
         Linux is a variant of UNIX that has gained popularity over the last several
      decades, powering devices as small as mobile phones and as large as room-
      filling supercomputers. In this chapter, we look at the history and development
      of Linux and cover the user and programmer interfaces that Linux presents
      --interfaces that owe a great deal to the UNIX tradition. We also discuss the
      design and implementation of these interfaces. Linux is a rapidly evolving
      operating system. This chapter describes developments through the Linux 3.2
      kernel, which was released in 2012.
      CHAPTER OBJECTIVES
      ·  To explore the history of the UNIX operating system from which Linux is
         derived and the principles upon which Linux's design is based.
      ·  To examine the Linux process model and illustrate how Linux schedules
         processes and provides interprocess communication.
      ·  To look at memory management in Linux.
      ·  To explore how Linux implements file systems and manages I/O devices.
18.1  Linux History
      Linux  looks  and  feels  much  like  any  other   UNIX  system;   indeed,  UNIX
      compatibility has been a major design goal of the Linux project. However,
      Linux is much younger than most UNIX systems. Its development began in
      1991, when a Finnish university student, Linus Torvalds, began developing
      a small but self-contained kernel for the 80386 processor, the first true 32-bit
      processor in Intel's range of PC-compatible CPUs.
                                                                                        781



782  Chapter 18    The Linux System
          Early in its development, the Linux source code was made available free--
     both at no cost and with minimal distributional restrictions--on the Internet.
     As a result, Linux's history has been one of collaboration by many developers
     from all around the world, corresponding almost exclusively over the Internet.
     From an initial kernel that partially implemented a small subset of the UNIX
     system services, the Linux system has grown to include all of the functionality
     expected of a modern UNIX system.
          In its early days, Linux development revolved largely around the central
     operating-system    kernel -- the   core,  privileged   executive   that   manages   all
     system   resources  and  interacts  directly      with  the  computer  hardware.     We
     need much more than this kernel, of course, to produce a full operating
     system. We thus need to make a distinction between the Linux kernel and
     a complete Linux system. The Linux kernel is an original piece of software
     developed from scratch by the Linux community. The Linux system, as we
     know it today, includes a multitude of components, some written from scratch,
     others borrowed from other development projects, and still others created in
     collaboration with other teams.
          The basic Linux system is a standard environment for applications and
     user programming, but it does not enforce any standard means of managing
     the available functionality as a whole. As Linux has matured, a need has arisen
     for another layer of functionality on top of the Linux system. This need has
     been met by various Linux distributions. A Linux distribution includes all the
     standard components of the Linux system, plus a set of administrative tools
     to simplify the initial installation and subsequent upgrading of Linux and to
     manage installation and removal of other packages on the system. A modern
     distribution  also  typically  includes    tools  for   management     of  file  systems,
     creation and management of user accounts, administration of networks, Web
     browsers, word processors, and so on.
     18.1.1   The Linux Kernel
     The first Linux kernel released to the public was version 0.01, dated May 14,
     1991. It had no networking, ran only on 80386-compatible Intel processors
     and  PC  hardware,  and  had     extremely    limited   device-driver      support.  The
     virtual memory subsystem was also fairly basic and included no support
     for memory-mapped files; however, even this early incarnation supported
     shared pages with copy-on-write and protected address spaces. The only file
     system supported was the Minix file system, as the first Linux kernels were
     cross-developed on a Minix platform.
          The next milestone, Linux 1.0, was released on March 14, 1994. This release
     culminated three years of rapid development of the Linux kernel. Perhaps the
     single biggest new feature was networking: 1.0 included support for UNIX's
     standard TCP/IP networking protocols, as well as a BSD-compatible socket
     interface for networking programming. Device-driver support was added for
     running IP over Ethernet or (via the PPP or SLIP protocols) over serial lines or
     modems.
          The 1.0 kernel also included a new, much enhanced file system without the
     limitations of the original Minix file system, and it supported a range of SCSI
     controllers for high-performance disk access. The developers extended the vir-
     tual memory subsystem to support paging to swap files and memory mapping



                                                     18.1  Linux History           783
of arbitrary files (but only read-only memory mapping was implemented in
1.0).
A range of extra hardware support was included in this release. Although
still restricted to the Intel PC platform, hardware support had grown to include
floppy-disk and CD-ROM devices, as well as sound cards, a range of mice, and
international keyboards. Floating-point emulation was provided in the kernel
for 80386 users who had no 80387 math coprocessor. System V UNIX-style
interprocess communication (IPC), including shared memory, semaphores,
and message queues, was implemented.
At this point, development started on the 1.1 kernel stream, but numerous
bug-fix patches were released subsequently for 1.0. A pattern was adopted as
the standard numbering convention for Linux kernels. Kernels with an odd
minor-version number, such as 1.1 or 2.5, are development kernels; even-
numbered minor-version numbers are stable production kernels. Updates
for the stable kernels are intended only as remedial versions, whereas the
development kernels may include newer and relatively untested functionality.
As we will see, this pattern remained in effect until version 3.
In March 1995, the 1.2 kernel was released. This release did not offer
nearly the same improvement in functionality as the 1.0 release, but it did
support a much wider variety of hardware, including the new PCI hardware
bus architecture. Developers added another PC-specific feature --support for
the 80386 CPU's virtual 8086 mode --to allow emulation of the DOS operating
system for PC computers. They also updated the IP implementation with
support  for  accounting  and  firewalling.  Simple  support      for  dynamically
loadable and unloadable kernel modules was supplied as well.
The 1.2 kernel was the final PC-only Linux kernel. The source distribution
for Linux 1.2 included partially implemented support for SPARC, Alpha, and
MIPS CPUs, but full integration of these other architectures did not begin until
after the 1.2 stable kernel was released.
The Linux 1.2 release concentrated on wider hardware support and more
complete implementations of existing functionality. Much new functionality
was under development at the time, but integration of the new code into the
main kernel source code was deferred until after the stable 1.2 kernel was
released. As a result, the 1.3 development stream saw a great deal of new
functionality added to the kernel.
This work was released in June 1996 as Linux version 2.0. This release
was given a major version-number increment because of two major new
capabilities: support for multiple architectures, including a 64-bit native Alpha
port, and symmetric multiprocessing (SMP) support. Additionally, the memory-
management code was substantially improved to provide a unified cache for
file-system data independent of the caching of block devices. As a result
of this change, the kernel offered greatly increased file-system and virtual-
memory performance. For the first time, file-system caching was extended
to networked file systems, and writable memory-mapped regions were also
supported. Other major improvements included the addition of internal kernel
threads, a mechanism exposing dependencies between loadable modules,
support for the automatic loading of modules on demand, file-system quotas,
and POSIX-compatible real-time process-scheduling classes.



784  Chapter 18  The Linux System
     Improvements continued with the release of Linux 2.2 in 1999. A port to
     UltraSPARC systems was added. Networking was enhanced with more flexible
     firewalling, improved routing and traffic management, and support for TCP
     large window and selective acknowledgement. Acorn, Apple, and NT disks
     could now be read, and NFS was enhanced with a new kernel-mode NFS
     daemon. Signal handling, interrupts, and some I/O were locked at a finer
     level than before to improve symmetric multiprocessor (SMP) performance.
     Advances in the 2.4 and 2.6 releases of the kernel included increased
     support for SMP systems, journaling file systems, and enhancements to the
     memory-management and block I/O systems. The process scheduler was
     modified in version 2.6, providing an efficient O(1) scheduling algorithm. In
     addition, the 2.6 kernel was preemptive, allowing a process to be preempted
     even while running in kernel mode.
     Linux kernel version 3.0 was released in July 2011. The major version bump
     from 2 to 3 occurred to commemorate the twentieth anniversary of Linux.
     New features include improved virtualization support, a new page write-back
     facility, improvements to the memory-management system, and yet another
     new process scheduler--the Completely Fair Scheduler (CFS). We focus on this
     newest kernel in the remainder of this chapter.
     18.1.2  The Linux System
     As we noted earlier, the Linux kernel forms the core of the Linux project, but
     other components make up a complete Linux operating system. Whereas the
     Linux kernel is composed entirely of code written from scratch specifically for
     the Linux project, much of the supporting software that makes up the Linux
     system is not exclusive to Linux but is common to a number of UNIX-like
     operating systems. In particular, Linux uses many tools developed as part
     of Berkeley's BSD operating system, MIT's X Window System, and the Free
     Software Foundation's GNU project.
     This sharing of tools has worked in both directions. The main system
     libraries of Linux were originated by the GNU project, but the Linux community
     greatly improved the libraries by addressing omissions, inefficiencies, and
     bugs. Other components, such as the GNU C compiler (gcc), were already of
     sufficiently high quality to be used directly in Linux. The network administra-
     tion tools under Linux were derived from code first developed for 4.3 BSD, but
     more recent BSD derivatives, such as FreeBSD, have borrowed code from Linux
     in return. Examples of this sharing include the Intel floating-point-emulation
     math library and the PC sound-hardware device drivers.
     The     Linux  system  as  a  whole  is  maintained  by  a  loose  network       of
     developers collaborating over the Internet, with small groups or individuals
     having responsibility for maintaining the integrity of specific components.
     A small number of public Internet file-transfer-protocol (FTP) archive sites
     act as de facto standard repositories for these components. The File System
     Hierarchy Standard document is also maintained by the Linux community
     as a means of ensuring compatibility across the various system components.
     This standard specifies the overall layout of a standard Linux file system; it
     determines under which directory names configuration files, libraries, system
     binaries, and run-time data files should be stored.



                                                           18.1  Linux History         785
18.1.3    Linux Distributions
In theory, anybody can install a Linux system by fetching the latest revisions
of the necessary system components from the FTP sites and compiling them. In
Linux's early days, this is precisely what a Linux user had to do. As Linux has
matured, however, various individuals and groups have attempted to make
this job less painful by providing standard, precompiled sets of packages for
easy installation.
   These  collections,    or    distributions,  include    much  more   than     just  the
basic  Linux     system.  They     typically   include  extra  system-installation     and
management utilities, as well as precompiled and ready-to-install packages
of many of the common UNIX tools, such as news servers, web browsers,
text-processing and editing tools, and even games.
   The    first  distributions     managed     these  packages   by  simply  providing
a  means  of     unpacking    all  the  files  into   the  appropriate  places.  One   of
the important contributions of modern distributions, however, is advanced
package management. Today's Linux distributions include a package-tracking
database that allows packages to be installed, upgraded, or removed painlessly.
   The SLS distribution, dating back to the early days of Linux, was the first
collection of Linux packages that was recognizable as a complete distribution.
Although it could be installed as a single entity, SLS lacked the package-
management       tools  now   expected  of      Linux   distributions.  The  Slackware
distribution represented a great improvement in overall quality, even though
it also had poor package management. In fact, it is still one of the most widely
installed distributions in the Linux community.
   Since Slackware's release, many commercial and noncommercial Linux
distributions have become available. Red Hat and Debian are particularly pop-
ular distributions; the first comes from a commercial Linux support company
and the second from the free-software Linux community. Other commercially
supported versions of Linux include distributions from Canonical and SuSE,
and others too numerous to list here. There are too many Linux distributions in
circulation for us to list all of them here. The variety of distributions does not
prevent Linux distributions from being compatible, however. The RPM package
file format is used, or at least understood, by the majority of distributions, and
commercial applications distributed in this format can be installed and run on
any distribution that can accept RPM files.
18.1.4    Linux Licensing
The Linux kernel is distributed under version 2.0 of the GNU General Public
License (GPL), the terms of which are set out by the Free Software Foundation.
Linux is not public-domain software. Public domain implies that the authors
have waived copyright rights in the software, but copyright rights in Linux
code are still held by the code's various authors. Linux is free software, however,
in the sense that people can copy it, modify it, use it in any manner they want,
and give away (or sell) their own copies.
   The main implication of Linux's licensing terms is that nobody using Linux,
or creating a derivative of Linux (a legitimate exercise), can distribute the
derivative without including the source code. Software released under the GPL
cannot be redistributed as a binary-only product. If you release software that
includes any components covered by the GPL, then, under the GPL, you must



786   Chapter 18     The Linux System
      make source code available alongside any binary distributions. (This restriction
      does not prohibit making--or even selling--binary software distributions, as
      long as anybody who receives binaries is also given the opportunity to get the
      originating source code for a reasonable distribution charge.)
18.2  Design Principles
      In its overall design, Linux resembles other traditional, nonmicrokernel UNIX
      implementations. It is a multiuser, preemptively multitasking system with a
      full set of UNIX-compatible tools. Linux's file system adheres to traditional UNIX
      semantics, and the standard UNIX networking model is fully implemented. The
      internal details of Linux's design have been influenced heavily by the history
      of this operating system's development.
      Although Linux runs on a wide variety of platforms, it was originally
      developed exclusively on PC architecture. A great deal of that early devel-
      opment was carried out by individual enthusiasts rather than by well-funded
      development or research facilities, so from the start Linux attempted to squeeze
      as much functionality as possible from limited resources. Today, Linux can run
      happily on a multiprocessor machine with many gigabytes of main memory
      and many terabytes of disk space, but it is still capable of operating usefully in
      under 16 MB of RAM.
      As PCs became more powerful and as memory and hard disks became
      cheaper,  the  original,  minimalist  Linux  kernels  grew  to  implement  more
      UNIX functionality. Speed and efficiency are still important design goals, but
      much recent and current work on Linux has concentrated on a third major
      design goal: standardization. One of the prices paid for the diversity of UNIX
      implementations currently available is that source code written for one may not
      necessarily compile or run correctly on another. Even when the same system
      calls are present on two different UNIX systems, they do not necessarily behave
      in exactly the same way. The POSIX standards comprise a set of specifications
      for different aspects of operating-system behavior. There are POSIX documents
      for common operating-system functionality and for extensions such as process
      threads and real-time operations. Linux is designed to comply with the relevant
      POSIX documents, and at least two Linux distributions have achieved official
      POSIX certification.
      Because it gives standard interfaces to both the programmer and the user,
      Linux presents few surprises to anybody familiar with UNIX. We do not detail
      these interfaces here. The sections on the programmer interface (Section A.3)
      and user interface (Section A.4) of BSD apply equally well to Linux. By default,
      however, the Linux programming interface adheres to SVR4 UNIX semantics,
      rather than to BSD behavior. A separate set of libraries is available to implement
      BSD semantics in places where the two behaviors differ significantly.
      Many other standards exist in the UNIX world, but full certification of
      Linux with respect to these standards is sometimes slowed because certification
      is often available only for a fee, and the expense involved in certifying an
      operating system's compliance with most standards is substantial. However,
      supporting a wide base of applications is important for any operating system,
      so implementation of standards is a major goal for Linux development, even
      if the implementation is not formally certified. In addition to the basic POSIX



                                                     18.2       Design Principles     787
standard, Linux currently supports the POSIX threading extensions--Pthreads
--and a subset of the POSIX extensions for real-time process control.
18.2.1   Components of a Linux System
The Linux system is composed of three main bodies of code, in line with most
traditional UNIX implementations:
1.  Kernel.   The    kernel  is    responsible  for  maintaining         all  the  important
    abstractions of the operating system, including such things as virtual
    memory and processes.
2.  System libraries. The system libraries define a standard set of functions
    through which applications can interact with the kernel. These functions
    implement much of the operating-system functionality that does not need
    the full privileges of kernel code. The most important system library is
    the C library, known as libc. In addition to providing the standard C
    library, libc implements the user mode side of the Linux system call
    interface, as well as other critical system-level interfaces.
3.  System utilities. The system utilities are programs that perform indi-
    vidual, specialized management tasks. Some system utilities are invoked
    just once to initialize and configure some aspect of the system. Others
    --known as daemons in UNIX terminology--run permanently, handling
    such tasks as responding to incoming network connections, accepting
    logon requests from terminals, and updating log files.
    Figure 18.1 illustrates the various components that make up a full Linux
system.  The  most   important     distinction   here       is  between  the  kernel  and
everything else. All the kernel code executes in the processor's privileged
mode with full access to all the physical resources of the computer. Linux
refers to this privileged mode as kernel mode. Under Linux, no user code is
built into the kernel. Any operating-system-support code that does not need to
run in kernel mode is placed into the system libraries and runs in user mode.
Unlike kernel mode, user mode has access only to a controlled subset of the
system's resources.
         system-             user                    user
        management           processes               utility             compilers
         programs                               programs
                                   system shared libraries
                                   Linux kernel
                                   loadable kernel modules
                     Figure  18.1  Components of the Linux      system.



788  Chapter 18  The Linux System
     Although various modern operating systems have adopted a message-
     passing architecture for their kernel internals, Linux retains UNIX's historical
     model: the kernel is created as a single, monolithic binary. The main reason
     is performance. Because all kernel code and data structures are kept in a
     single address space, no context switches are necessary when a process calls an
     operating-system function or when a hardware interrupt is delivered. More-
     over, the kernel can pass data and make requests between various subsystems
     using relatively cheap C function invocation and not more complicated inter-
     process communication (IPC). This single address space contains not only the
     core scheduling and virtual memory code but all kernel code, including all
     device drivers, file systems, and networking code.
     Even though all the kernel components share this same melting pot, there
     is still room for modularity. In the same way that user applications can load
     shared libraries at run time to pull in a needed piece of code, so the Linux
     kernel can load (and unload) modules dynamically at run time. The kernel
     does not need to know in advance which modules may be loaded --they are
     truly independent loadable components.
     The Linux kernel forms the core of the Linux operating system. It provides
     all the functionality necessary to run processes, and it provides system services
     to give arbitrated and protected access to hardware resources. The kernel
     implements all the features required to qualify as an operating system. On
     its own, however, the operating system provided by the Linux kernel is not
     a complete UNIX system. It lacks much of the functionality and behavior of
     UNIX, and the features that it does provide are not necessarily in the format
     in which a UNIX application expects them to appear. The operating-system
     interface visible to running applications is not maintained directly by the
     kernel. Rather, applications make calls to the system libraries, which in turn
     call the operating-system services as necessary.
     The system libraries provide many types of functionality. At the simplest
     level, they allow applications to make system calls to the Linux kernel. Making
     a system call involves transferring control from unprivileged user mode to
     privileged kernel mode; the details of this transfer vary from architecture to
     architecture. The libraries take care of collecting the system-call arguments and,
     if necessary, arranging those arguments in the special form necessary to make
     the system call.
     The libraries may also provide more complex versions of the basic system
     calls. For example, the C language's buffered file-handling functions are all
     implemented in the system libraries, providing more advanced control of file
     I/O than the basic kernel system calls. The libraries also provide routines that do
     not correspond to system calls at all, such as sorting algorithms, mathematical
     functions, and string-manipulation routines. All the functions necessary to
     support the running of UNIX or POSIX applications are implemented in the
     system libraries.
     The Linux system includes a wide variety of user-mode programs--both
     system utilities and user utilities. The system utilities include all the programs
     necessary to initialize and then administer the system, such as those to set
     up networking interfaces and to add and remove users from the system.
     User utilities are also necessary to the basic operation of the system but do
     not require elevated privileges to run. They include simple file-management
     utilities such as those to copy files, create directories, and edit text files. One



                                                        18.3     Kernel Modules           789
      of the most important user utilities is the shell, the standard command-line
      interface on UNIX systems. Linux supports many shells; the most common is
      the bourne-Again shell (bash).
18.3  Kernel Modules
      The Linux kernel has the ability to load and unload arbitrary sections of kernel
      code on demand. These loadable kernel modules run in privileged kernel mode
      and as a consequence have full access to all the hardware capabilities of the
      machine on which they run. In theory, there is no restriction on what a kernel
      module is allowed to do. Among other things, a kernel module can implement
      a device driver, a file system, or a networking protocol.
          Kernel modules are convenient for several reasons. Linux's source code is
      free, so anybody wanting to write kernel code is able to compile a modified
      kernel  and  to  reboot  into  that  new  functionality.   However,   recompiling,
      relinking, and reloading the entire kernel is a cumbersome cycle to undertake
      when you are developing a new driver. If you use kernel modules, you do not
      have to make a new kernel to test a new driver--the driver can be compiled
      on its own and loaded into the already running kernel. Of course, once a new
      driver is written, it can be distributed as a module so that other users can
      benefit from it without having to rebuild their kernels.
          This latter point has another implication. Because it is covered by the
      GPL license, the Linux kernel cannot be released with proprietary components
      added to it unless those new components are also released under the GPL and
      the source code for them is made available on demand. The kernel's module
      interface allows third parties to write and distribute, on their own terms, device
      drivers or file systems that could not be distributed under the GPL.
          Kernel modules allow a Linux system to be set up with a standard minimal
      kernel, without any extra device drivers built in. Any device drivers that the
      user needs can be either loaded explicitly by the system at startup or loaded
      automatically by the system on demand and unloaded when not in use. For
      example, a mouse driver can be loaded when a USB mouse is plugged into the
      system and unloaded when the mouse is unplugged.
          The module support under Linux has four components:
      1.  The module-management system allows modules to be loaded into
          memory and to communicate with the rest of the kernel.
      2.  The module loader and unloader, which are user-mode utilities, work
          with the module-management system to load a module into memory.
      3.  The driver-registration system allows modules to tell the rest of the
          kernel that a new driver has become available.
      4.  Aconflict-resolution       mechanism  allows  different  device   drivers       to
          reserve hardware resources and to protect those resources from accidental
          use by another driver.
      18.3.1  Module Management
      Loading a module requires more than just loading its binary contents into
      kernel memory. The system must also make sure that any references the



790  Chapter 18   The Linux System
     module makes to kernel symbols or entry points are updated to point to the
     correct locations in the kernel's address space. Linux deals with this reference
     updating by splitting the job of module loading into two separate sections: the
     management of sections of module code in kernel memory and the handling
     of symbols that modules are allowed to reference.
     Linux maintains an internal symbol table in the kernel. This symbol table
     does not contain the full set of symbols defined in the kernel during the latter's
     compilation; rather, a symbol must be explicitly exported. The set of exported
     symbols constitutes a well-defined interface by which a module can interact
     with the kernel.
     Although exporting symbols from a kernel function requires an explicit
     request by the programmer, no special effort is needed to import those symbols
     into a module. A module writer just uses the standard external linking of the
     C language. Any external symbols referenced by the module but not declared
     by it are simply marked as unresolved in the final module binary produced by
     the compiler. When a module is to be loaded into the kernel, a system utility
     first scans the module for these unresolved references. All symbols that still
     need to be resolved are looked up in the kernel's symbol table, and the correct
     addresses of those symbols in the currently running kernel are substituted into
     the module's code. Only then is the module passed to the kernel for loading. If
     the system utility cannot resolve all references in the module by looking them
     up in the kernel's symbol table, then the module is rejected.
     The loading of the module is performed in two stages. First, the module-
     loader utility asks the kernel to reserve a continuous area of virtual kernel
     memory  for  the  module.  The  kernel  returns    the  address  of  the  memory
     allocated, and the loader utility can use this address to relocate the module's
     machine code to the correct loading address. A second system call then passes
     the module, plus any symbol table that the new module wants to export, to the
     kernel. The module itself is now copied verbatim into the previously allocated
     space, and the kernel's symbol table is updated with the new symbols for
     possible use by other modules not yet loaded.
     The final module-management component is the module requester. The
     kernel defines a communication interface to which a module-management
     program can connect. With this connection established, the kernel will inform
     the management process whenever a process requests a device driver, file
     system, or network service that is not currently loaded and will give the
     manager the opportunity to load that service. The original service request will
     complete once the module is loaded. The manager process regularly queries
     the kernel to see whether a dynamically loaded module is still in use and
     unloads that module when it is no longer actively needed.
     18.3.2  Driver Registration
     Once a module is loaded, it remains no more than an isolated region of memory
     until it lets the rest of the kernel know what new functionality it provides.
     The kernel maintains dynamic tables of all known drivers and provides a
     set of routines to allow drivers to be added to or removed from these tables
     at any time. The kernel makes sure that it calls a module's startup routine
     when that module is loaded and calls the module's cleanup routine before



                                                 18.3       Kernel Modules         791
that module is unloaded. These routines are responsible for registering the
module's functionality.
    A module may register many types of functionality; it is not limited
to only one type. For example, a device driver might want to register two
separate mechanisms for accessing the device. Registration tables include,
among others, the following items:
·   Device drivers. These drivers include character devices (such as printers,
    terminals, and mice), block devices (including all disk drives), and network
    interface devices.
·   File systems. The file system may be anything that implements Linux's
    virtual file system calling routines. It might implement a format for storing
    files on a disk, but it might equally well be a network file system, such as
    NFS, or a virtual file system whose contents are generated on demand, such
    as Linux's /proc file system.
·   Network    protocols.  A  module       may   implement   an  entire  networking
    protocol,  such  as  TCP  or  simply a  new  set  of packet-filtering  rules for
    a network firewall.
·   Binary format. This format specifies a way of recognizing, loading, and
    executing a new type of executable file.
In addition, a module can register a new set of entries in the sysctl and /proc
tables, to allow that module to be configured dynamically (Section 18.7.4).
18.3.3  Conflict Resolution
Commercial UNIX implementations are usually sold to run on a vendor's own
hardware. One advantage of a single-supplier solution is that the software
vendor has a good idea about what hardware configurations are possible.
PC  hardware,    however,  comes    in  a  vast  number  of  configurations,     with
large numbers of possible drivers for devices such as network cards and
video display adapters. The problem of managing the hardware configuration
becomes more severe when modular device drivers are supported, since the
currently active set of devices becomes dynamically variable.
    Linux provides a central conflict-resolution mechanism to help arbitrate
access to certain hardware resources. Its aims are as follows:
·   To prevent modules from clashing over access to hardware resources
·   To  prevent  autoprobes -- device-driver     probes  that    auto-detect     device
    configuration--from interfering with existing device drivers
·   To resolve conflicts among multiple drivers trying to access the same
    hardware --as, for example, when both the parallel printer driver and
    the parallel line IP (PLIP) network driver try to talk to the parallel port
    To these ends, the kernel maintains lists of allocated hardware resources.
The PC has a limited number of possible I/O ports (addresses in its hardware
I/O address space), interrupt lines, and DMA channels. When any device driver
wants to access such a resource, it is expected to reserve the resource with



792   Chapter 18  The Linux System
      the kernel database first. This requirement incidentally allows the system
      administrator to determine exactly which resources have been allocated by
      which driver at any given point.
         A module is expected to use this mechanism to reserve in advance any
      hardware resources that it expects to use. If the reservation is rejected because
      the resource is not present or is already in use, then it is up to the module
      to decide how to proceed. It may fail in its initialization attempt and request
      that it be unloaded if it cannot continue, or it may carry on, using alternative
      hardware resources.
18.4  Process Management
      A process is the basic context in which all user-requested activity is serviced
      within the operating system. To be compatible with other UNIX systems, Linux
      must use a process model similar to those of other versions of UNIX. Linux
      operates differently from UNIX in a few key places, however. In this section,
      we review the traditional UNIX process model (Section A.3.2) and introduce
      Linux's threading model.
      18.4.1    The fork() and exec() Process Model
      The basic principle of UNIX process management is to separate into two steps
      two operations that are usually combined into one: the creation of a new
      process and the running of a new program. A new process is created by the
      fork() system call, and a new program is run after a call to exec(). These are
      two distinctly separate functions. We can create a new process with fork()
      without running a new program--the new subprocess simply continues to
      execute exactly the same program, at exactly the same point, that the first
      (parent) process was running. In the same way, running a new program does
      not require that a new process be created first. Any process may call exec() at
      any time. A new binary object is loaded into the process's address space and
      the new executable starts executing in the context of the existing process.
         This model has the advantage of great simplicity. It is not necessary to
      specify every detail of the environment of a new program in the system call that
      runs that program. The new program simply runs in its existing environment.
      If a parent process wishes to modify the environment in which a new program
      is to be run, it can fork and then, still running the original executable in a child
      process, make any system calls it requires to modify that child process before
      finally executing the new program.
         Under UNIX, then, a process encompasses all the information that the
      operating system must maintain to track the context of a single execution of a
      single program. Under Linux, we can break down this context into a number of
      specific sections. Broadly, process properties fall into three groups: the process
      identity, environment, and context.
      18.4.1.1  Process Identity
      A process identity consists mainly of the following items:
      ·  Process ID (PID). Each process has a unique identifier. The PID is used to
         specify the process to the operating system when an application makes a



                                   18.4              Process Management              793
     system call to signal, modify, or wait for the process. Additional identifiers
     associate the process with a process group (typically, a tree of processes
     forked by a single user command) and login session.
·    Credentials. Each process must have an associated user ID and one or more
     group IDs (user groups are discussed in Section 11.6.2) that determine the
     rights of a process to access system resources and files.
·    Personality. Process personalities are not traditionally found on UNIX
     systems, but under Linux each process has an associated personality
     identifier that can slightly modify the semantics of certain system calls.
     Personalities are primarily used by emulation libraries to request that
     system calls be compatible with certain varieties of UNIX.
·    Namespace. Each process is associated with a specific view of the file-
     system hierarchy, called its namespace. Most processes share a common
     namespace and thus operate on a shared file-system hierarchy. Processes
     and their children can, however, have different namespaces, each with a
     unique file-system hierarchy--their own root directory and set of mounted
     file systems.
Most of these identifiers are under the limited control of the process itself.
The  process  group  and  session  identifiers  can  be      changed  if  the  process
wants to start a new group or session. Its credentials can be changed, subject
to appropriate security checks. However, the primary PID of a process is
unchangeable and uniquely identifies that process until termination.
18.4.1.2  Process Environment
A process's environment is inherited from its parent and is composed of two
null-terminated vectors: the argument vector and the environment vector. The
argument vector simply lists the command-line arguments used to invoke the
running program; it conventionally starts with the name of the program itself.
The environment vector is a list of "NAME=VALUE" pairs that associates named
environment variables with arbitrary textual values. The environment is not
held in kernel memory but is stored in the process's own user-mode address
space as the first datum at the top of the process's stack.
     The argument and environment vectors are not altered when a new process
is created. The new child process will inherit the environment of its parent.
However, a completely new environment is set up when a new program
is invoked. On calling exec(), a process must supply the environment for
the new program. The kernel passes these environment variables to the next
program, replacing the process's current environment. The kernel otherwise
leaves the environment and command-line vectors alone--their interpretation
is left entirely to the user-mode libraries and applications.
     The passing of environment variables from one process to the next and the
inheriting of these variables by the children of a process provide flexible ways
to pass information to components of the user-mode system software. Various
important environment variables have conventional meanings to related parts
of the system software. For example, the TERM variable is set up to name the
type of terminal connected to a user's login session. Many programs use this



794  Chapter 18  The Linux System
     variable to determine how to perform operations on the user's display, such as
     moving the cursor and scrolling a region of text. Programs with multilingual
     support use the LANG variable to determine the language in which to display
     system messages for programs that include multilingual support.
        The environment-variable mechanism custom-tailors the operating system
     on a per-process basis. Users can choose their own languages or select their
     own editors independently of one another.
     18.4.1.3  Process Context
     The process identity and environment properties are usually set up when a
     process is created and not changed until that process exits. A process may
     choose to change some aspects of its identity if it needs to do so, or it may
     alter its environment. In contrast, process context is the state of the running
     program at any one time; it changes constantly. Process context includes the
     following parts:
     ·  Scheduling context. The most important part of the process context is its
        scheduling context--the information that the scheduler needs to suspend
        and restart the process. This information includes saved copies of all the
        process's registers. Floating-point registers are stored separately and are
        restored only when needed. Thus, processes that do not use floating-point
        arithmetic do not incur the overhead of saving that state. The scheduling
        context also includes information about scheduling priority and about any
        outstanding signals waiting to be delivered to the process. A key part of
        the scheduling context is the process's kernel stack, a separate area of
        kernel memory reserved for use by kernel-mode code. Both system calls
        and interrupts that occur while the process is executing will use this stack.
     ·  Accounting.    The  kernel  maintains   accounting      information  about      the
        resources currently being consumed by each process and the total resources
        consumed by the process in its entire lifetime so far.
     ·  File table. The file table is an array of pointers to kernel file structures
        representing open files. When making file-I/O system calls, processes refer
        to files by an integer, known as a file descriptor (fd), that the kernel uses
        to index into this table.
     ·  File-system context. Whereas the file table lists the existing open files, the
        file-system context applies to requests to open new files. The file-system
        context includes the process's root directory, current working directory,
        and namespace.
     ·  Signal-handler table. UNIX systems can deliver asynchronous signals to
        a process in response to various external events. The signal-handler table
        defines the action to take in response to a specific signal. Valid actions
        include ignoring the signal, terminating the process, and invoking a routine
        in the process's address space.
     ·  Virtual memory context. The virtual memory context describes the full
        contents of a process's private address space; we discuss it in Section 18.6.



                                                         18.5             Scheduling        795
      18.4.2  Processes and Threads
      Linux provides the fork() system call, which duplicates a process without
      loading a new executable image. Linux also provides the ability to create
      threads via the clone() system call. Linux does not distinguish between
      processes and threads, however. In fact, Linux generally uses the term task
      --rather than process or thread--when referring to a flow of control within a
      program. The clone() system call behaves identically to fork(), except that
      it accepts as arguments a set of flags that dictate what resources are shared
      between the parent and child (whereas a process created with fork() shares
      no resources with its parent). The flags include:
                  flag                       meaning
                  CLONE_FS             File-system information is shared.
                  CLONE_VM             The same memory space is shared.
              CLONE_SIGHAND                  Signal handlers are shared.
                  CLONE_FILES          The set of open files is shared.
      Thus, if clone() is passed the flags CLONE FS, CLONE VM, CLONE SIGHAND,
      and CLONE FILES, the parent and child tasks will share the same file-system
      information (such as the current working directory), the same memory space,
      the same signal handlers, and the same set of open files. Using clone() in this
      fashion is equivalent to creating a thread in other systems, since the parent
      task shares most of its resources with its child task. If none of these flags is set
      when clone() is invoked, however, the associated resources are not shared,
      resulting in functionality similar to that of the fork() system call.
      The lack of distinction between processes and threads is possible because
      Linux does not hold a process's entire context within the main process data
      structure. Rather, it holds the context within independent subcontexts. Thus,
      a process's file-system context, file-descriptor table, signal-handler table, and
      virtual memory context are held in separate data structures. The process data
      structure simply contains pointers to these other structures, so any number of
      processes can easily share a subcontext by pointing to the same subcontext and
      incrementing a reference count.
      The arguments to the clone() system call tell it which subcontexts to copy
      and which to share. The new process is always given a new identity and a new
      scheduling context--these are the essentials of a Linux process. According to
      the arguments passed, however, the kernel may either create new subcontext
      data structures initialized so as to be copies of the parent's or set up the new
      process to use the same subcontext data structures being used by the parent.
      The fork() system call is nothing more than a special case of clone() that
      copies all subcontexts, sharing none.
18.5  Scheduling
      Scheduling is the job of allocating CPU time to different tasks within an operat-
      ing system. Linux, like all UNIX systems, supports preemptive multitasking.
      In such a system, the process scheduler decides which process runs and when.



796  Chapter 18      The Linux System
     Making these decisions in a way that balances fairness and performance across
     many different workloads is one of the more complicated challenges in modern
     operating systems.
         Normally, we think of scheduling as the running and interrupting of user
     processes, but another aspect of scheduling is also important to Linux: the
     running of the various kernel tasks. Kernel tasks encompass both tasks that are
     requested by a running process and tasks that execute internally on behalf of
     the kernel itself, such as tasks spawned by Linux's I/O subsystem.
     18.5.1     Process Scheduling
     Linux has two separate process-scheduling algorithms. One is a time-sharing
     algorithm for fair, preemptive scheduling among multiple processes. The other
     is designed for real-time tasks, where absolute priorities are more important
     than fairness.
         The scheduling algorithm used for routine time-sharing tasks received
     a   major  overhaul   with  version  2.6  of   the  kernel.   Earlier  versions  ran    a
     variation of the traditional UNIX scheduling algorithm. This algorithm does
     not provide adequate support for SMP systems, does not scale well as the
     number of tasks on the system grows, and does not maintain fairness among
     interactive tasks, particularly on systems such as desktops and mobile devices.
     The process scheduler was first overhauled with version 2.5 of the kernel.
     Version 2.5 implemented a scheduling algorithm that selects which task to
     run in constant time --known as O(1)--regardless of the number of tasks
     or  processors  in    the  system.  The  new   scheduler  also  provided  increased
     support    for  SMP,  including     processor  affinity  and  load  balancing.   These
     changes, while improving scalability, did not improve interactive performance
     or fairness--and, in fact, made these problems worse under certain workloads.
     Consequently, the process scheduler was overhauled a second time, with Linux
     kernel version 2.6. This version ushered in the Completely Fair Scheduler
     (CFS).
         The Linux scheduler is a preemptive, priority-based algorithm with two
     separate priority ranges: a real-time range from 0 to 99 and a nice value
     ranging from -20 to 19. Smaller nice values indicate higher priorities. Thus,
     by increasing the nice value, you are decreasing your priority and being "nice"
     to the rest of the system.
         CFS is a significant departure from the traditional UNIX process scheduler.
     In the latter, the core variables in the scheduling algorithm are priority and
     time slice. The time slice is the length of time --the slice of the processor--
     that a process is afforded. Traditional UNIX systems give processes a fixed
     time slice, perhaps with a boost or penalty for high- or low-priority processes,
     respectively. A process may run for the length of its time slice, and higher-
     priority processes run before lower-priority processes. It is a simple algorithm
     that many non-UNIX systems employ. Such simplicity worked well for early
     time-sharing systems but has proved incapable of delivering good interactive
     performance and fairness on today's modern desktops and mobile devices.
         CFS introduced a new scheduling algorithm called fair scheduling that
     eliminates time slices in the traditional sense. Instead of time slices, all processes
     are allotted a proportion of the processor's time. CFS calculates how long a
     process should run as a function of the total number of runnable processes.



                                                    18.5         Scheduling            797
To start, CFS says that if there are N runnable processes, then each should
be afforded 1/N of the processor's time. CFS then adjusts this allotment by
weighting each process's allotment by its nice value. Processes with the default
nice value have a weight of 1--their priority is unchanged. Processes with a
smaller nice value (higher priority) receive a higher weight, while processes
with a larger nice value (lower priority) receive a lower weight. CFS then runs
each process for a "time slice" proportional to the process's weight divided by
the total weight of all runnable processes.
To calculate the actual length of time a process runs, CFS relies on a
configurable variable called target latency, which is the interval of time during
which every runnable task should run at least once. For example, assume
that the target latency is 10 milliseconds. Further assume that we have two
runnable processes of the same priority. Each of these processes has the same
weight and therefore receives the same proportion of the processor's time. In
this case, with a target latency of 10 milliseconds, the first process runs for
5 milliseconds, then the other process runs for 5 milliseconds, then the first
process runs for 5 milliseconds again, and so forth. If we have 10 runnable
processes, then CFS will run each for a millisecond before repeating.
But what if we had, say, 1, 000 processes? Each process would run for 1
microsecond if we followed the procedure just described. Due to switching
costs,  scheduling   processes  for  such    short  lengths  of  time  is  inefficient.
CFS consequently relies on a second configurable variable, the minimum
granularity, which is a minimum length of time any process is allotted the
processor. All processes, regardless of the target latency, will run for at least the
minimum granularity. In this manner, CFS ensures that switching costs do not
grow unacceptably large when the number of runnable processes grows too
large. In doing so, it violates its attempts at fairness. In the usual case, however,
the number of runnable processes remains reasonable, and both fairness and
switching costs are maximized.
With the switch to fair scheduling, CFS behaves differently from traditional
UNIX process schedulers in several ways. Most notably, as we have seen, CFS
eliminates the concept of a static time slice. Instead, each process receives
a proportion of the processor's time. How long that allotment is depends on
how many other processes are runnable. This approach solves several problems
in mapping priorities to time slices inherent in preemptive, priority-based
scheduling algorithms. It is possible, of course, to solve these problems in other
ways without abandoning the classic UNIX scheduler. CFS, however, solves the
problems with a simple algorithm that performs well on interactive workloads
such as mobile devices without compromising throughput performance on the
largest of servers.
18.5.2  Real-Time Scheduling
Linux's real-time scheduling algorithm is significantly simpler than the fair
scheduling employed for standard time-sharing processes. Linux implements
the two real-time scheduling classes required by POSIX.1b: first-come, first-
served (FCFS) and round-robin (Section 6.3.1 and Section 6.3.4, respectively). In
both cases, each process has a priority in addition to its scheduling class. The
scheduler always runs the process with the highest priority. Among processes
of equal priority, it runs the process that has been waiting longest. The only



798  Chapter 18   The Linux System
     difference between FCFS and round-robin scheduling is that FCFS processes
     continue to run until they either exit or block, whereas a round-robin process
     will be preempted after a while and will be moved to the end of the scheduling
     queue, so round-robin processes of equal priority will automatically time-share
     among themselves.
         Linux's real-time scheduling is soft--rather than hard--real time. The
     scheduler offers strict guarantees about the relative priorities of real-time
     processes, but the kernel does not offer any guarantees about how quickly
     a real-time process will be scheduled once that process becomes runnable. In
     contrast, a hard real-time system can guarantee a minimum latency between
     when a process becomes runnable and when it actually runs.
     18.5.3    Kernel Synchronization
     The way the kernel schedules its own operations is fundamentally different
     from the way it schedules processes. A request for kernel-mode execution
     can occur in two ways. A running program may request an operating-system
     service, either explicitly via a system call or implicitly--for example, when a
     page fault occurs. Alternatively, a device controller may deliver a hardware
     interrupt that causes the CPU to start executing a kernel-defined handler for
     that interrupt.
         The problem for the kernel is that all these tasks may try to access the same
     internal data structures. If one kernel task is in the middle of accessing some
     data structure when an interrupt service routine executes, then that service
     routine cannot access or modify the same data without risking data corruption.
     This fact relates to the idea of critical sections--portions of code that access
     shared data and thus must not be allowed to execute concurrently. As a result,
     kernel synchronization involves much more than just process scheduling. A
     framework is required that allows kernel tasks to run without violating the
     integrity of shared data.
         Prior to version 2.6, Linux was a nonpreemptive kernel, meaning that a
     process running in kernel mode could not be preempted --even if a higher-
     priority process became available to run. With version 2.6, the Linux kernel
     became fully preemptive. Now, a task can be preempted when it is running in
     the kernel.
         The Linux kernel provides spinlocks and semaphores (as well as reader­
     writer versions of these two locks) for locking in the kernel. On SMP machines,
     the fundamental locking mechanism is a spinlock, and the kernel is designed
     so  that  spinlocks  are   held  for  only   short  durations.  On      single-processor
     machines, spinlocks are not appropriate for use and are replaced by enabling
     and disabling kernel preemption. That is, rather than holding a spinlock, the
     task disables kernel preemption. When the task would otherwise release the
     spinlock, it enables kernel preemption. This pattern is summarized below:
                                single processor         multiple processors
                          Disable kernel preemption.     Acquire spin lock.
                          Enable kernel preemption.      Release spin lock.



                                                           18.5      Scheduling         799
      Linux uses an interesting approach to disable and enable kernel pre-
emption. It provides two simple kernel interfaces--preempt disable() and
preempt enable(). In addition, the kernel is not preemptible if a kernel-mode
task is holding a spinlock. To enforce this rule, each task in the system has
a thread-info structure that includes the field preempt count, which is a
counter indicating the number of locks being held by the task. The counter is
incremented when a lock is acquired and decremented when a lock is released.
If the value of preempt count for the task currently running is greater than
zero, it is not safe to preempt the kernel, as this task currently holds a lock. If
the count is zero, the kernel can safely be interrupted, assuming there are no
outstanding calls to preempt disable().
      Spinlocks--along with the enabling and disabling of kernel preemption--
are used in the kernel only when the lock is held for short durations. When a
lock must be held for longer periods, semaphores are used.
      The second protection technique used by Linux applies to critical sections
that  occur  in  interrupt    service   routines.  The  basic  tool  is  the  processor 's
interrupt-control hardware. By disabling interrupts (or using spinlocks) during
a critical section, the kernel guarantees that it can proceed without the risk of
concurrent access to shared data structures.
      However, there is a penalty for disabling interrupts. On most hardware
architectures, interrupt enable and disable instructions are not cheap. More
importantly, as long as interrupts remain disabled, all I/O is suspended, and
any device waiting for servicing will have to wait until interrupts are reenabled;
thus, performance degrades. To address this problem, the Linux kernel uses a
synchronization architecture that allows long critical sections to run for their
entire duration without having interrupts disabled. This ability is especially
useful in the networking code. An interrupt in a network device driver can
signal the arrival of an entire network packet, which may result in a great deal
of code being executed to disassemble, route, and forward that packet within
the interrupt service routine.
      Linux implements this architecture by separating interrupt service routines
into  two  sections:   the  top  half   and  the   bottom  half.  The    top  half  is  the
standard interrupt service routine that runs with recursive interrupts disabled.
Interrupts of the same number (or line) are disabled, but other interrupts may
run. The bottom half of a service routine is run, with all interrupts enabled,
by a miniature scheduler that ensures that bottom halves never interrupt
themselves. The bottom-half scheduler is invoked automatically whenever
an interrupt service routine exits.
      This separation means that the kernel can complete any complex processing
that has to be done in response to an interrupt without worrying about being
interrupted itself. If another interrupt occurs while a bottom half is executing,
then that interrupt can request that the same bottom half execute, but the
execution will be deferred until the one currently running completes. Each
execution of the bottom half can be interrupted by a top half but can never be
interrupted by a similar bottom half.
      The top-half/bottom-half architecture is completed by a mechanism for
disabling selected bottom halves while executing normal, foreground kernel
code. The kernel can code critical sections easily using this system. Interrupt
handlers   can   code  their  critical  sections   as  bottom  halves;   and  when      the
foreground kernel wants to enter a critical section, it can disable any relevant



800   Chapter  18  The Linux System
               top-half interrupt handlers
               bottom-half interrupt handlers                              increasing priority
               kernel-system service routines (preemptible)
               user-mode programs (preemptible)
                   Figure 18.2               Interrupt protection levels.
      bottom halves to prevent any other critical sections from interrupting it. At
      the end of the critical section, the kernel can reenable the bottom halves and
      run any bottom-half tasks that have been queued by top-half interrupt service
      routines during the critical section.
      Figure 18.2 summarizes the various levels of interrupt protection within
      the kernel. Each level may be interrupted by code running at a higher level
      but will never be interrupted by code running at the same or a lower level.
      Except for user-mode code, user processes can always be preempted by another
      process when a time-sharing scheduling interrupt occurs.
      18.5.4   Symmetric Multiprocessing
      The Linux 2.0 kernel was the first stable Linux kernel to support symmetric
      multiprocessor (SMP) hardware, allowing separate processes to execute in
      parallel on separate processors. The original implementation of SMP imposed
      the restriction that only one processor at a time could be executing kernel code.
      In version 2.2 of the kernel, a single kernel spinlock (sometimes termed
      BKL for "big kernel lock") was created to allow multiple processes (running on
      different processors) to be active in the kernel concurrently. However, the BKL
      provided a very coarse level of locking granularity, resulting in poor scalability
      to machines with many processors and processes. Later releases of the kernel
      made the SMP implementation more scalable by splitting this single kernel
      spinlock into multiple locks, each of which protects only a small subset of the
      kernel's data structures. Such spinlocks are described in Section 18.5.3. The 3.0
      kernel provides additional SMP enhancements, including ever-finer locking,
      processor affinity, and load-balancing algorithms.
18.6  Memory Management
      Memory management under Linux has two components. The first deals with
      allocating and freeing physical memory--pages, groups of pages, and small
      blocks of RAM. The second handles virtual memory, which is memory-mapped
      into the address space of running processes. In this section, we describe these
      two components and then examine the mechanisms by which the loadable
      components of a new program are brought into a process's virtual memory in
      response to an exec() system call.



                                         18.6     Memory Management                  801
18.6.1    Management of Physical Memory
Due to specific hardware constraints, Linux separates physical memory into
four different zones, or regions:
·  ZONE DMA
·  ZONE DMA32
·  ZONE NORMAL
·  ZONE HIGHMEM
These zones are architecture specific. For example, on the Intel x86-32 architec-
ture, certain ISA (industry standard architecture) devices can only access the
lower 16 MB of physical memory using DMA. On these systems, the first 16
MB of physical memory comprise ZONE DMA. On other systems, certain devices
can only access the first 4 GB of physical memory, despite supporting 64-
bit addresses. On such systems, the first 4 GB of physical memory comprise
ZONE DMA32. ZONE HIGHMEM (for "high memory") refers to physical memory
that is not mapped into the kernel address space. For example, on the 32-bit Intel
architecture (where 232 provides a 4-GB address space), the kernel is mapped
into the first 896 MB of the address space; the remaining memory is referred
to as high memory and is allocated from ZONE HIGHMEM. Finally, ZONE NORMAL
comprises everything else --the normal, regularly mapped pages. Whether
an architecture has a given zone depends on its constraints. A modern, 64-bit
architecture such as Intel x86-64 has a small 16 MB ZONE DMA (for legacy devices)
and all the rest of its memory in ZONE NORMAL, with no "high memory".
   The    relationship  of  zones  and  physical  addresses     on  the  Intel       x86-32
architecture is shown in Figure 18.3. The kernel maintains a list of free pages
for each zone. When a request for physical memory arrives, the kernel satisfies
the request using the appropriate zone.
   The primary physical-memory manager in the Linux kernel is the page
allocator. Each zone has its own allocator, which is responsible for allocating
and freeing all physical pages for the zone and is capable of allocating ranges
of physically contiguous pages on request. The allocator uses a buddy system
(Section  9.8.1)  to  keep  track  of  available  physical  pages.  In   this        scheme,
adjacent units of allocatable memory are paired together (hence its name). Each
allocatable memory region has an adjacent partner (or buddy). Whenever two
allocated partner regions are freed up, they are combined to form a larger
region--a buddy heap. That larger region also has a partner, with which it can
combine to form a still larger free region. Conversely, if a small memory request
                            zone                  physical memory
                        ZONE_DMA                  < 16 MB
                      ZONE_NORMAL                 16 .. 896 MB
                      ZONE_HIGHMEM                > 896     MB
   Figure 18.3        Relationship of zones and physical addresses in Intel x86-32.



802  Chapter 18  The Linux System
     cannot be satisfied by allocation of an existing small free region, then a larger
     free region will be subdivided into two partners to satisfy the request. Separate
     linked lists are used to record the free memory regions of each allowable size.
     Under Linux, the smallest size allocatable under this mechanism is a single
     physical page. Figure 18.4 shows an example of buddy-heap allocation. A 4-KB
     region is being allocated, but the smallest available region is 16 KB. The region
     is broken up recursively until a piece of the desired size is available.
         Ultimately, all memory allocations in the Linux kernel are made either
     statically, by drivers that reserve a contiguous area of memory during system
     boot time, or dynamically, by the page allocator. However, kernel functions
     do not have to use the basic allocator to reserve memory. Several specialized
     memory-management subsystems use the underlying page allocator to man-
     age their own pools of memory. The most important are the virtual memory
     system, described in Section 18.6.2; the kmalloc() variable-length allocator;
     the slab allocator, used for allocating memory for kernel data structures; and
     the page cache, used for caching pages belonging to files.
         Many components of the Linux operating system need to allocate entire
     pages on request, but often smaller blocks of memory are required. The kernel
     provides an additional allocator for arbitrary-sized requests, where the size of
     a request is not known in advance and may be only a few bytes. Analogous
     to the C language's malloc() function, this kmalloc() service allocates entire
     physical pages on demand but then splits them into smaller pieces. The kernel
     maintains lists of pages in use by the kmalloc() service. Allocating memory
     involves determining the appropriate list and either taking the first free piece
     available on the list or allocating a new page and splitting it up. Memory regions
     claimed by the kmalloc() system are allocated permanently until they are
     freed explicitly with a corresponding call to kfree(); the kmalloc() system
     cannot reallocate or reclaim these regions in response to memory shortages.
         Another strategy adopted by Linux for allocating kernel memory is known
     as  slab  allocation.  A  slab  is  used       for  allocating  memory      for  kernel  data
     structures and is made up of one or more physically contiguous pages. A
     cache consists of one or more slabs. There is a single cache for each unique
     kernel data structure--for example, a cache for the data structure representing
     process descriptors, a cache for file objects, a cache for inodes, and so forth.
                                                    8KB                     8KB
                 16KB
                                                                            4KB
                                                    8KB
                                                                            4KB
                 Figure 18.4         Splitting  of  memory  in  the  buddy  system.



                                                 18.6    Memory         Management     803
                  kernel objects         caches             slabs
             3-KB
            objects
                                                                        physically
                                                                        contiguous
                                                                        pages
             7-KB
            objects
                           Figure 18.5   Slab allocator in Linux.
Each cache is populated with objects that are instantiations of the kernel
data   structure  the  cache      represents.    For  example,     the  cache  representing
inodes stores instances of inode structures, and the cache representing process
descriptors stores instances of process descriptor structures. The relationship
among slabs, caches, and objects is shown in Figure 18.5. The figure shows two
kernel objects 3 KB in size and three objects 7 KB in size. These objects are stored
in the respective caches for 3-KB and 7-KB objects.
       The slab-allocation algorithm uses caches to store kernel objects. When a
cache is created, a number of objects are allocated to the cache. The number of
objects in the cache depends on the size of the associated slab. For example,
a 12-KB slab (made up of three contiguous 4-KB pages) could store six 2-KB
objects. Initially, all the objects in the cache are marked as free. When a new
object for a kernel data structure is needed, the allocator can assign any free
object from the cache to satisfy the request. The object assigned from the cache
is marked as used.
       Let's consider a scenario in which the kernel requests memory from the
slab allocator for an object representing a process descriptor. In Linux systems,
a  process   descriptor    is  of   the  type    struct  task   struct,  which      requires
approximately 1.7 KB of memory. When the Linux kernel creates a new task,
it requests the necessary memory for the struct             task   struct object from its
cache.  The  cache   will  fulfill  the  request using   a  struct      task   struct  object
that has already been allocated in a slab and is marked as free.
       In Linux, a slab may be in one of three possible states:
   1.  Full. All objects in the slab are marked as used.
   2.  Empty. All objects in the slab are marked as free.
   3.  Partial. The slab consists of both used and free objects.
The slab allocator first attempts to satisfy the request with a free object in a
partial slab. If none exist, a free object is assigned from an empty slab. If no
empty slabs are available, a new slab is allocated from contiguous physical



804  Chapter 18  The Linux System
     pages and assigned to a cache; memory for the object is allocated from this
     slab.
     Two other main subsystems in Linux do their own management of physical
     pages: the page cache and the virtual memory system. These systems are closely
     related to each other. The page cache is the kernel's main cache for files and
     is the main mechanism through which I/O to block devices (Section 18.8.1)
     is performed. File systems of all types, including the native Linux disk-based
     file systems and the NFS networked file system, perform their I/O through
     the page cache. The page cache stores entire pages of file contents and is not
     limited to block devices. It can also cache networked data. The virtual memory
     system manages the contents of each process's virtual address space. These
     two systems interact closely with each other because reading a page of data
     into the page cache requires mapping pages in the page cache using the virtual
     memory system. In the following section, we look at the virtual memory system
     in greater detail.
     18.6.2  Virtual Memory
     The Linux virtual memory system is responsible for maintaining the address
     space accessible to each process. It creates pages of virtual memory on demand
     and manages loading those pages from disk and swapping them back out to
     disk as required. Under Linux, the virtual memory manager maintains two
     separate views of a process's address space: as a set of separate regions and as
     a set of pages.
     The first view of an address space is the logical view, describing instructions
     that the virtual memory system has received concerning the layout of the
     address space. In this view, the address space consists of a set of nonoverlapping
     regions, each region representing a continuous, page-aligned subset of the
     address space. Each region is described internally by a single vm area struct
     structure that defines the properties of the region, including the process's read,
     write, and execute permissions in the region as well as information about any
     files associated with the region. The regions for each address space are linked
     into a balanced binary tree to allow fast lookup of the region corresponding to
     any virtual address.
     The kernel also maintains a second, physical view of each address space.
     This view is stored in the hardware page tables for the process. The page-
     table entries identify the exact current location of each page of virtual memory,
     whether it is on disk or in physical memory. The physical view is managed by a
     set of routines, which are invoked from the kernel's software-interrupt handlers
     whenever a process tries to access a page that is not currently present in the
     page tables. Each vm area struct in the address-space description contains a
     field pointing to a table of functions that implement the key page-management
     functionality for any given virtual memory region. All requests to read or write
     an unavailable page are eventually dispatched to the appropriate handler
     in the function table for the vm area struct, so that the central memory-
     management routines do not have to know the details of managing each
     possible type of memory region.



                                          18.6  Memory Management                  805
18.6.2.1  Virtual Memory Regions
Linux implements several types of virtual memory regions. One property
that characterizes virtual memory is the backing store for the region, which
describes where the pages for the region come from. Most memory regions
are backed either by a file or by nothing. A region backed by nothing is the
simplest type of virtual memory region. Such a region represents demand-zero
memory: when a process tries to read a page in such a region, it is simply given
back a page of memory filled with zeros.
A region backed by a file acts as a viewport onto a section of that file.
Whenever the process tries to access a page within that region, the page table
is filled with the address of a page within the kernel's page cache corresponding
to the appropriate offset in the file. The same page of physical memory is used
by both the page cache and the process's page tables, so any changes made to
the file by the file system are immediately visible to any processes that have
mapped that file into their address space. Any number of processes can map
the same region of the same file, and they will all end up using the same page
of physical memory for the purpose.
A virtual memory region is also defined by its reaction to writes. The
mapping of a region into the process's address space can be either private or
shared. If a process writes to a privately mapped region, then the pager detects
that a copy-on-write is necessary to keep the changes local to the process. In
contrast, writes to a shared region result in updating of the object mapped into
that region, so that the change will be visible immediately to any other process
that is mapping that object.
18.6.2.2  Lifetime of a Virtual Address Space
The kernel creates a new virtual address space in two situations: when a process
runs a new program with the exec() system call and when a new process is
created by the fork() system call. The first case is easy. When a new program is
executed, the process is given a new, completely empty virtual address space.
It is up to the routines for loading the program to populate the address space
with virtual memory regions.
The second case, creating a new process with fork(), involves creating
a complete copy of the existing process's virtual address space. The kernel
copies the parent process's vm area struct descriptors, then creates a new set
of page tables for the child. The parent's page tables are copied directly into
the child's, and the reference count of each page covered is incremented. Thus,
after the fork, the parent and child share the same physical pages of memory
in their address spaces.
A special case occurs when the copying operation reaches a virtual memory
region that is mapped privately. Any pages to which the parent process has
written within such a region are private, and subsequent changes to these pages
by either the parent or the child must not update the page in the other process's
address space. When the page-table entries for such regions are copied, they
are set to be read only and are marked for copy-on-write. As long as neither
process modifies these pages, the two processes share the same page of physical
memory. However, if either process tries to modify a copy-on-write page, the
reference count on the page is checked. If the page is still shared, then the



806  Chapter 18  The Linux System
     process copies the page's contents to a brand-new page of physical memory
     and uses its copy instead. This mechanism ensures that private data pages are
     shared between processes whenever possible and copies are made only when
     absolutely necessary.
     18.6.2.3  Swapping and Paging
     An important task for a virtual memory system is to relocate pages of memory
     from physical memory out to disk when that memory is needed. Early UNIX
     systems performed this relocation by swapping out the contents of entire
     processes at once, but modern versions of UNIX rely more on paging--the
     movement of individual pages of virtual memory between physical memory
     and disk. Linux does not implement whole-process swapping; it uses the newer
     paging mechanism exclusively.
     The paging system can be divided into two sections. First, the policy
     algorithm decides which pages to write out to disk and when to write them.
     Second, the paging mechanism carries out the transfer and pages data back
     into physical memory when they are needed again.
     Linux's pageout policy uses a modified version of the standard clock (or
     second-chance) algorithm described in Section 9.4.5.2. Under Linux, a multiple-
     pass clock is used, and every page has an age that is adjusted on each pass of
     the clock. The age is more precisely a measure of the page's youthfulness, or
     how much activity the page has seen recently. Frequently accessed pages will
     attain a higher age value, but the age of infrequently accessed pages will drop
     toward zero with each pass. This age valuing allows the pager to select pages
     to page out based on a least frequently used (LFU) policy.
     The paging mechanism supports paging both to dedicated swap devices
     and partitions and to normal files, although swapping to a file is significantly
     slower due to the extra overhead incurred by the file system. Blocks are
     allocated from the swap devices according to a bitmap of used blocks, which
     is maintained in physical memory at all times. The allocator uses a next-fit
     algorithm to try to write out pages to continuous runs of disk blocks for
     improved performance. The allocator records the fact that a page has been
     paged out to disk by using a feature of the page tables on modern processors:
     the page-table entry's page-not-present bit is set, allowing the rest of the page-
     table entry to be filled with an index identifying where the page has been
     written.
     18.6.2.4  Kernel Virtual Memory
     Linux reserves for its own internal use a constant, architecture-dependent
     region of the virtual address space of every process. The page-table entries
     that map to these kernel pages are marked as protected, so that the pages are
     not visible or modifiable when the processor is running in user mode. This
     kernel virtual memory area contains two regions. The first is a static area that
     contains page-table references to every available physical page of memory
     in the system, so that a simple translation from physical to virtual addresses
     occurs when kernel code is run. The core of the kernel, along with all pages
     allocated by the normal page allocator, resides in this region.



                                               18.6  Memory Management               807
     The remainder of the kernel's reserved section of address space is not
reserved for any specific purpose. Page-table entries in this address range
can be modified by the kernel to point to any other areas of memory. The
kernel provides a pair of facilities that allow kernel code to use this virtual
memory. The vmalloc() function allocates an arbitrary number of physical
pages of memory that may not be physically contiguous into a single region of
virtually contiguous kernel memory. The vremap() function maps a sequence
of virtual addresses to point to an area of memory used by a device driver for
memory-mapped I/O.
18.6.3    Execution and Loading of User Programs
The  Linux   kernel's  execution    of  user  programs  is  triggered  by  a   call  to
the  exec()  system    call.  This  exec()    call  commands  the  kernel  to  run   a
new program within the current process, completely overwriting the current
execution context with the initial context of the new program. The first job of
this system service is to verify that the calling process has permission rights to
the file being executed. Once that matter has been checked, the kernel invokes
a loader routine to start running the program. The loader does not necessarily
load the contents of the program file into physical memory, but it does at least
set up the mapping of the program into virtual memory.
     There is no single routine in Linux for loading a new program. Instead,
Linux maintains a table of possible loader functions, and it gives each such
function the opportunity to try loading the given file when an exec() system
call is made. The initial reason for this loader table was that, between the
releases of the 1.0 and 1.2 kernels, the standard format for Linux's binary files
was changed. Older Linux kernels understood the a.out format for binary
files--a relatively simple format common on older UNIX systems. Newer
Linux systems use the more modern ELF format, now supported by most
current UNIX implementations. ELF has a number of advantages over a.out,
including flexibility and extendability. New sections can be added to an ELF
binary (for example, to add extra debugging information) without causing
the loader routines to become confused. By allowing registration of multiple
loader routines, Linux can easily support the ELF and a.out binary formats in
a single running system.
     In Section 18.6.3.1 and Section 18.6.3.2, we concentrate exclusively on the
loading and running of ELF-format binaries. The procedure for loading a.out
binaries is simpler but similar in operation.
18.6.3.1  Mapping of Programs into Memory
Under Linux, the binary loader does not load a binary file into physical memory.
Rather, the pages of the binary file are mapped into regions of virtual memory.
Only when the program tries to access a given page will a page fault result in
the loading of that page into physical memory using demand paging.
     It is the responsibility of the kernel's binary loader to set up the initial
memory mapping. An ELF-format binary file consists of a header followed by
several page-aligned sections. The ELF loader works by reading the header and
mapping the sections of the file into separate regions of virtual memory.
     Figure 18.6 shows the typical layout of memory regions set up by the ELF
loader. In a reserved region at one end of the address space sits the kernel, in



808  Chapter 18  The Linux System
                 kernel virtual memory              memory invisible to user-mode code
                          stack
                 memory-mapped region
                 memory-mapped region
                 memory-mapped region
                                                    the `brk' pointer
                      run-time data
                      uninitialized data
                      initialized data
                      program text
                                                    forbidden region
                          Figure 18.6     Memory layout for ELF programs.
     its own privileged region of virtual memory inaccessible to normal user-mode
     programs. The rest of virtual memory is available to applications, which can use
     the kernel's memory-mapping functions to create regions that map a portion
     of a file or that are available for application data.
     The loader's job is to set up the initial memory mapping to allow the
     execution of the program to start. The regions that need to be initialized include
     the stack and the program's text and data regions.
     The       stack  is  created   at    the  top  of  the  user-mode     virtual  memory;  it
     grows downward toward lower-numbered addresses. It includes copies of the
     arguments and environment variables given to the program in the exec()
     system call. The other regions are created near the bottom end of virtual
     memory. The sections of the binary file that contain program text or read-only
     data are mapped into memory as a write-protected region. Writable initialized
     data are mapped next; then any uninitialized data are mapped in as a private
     demand-zero region.
     Directly beyond these fixed-sized regions is a variable-sized region that
     programs can expand as needed to hold data allocated at run time. Each
     process has a pointer, brk, that points to the current extent of this data region,
     and processes can extend or contract their brk region with a single system call
     -- sbrk().
     Once these mappings have been set up, the loader initializes the process's
     program-counter register with the starting point recorded in the ELF header,
     and the process can be scheduled.
     18.6.3.2  Static and Dynamic Linking
     Once the program has been loaded and has started running, all the necessary
     contents of the binary file have been loaded into the process's virtual address



                                                           18.7   File Systems             809
      space. However, most programs also need to run functions from the system
      libraries, and these library functions must also be loaded. In the simplest
      case, the necessary library functions are embedded directly in the program's
      executable binary file. Such a program is statically linked to its libraries, and
      statically linked executables can commence running as soon as they are loaded.
           The main disadvantage of static linking is that every program generated
      must contain copies of exactly the same common system library functions. It is
      much more efficient, in terms of both physical memory and disk-space usage,
      to load the system libraries into memory only once. Dynamic linking allows
      that to happen.
           Linux implements dynamic linking in user mode through a special linker
      library. Every dynamically linked program contains a small, statically linked
      function that is called when the program starts. This static function just maps
      the link library into memory and runs the code that the function contains. The
      link library determines the dynamic libraries required by the program and the
      names of the variables and functions needed from those libraries by reading the
      information contained in sections of the ELF binary. It then maps the libraries
      into the middle of virtual memory and resolves the references to the symbols
      contained in those libraries. It does not matter exactly where in memory these
      shared libraries are mapped: they are compiled into position-independent
      code (PIC), which can run at any address in memory.
18.7  File Systems
      Linux retains UNIX's standard file-system model. In UNIX, a file does not have
      to be an object stored on disk or fetched over a network from a remote file
      server. Rather, UNIX files can be anything capable of handling the input or
      output of a stream of data. Device drivers can appear as files, and interprocess-
      communication channels or network connections also look like files to the
      user.
           The Linux kernel handles all these types of files by hiding the implemen-
      tation details of any single file type behind a layer of software, the virtual file
      system (VFS). Here, we first cover the virtual file system and then discuss the
      standard Linux file system--ext3.
      18.7.1  The Virtual File System
      The    Linux  VFS  is  designed  around  object-oriented    principles.  It  has     two
      components: a set of definitions that specify what file-system objects are
      allowed to look like and a layer of software to manipulate the objects. The
      VFS defines four main object types:
      ·    An inode object represents an individual file.
      ·    A file object represents an open file.
      ·    A superblock object represents an entire file system.
      ·    A dentry object represents an individual directory entry.



810  Chapter 18     The Linux System
          For each of these four object types, the VFS defines a set of operations.
     Every object of one of these types contains a pointer to a function table. The
     function table lists the addresses of the actual functions that implement the
     defined operations for that object. For example, an abbreviated API for some of
     the file object's operations includes:
     ·    int  open(.  .   .)  --     Open a file.
     ·    ssize  t  read(.    .     .)  --  Read from a file.
     ·    ssize  t  write(.      .  .)      --  Write to a file.
     ·    int  mmap(.  .   .)  --     Memory-map a file.
     The  complete  definition          of  the  file     object  is  specified  in     the   struct
     file operations, which is located in the file /usr/include/linux/fs.h.
     An implementation of the file object (for a specific file type) is required to
     implement each function specified in the definition of the file object.
          The VFS software layer can perform an operation on one of the file-system
     objects by calling the appropriate function from the object's function table,
     without having to know in advance exactly what kind of object it is dealing
     with. The VFS does not know, or care, whether an inode represents a networked
     file, a disk file, a network socket, or a directory file. The appropriate function
     for that file's read() operation will always be at the same place in its function
     table, and the VFS software layer will call that function without caring how the
     data are actually read.
          The inode and file objects are the mechanisms used to access files. An inode
     object is a data structure containing pointers to the disk blocks that contain the
     actual file contents, and a file object represents a point of access to the data in an
     open file. A process cannot access an inode's contents without first obtaining a
     file object pointing to the inode. The file object keeps track of where in the file
     the process is currently reading or writing, to keep track of sequential file I/O.
     It also remembers the permissions (for example, read or write) requested when
     the file was opened and tracks the process's activity if necessary to perform
     adaptive read-ahead, fetching file data into memory before the process requests
     the data, to improve performance.
          File objects typically belong to a single process, but inode objects do not.
     There is one file object for every instance of an open file, but always only a
     single inode object. Even when a file is no longer in use by any process, its
     inode object may still be cached by the VFS to improve performance if the file
     is used again in the near future. All cached file data are linked onto a list in the
     file's inode object. The inode also maintains standard information about each
     file, such as the owner, size, and time most recently modified.
          Directory files are dealt with slightly differently from other files. The UNIX
     programming interface defines a number of operations on directories, such as
     creating, deleting, and renaming a file in a directory. The system calls for these
     directory operations do not require that the user open the files concerned,
     unlike the case for reading or writing data. The VFS therefore defines these
     directory operations in the inode object, rather than in the file object.
          The  superblock     object    represents     a  connected   set  of    files  that  form  a
     self-contained file system. The operating-system kernel maintains a single



                                                              18.7    File Systems     811
superblock object for each disk device mounted as a file system and for each
networked file system currently connected. The main responsibility of the
superblock object is to provide access to inodes. The VFS identifies every
inode by a unique file-system/inode number pair, and it finds the inode
corresponding to a particular inode number by asking the superblock object to
return the inode with that number.
Finally, a dentry object represents a directory entry, which may include the
name of a directory in the path name of a file (such as /usr) or the actual file
(such as stdio.h). For example, the file /usr/include/stdio.h contains the
directory entries (1) /, (2) usr, (3) include, and (4) stdio.h. Each of these
values is represented by a separate dentry object.
As        an  example     of   how    dentry   objects   are   used,  consider  the    situ-
ation  in  which     a    process     wishes   to  open  the   file  with  the  pathname
/usr/include/stdio.h using an editor. Because Linux treats directory names
as files, translating this path requires first obtaining the inode for the root--
/. The operating system must then read through this file to obtain the inode
for the file include. It must continue this process until it obtains the inode for
the file stdio.h. Because path-name translation can be a time-consuming task,
Linux maintains a cache of dentry objects, which is consulted during path-name
translation. Obtaining the inode from the dentry cache is considerably faster
than having to read the on-disk file.
18.7.2     The Linux ext3 File System
The standard on-disk file system used by Linux is called ext3, for historical
reasons.   Linux     was  originally  programmed         with  a     Minix-compatible  file
system, to ease exchanging data with the Minix development system, but
that file system was severely restricted by 14-character file-name limits and a
maximum file-system size of 64 MB. The Minix file system was superseded by
a new file system, which was christened the extended file system (extfs). A
later redesign to improve performance and scalability and to add a few missing
features led to the second extended file system (ext2). Further development
added journaling capabilities, and the system was renamed the third extended
file system (ext3). Linux kernel developers are working on augmenting ext3
with modern file-system features such as extents. This new file system is called
the fourth extended file system (ext4). The rest of this section discusses ext3,
however, since it remains the most-deployed Linux file system. Most of the
discussion applies equally to ext4.
Linux's ext3 has much in common with the BSD Fast File System (FFS)
(Section   A.7.7).   It  uses  a  similar  mechanism     for   locating    the  data  blocks
belonging     to  a  specific  file,  storing  data-block  pointers   in   indirect   blocks
throughout the file system with up to three levels of indirection. As in FFS,
directory files are stored on disk just like normal files, although their contents
are interpreted differently. Each block in a directory file consists of a linked list
of entries. In turn, each entry contains the length of the entry, the name of a
file, and the inode number of the inode to which that entry refers.
The main differences between ext3 and FFS lie in their disk-allocation
policies. In FFS, the disk is allocated to files in blocks of 8 KB. These blocks
are subdivided into fragments of 1 KB for storage of small files or partially
filled blocks at the ends of files. In contrast, ext3 does not use fragments at all



812  Chapter 18  The Linux System
     but performs all its allocations in smaller units. The default block size on ext3
     varies as a function of the total size of the file system. Supported block sizes
     are 1, 2, 4, and 8 KB.
     To maintain high performance, the operating system must try to perform
     I/O operations in large chunks whenever possible by clustering physically
     adjacent I/O requests. Clustering reduces the per-request overhead incurred by
     device drivers, disks, and disk-controller hardware. A block-sized I/O request
     size is too small to maintain good performance, so ext3 uses allocation policies
     designed to place logically adjacent blocks of a file into physically adjacent
     blocks on disk, so that it can submit an I/O request for several disk blocks as a
     single operation.
     The ext3 allocation policy works as follows: As in FFS, an ext3 file system is
     partitioned into multiple segments. In ext3, these are called block groups. FFS
     uses the similar concept of cylinder groups, where each group corresponds to
     a single cylinder of a physical disk. (Note that modern disk-drive technology
     packs sectors onto the disk at different densities, and thus with different
     cylinder sizes, depending on how far the disk head is from the center of the
     disk. Therefore, fixed-sized cylinder groups do not necessarily correspond to
     the disk's geometry.)
     When allocating a file, ext3 must first select the block group for that file.
     For data blocks, it attempts to allocate the file to the block group to which the
     file's inode has been allocated. For inode allocations, it selects the block group
     in which the file's parent directory resides for nondirectory files. Directory
     files are not kept together but rather are dispersed throughout the available
     block groups. These policies are designed not only to keep related information
     within the same block group but also to spread out the disk load among the
     disk's block groups to reduce the fragmentation of any one area of the disk.
     Within a block group, ext3 tries to keep allocations physically contiguous
     if possible, reducing fragmentation if it can. It maintains a bitmap of all free
     blocks in a block group. When allocating the first blocks for a new file, it
     starts searching for a free block from the beginning of the block group. When
     extending a file, it continues the search from the block most recently allocated
     to the file. The search is performed in two stages. First, ext3 searches for an
     entire free byte in the bitmap; if it fails to find one, it looks for any free bit.
     The search for free bytes aims to allocate disk space in chunks of at least eight
     blocks where possible.
     Once a free block has been identified, the search is extended backward until
     an allocated block is encountered. When a free byte is found in the bitmap,
     this backward extension prevents ext3 from leaving a hole between the most
     recently allocated block in the previous nonzero byte and the zero byte found.
     Once the next block to be allocated has been found by either bit or byte search,
     ext3 extends the allocation forward for up to eight blocks and preallocates
     these extra blocks to the file. This preallocation helps to reduce fragmentation
     during interleaved writes to separate files and also reduces the CPU cost of
     disk allocation by allocating multiple blocks simultaneously. The preallocated
     blocks are returned to the free-space bitmap when the file is closed.
     Figure      18.7   illustrates  the  allocation  policies.  Each  row  represents    a
     sequence of set and unset bits in an allocation bitmap, indicating used and
     free blocks on disk. In the first case, if we can find any free blocks sufficiently
     near the start of the search, then we allocate them no matter how fragmented



                                                           18.7             File Systems  813
        allocating scattered free blocks
        allocating continuous free blocks
        block in use                       block selected  bit boundary
                                           by allocator
        free block                         bitmap search   byte boundary
                    Figure 18.7            ext3 block-allocation policies.
they may be. The fragmentation is partially compensated for by the fact that
the blocks are close together and can probably all be read without any disk
seeks. Furthermore, allocating them all to one file is better in the long run than
allocating isolated blocks to separate files once large free areas become scarce
on disk. In the second case, we have not immediately found a free block close
by, so we search forward for an entire free byte in the bitmap. If we allocated
that byte as a whole, we would end up creating a fragmented area of free space
between it and the allocation preceding it. Thus, before allocating, we back
up to make this allocation flush with the allocation preceding it, and then we
allocate forward to satisfy the default allocation of eight blocks.
18.7.3  Journaling
The ext3 file system supports a popular feature called journaling, whereby
modifications to the file system are written sequentially to a journal. A set of
operations that performs a specific task is a transaction. Once a transaction
is written to the journal, it is considered to be committed. Meanwhile, the
journal entries relating to the transaction are replayed across the actual file-
system structures. As the changes are made, a pointer is updated to indicate
which actions have completed and which are still incomplete. When an entire
committed transaction is completed, it is removed from the journal. The journal,
which is actually a circular buffer, may be in a separate section of the file
system, or it may even be on a separate disk spindle. It is more efficient, but
more complex, to have it under separate read ­write heads, thereby decreasing
head contention and seek times.
If the system crashes, some transactions may remain in the journal. Those
transactions were never completed to the file system even though they were
committed by the operating system, so they must be completed once the system



814  Chapter 18  The Linux System
     recovers. The transactions can be executed from the pointer until the work is
     complete, and the file-system structures remain consistent. The only problem
     occurs when a transaction has been aborted--that is, it was not committed
     before the system crashed. Any changes from those transactions that were
     applied to the file system must be undone, again preserving the consistency of
     the file system. This recovery is all that is needed after a crash, eliminating all
     problems with consistency checking.
     Journaling file systems may perform some operations faster than non-
     journaling systems, as updates proceed much faster when they are applied
     to the in-memory journal rather than directly to the on-disk data structures.
     The reason for this improvement is found in the performance advantage of
     sequential I/O over random I/O. Costly synchronous random writes to the file
     system are turned into much less costly synchronous sequential writes to the
     file system's journal. Those changes, in turn, are replayed asynchronously via
     random writes to the appropriate structures. The overall result is a significant
     gain in performance of file-system metadata-oriented operations, such as file
     creation and deletion. Due to this performance improvement, ext3 can be
     configured to journal only metadata and not file data.
     18.7.4  The Linux Process File System
     The flexibility of the Linux VFS enables us to implement a file system that does
     not store data persistently at all but rather provides an interface to some other
     functionality. The Linux process file system, known as the /proc file system,
     is an example of a file system whose contents are not actually stored anywhere
     but are computed on demand according to user file I/O requests.
     A /proc file system is not unique to Linux. SVR4 UNIX introduced a /proc
     file system as an efficient interface to the kernel's process debugging support.
     Each subdirectory of the file system corresponded not to a directory on any
     disk but rather to an active process on the current system. A listing of the file
     system reveals one directory per process, with the directory name being the
     ASCII decimal representation of the process's unique process identifier (PID).
     Linux implements such a /proc file system but extends it greatly by
     adding a number of extra directories and text files under the file system's root
     directory. These new entries correspond to various statistics about the kernel
     and the associated loaded drivers. The /proc file system provides a way for
     programs to access this information as plain text files; the standard UNIX user
     environment provides powerful tools to process such files. For example, in
     the past, the traditional UNIX ps command for listing the states of all running
     processes has been implemented as a privileged process that reads the process
     state directly from the kernel's virtual memory. Under Linux, this command
     is implemented as an entirely unprivileged program that simply parses and
     formats the information from /proc.
     The /proc file system must implement two things: a directory structure
     and the file contents within. Because a UNIX file system is defined as a set of file
     and directory inodes identified by their inode numbers, the /proc file system
     must define a unique and persistent inode number for each directory and the
     associated files. Once such a mapping exists, the file system can use this inode
     number to identify just what operation is required when a user tries to read
     from a particular file inode or to perform a lookup in a particular directory



                                                  18.8  Input and Output                   815
      inode. When data are read from one of these files, the /proc file system will
      collect the appropriate information, format it into textual form, and place it
      into the requesting process's read buffer.
      The mapping from inode number to information type splits the inode
      number into two fields. In Linux, a PID is 16 bits in size, but an inode number
      is 32 bits. The top 16 bits of the inode number are interpreted as a PID, and the
      remaining bits define what type of information is being requested about that
      process.
      A PID of zero is not valid, so a zero PID field in the inode number is
      taken to mean that this inode contains global--rather than process-specific--
      information. Separate global files exist in /proc to report information such as
      the kernel version, free memory, performance statistics, and drivers currently
      running.
      Not all the inode numbers in this range are reserved. The kernel can allocate
      new /proc inode mappings dynamically, maintaining a bitmap of allocated
      inode numbers. It also maintains a tree data structure of registered global /proc
      file-system entries. Each entry contains the file's inode number, file name, and
      access permissions, along with the special functions used to generate the file's
      contents. Drivers can register and deregister entries in this tree at any time,
      and a special section of the tree--appearing under the /proc/sys directory
      --is reserved for kernel variables. Files under this tree are managed by a set
      of common handlers that allow both reading and writing of these variables,
      so a system administrator can tune the value of kernel parameters simply by
      writing out the new desired values in ASCII decimal to the appropriate file.
      To allow efficient access to these variables from within applications, the
      /proc/sys subtree is made available through a special system call, sysctl(),
      that reads and writes the same variables in binary, rather than in text, without
      the overhead of the file system. sysctl() is not an extra facility; it simply reads
      the /proc dynamic entry tree to identify the variables to which the application
      is referring.
18.8  Input and Output
      To the user, the I/O system in Linux looks much like that in any UNIX system.
      That is, to the extent possible, all device drivers appear as normal files. Users
      can open an access channel to a device in the same way they open any
      other file --devices can appear as objects within the file system. The system
      administrator can create special files within a file system that contain references
      to a specific device driver, and a user opening such a file will be able to read
      from and write to the device referenced. By using the normal file-protection
      system, which determines who can access which file, the administrator can set
      access permissions for each device.
      Linux splits all devices into three classes: block devices, character devices,
      and network devices. Figure 18.8 illustrates the overall structure of the device-
      driver system.
      Block devices include all devices that allow random access to completely
      independent, fixed-sized blocks of data, including hard disks and floppy disks,
      CD-ROMs and Blu-ray discs, and flash memory. Block devices are typically



816  Chapter 18        The Linux System
     used to store file systems, but direct access to a block device is also allowed
     so that programs can create and repair the file system that the device contains.
     Applications can also access these block devices directly if they wish. For
     example, a database application may prefer to perform its own fine-tuned
     layout of data onto a disk rather than using the general-purpose file system.
     Character devices include most other devices, such as mice and keyboards.
     The fundamental difference between block and character devices is random
     access--block devices are accessed randomly, while character devices are
     accessed serially. For example, seeking to a certain position in a file might
     be supported for a DVD but makes no sense for a pointing device such as a
     mouse.
     Network           devices  are   dealt     with  differently     from        block  and     character
     devices.  Users    cannot        directly  transfer  data    to  network     devices.       Instead,
     they must communicate indirectly by opening a connection to the kernel's
     networking subsystem. We discuss the interface to network devices separately
     in Section 18.10.
     18.8.1    Block Devices
     Block devices provide the main interface to all disk devices in a system.
     Performance is particularly important for disks, and the block-device system
     must provide functionality to ensure that disk access is as fast as possible. This
     functionality is achieved through the scheduling of I/O operations.
     In the context of block devices, a block represents the unit with which the
     kernel performs I/O. When a block is read into memory, it is stored in a buffer.
     The request manager is the layer of software that manages the reading and
     writing of buffer contents to and from a block-device driver.
     A separate list of requests is kept for each block-device driver. Traditionally,
     these requests have been scheduled according to a unidirectional-elevator
     (C-SCAN) algorithm that exploits the order in which requests are inserted in
     and removed from the lists. The request lists are maintained in sorted order of
     increasing starting-sector number. When a request is accepted for processing
     by a block-device driver, it is not removed from the list. It is removed only after
     the I/O is complete, at which point the driver continues with the next request
     in the list, even if new requests have been inserted in the list before the active
                                                user application
               file system      block           character                         network
                                device file     device file                       socket
                       I/O scheduler                                  line        protocol
                                                      TTY driver      discipline         driver
               block        SCSI manager
               device                           character                         network
               driver       SCSI device               device                      device
                            driver                    driver                             driver
                            Figure 18.8         Device-driver block  structure.



                                                 18.8     Input and Output          817
request. As new I/O requests are made, the request manager attempts to merge
requests in the lists.
Linux kernel version 2.6 introduced a new I/O scheduling algorithm.
Although    a  simple    elevator  algorithm     remains  available,  the  default  I/O
scheduler is now the Completely Fair Queueing (CFQ) scheduler. The CFQ I/O
scheduler is fundamentally different from elevator-based algorithms. Instead
of sorting requests into a list, CFQ maintains a set of lists--by default, one
for each process. Requests originating from a process go in that process's list.
For example, if two processes are issuing I/O requests, CFQ will maintain
two separate lists of requests, one for each process. The lists are maintained
according to the C-SCAN algorithm.
CFQ       services  the  lists  differently  as  well.  Where  a  traditional  C-SCAN
algorithm is indifferent to a specific process, CFQ services each process's list
round-robin. It pulls a configurable number of requests (by default, four)
from each list before moving on to the next. This method results in fairness
at the process level--each process receives an equal fraction of the disk's
bandwidth. The result is beneficial with interactive workloads where I/O
latency is important. In practice, however, CFQ performs well with most
workloads.
18.8.2    Character Devices
A character-device driver can be almost any device driver that does not offer
random access to fixed blocks of data. Any character-device drivers registered
to the Linux kernel must also register a set of functions that implement the
file I/O operations that the driver can handle. The kernel performs almost no
preprocessing of a file read or write request to a character device. It simply
passes the request to the device in question and lets the device deal with the
request.
The main exception to this rule is the special subset of character-device
drivers that implement terminal devices. The kernel maintains a standard
interface to these drivers by means of a set of tty struct structures. Each of
these structures provides buffering and flow control on the data stream from
the terminal device and feeds those data to a line discipline.
A line discipline is an interpreter for the information from the terminal
device. The most common line discipline is the tty discipline, which glues the
terminal's data stream onto the standard input and output streams of a user's
running processes, allowing those processes to communicate directly with the
user's terminal. This job is complicated by the fact that several such processes
may be running simultaneously, and the tty line discipline is responsible for
attaching and detaching the terminal's input and output from the various
processes connected to it as those processes are suspended or awakened by the
user.
Other line disciplines also are implemented that have nothing to do with
I/O to a user process. The PPP and SLIP networking protocols are ways of
encoding a networking connection over a terminal device such as a serial
line. These protocols are implemented under Linux as drivers that at one end
appear to the terminal system as line disciplines and at the other end appear
to the networking system as network-device drivers. After one of these line
disciplines has been enabled on a terminal device, any data appearing on that
terminal will be routed directly to the appropriate network-device driver.



818   Chapter 18  The Linux System
18.9  Interprocess Communication
      Linux provides a rich environment for processes to communicate with each
      other. Communication may be just a matter of letting another process know
      that some event has occurred, or it may involve transferring data from one
      process to another.
      18.9.1  Synchronization and Signals
      The standard Linux mechanism for informing a process that an event has
      occurred is the signal. Signals can be sent from any process to any other
      process, with restrictions on signals sent to processes owned by another user.
      However, a limited number of signals are available, and they cannot carry
      information. Only the fact that a signal has occurred is available to a process.
      Signals are not generated only by processes. The kernel also generates signals
      internally. For example, it can send a signal to a server process when data
      arrive on a network channel, to a parent process when a child terminates, or to
      a waiting process when a timer expires.
      Internally, the Linux kernel does not use signals to communicate with
      processes running in kernel mode. If a kernel-mode process is expecting an
      event to occur, it will not use signals to receive notification of that event.
      Rather, communication about incoming asynchronous events within the kernel
      takes place through the use of scheduling states and wait queue structures.
      These mechanisms allow kernel-mode processes to inform one another about
      relevant events, and they also allow events to be generated by device drivers or
      by the networking system. Whenever a process wants to wait for some event
      to complete, it places itself on a wait queue associated with that event and
      tells the scheduler that it is no longer eligible for execution. Once the event has
      completed, every process on the wait queue will be awoken. This procedure
      allows multiple processes to wait for a single event. For example, if several
      processes are trying to read a file from a disk, then they will all be awakened
      once the data have been read into memory successfully.
      Although signals have always been the main mechanism for commu-
      nicating asynchronous events among processes, Linux also implements the
      semaphore mechanism of System V UNIX. A process can wait on a semaphore
      as easily as it can wait for a signal, but semaphores have two advantages:
      large numbers of semaphores can be shared among multiple independent pro-
      cesses, and operations on multiple semaphores can be performed atomically.
      Internally, the standard Linux wait queue mechanism synchronizes processes
      that are communicating with semaphores.
      18.9.2  Passing of Data among Processes
      Linux offers several mechanisms for passing data among processes. The stan-
      dard UNIX pipe mechanism allows a child process to inherit a communication
      channel from its parent; data written to one end of the pipe can be read at the
      other. Under Linux, pipes appear as just another type of inode to virtual file
      system software, and each pipe has a pair of wait queues to synchronize the
      reader and writer. UNIX also defines a set of networking facilities that can send
      streams of data to both local and remote processes. Networking is covered in
      Section 18.10.



                                        18.10    Network Structure                819
    Another   process  communications   method,  shared  memory,  offers          an
extremely fast way to communicate large or small amounts of data. Any data
written by one process to a shared memory region can be read immediately by
any other process that has mapped that region into its address space. The main
disadvantage of shared memory is that, on its own, it offers no synchronization.
A process can neither ask the operating system whether a piece of shared
memory has been written to nor suspend execution until such a write occurs.
Shared memory becomes particularly powerful when used in conjunction with
another interprocess-communication mechanism that provides the missing
synchronization.
    A shared-memory region in Linux is a persistent object that can be created
or deleted by processes. Such an object is treated as though it were a small,
independent address space. The Linux paging algorithms can elect to page
shared-memory pages out to disk, just as they can page out a process's data
pages. The shared-memory object acts as a backing store for shared-memory
regions, just as a file can act as a backing store for a memory-mapped memory
region. When a file is mapped into a virtual address space region, then any
page faults that occur cause the appropriate page of the file to be mapped into
virtual memory. Similarly, shared-memory mappings direct page faults to map
in pages from a persistent shared-memory object. Also just as for files, shared-
memory objects remember their contents even if no processes are currently
mapping them into virtual memory.
18.10 Network Structure
Networking is a key area of functionality for Linux. Not only does Linux
support the standard Internet protocols used for most UNIX-to-UNIX com-
munications, but it also implements a number of protocols native to other,
non-UNIX operating systems. In particular, since Linux was originally imple-
mented primarily on PCs, rather than on large workstations or on server-class
systems, it supports many of the protocols typically used on PC networks, such
as AppleTalk and IPX.
    Internally, networking in the Linux kernel is implemented by three layers
of software:
1.  The socket interface
2.  Protocol drivers
3.  Network-device drivers
    User applications perform all networking requests through the socket
interface. This interface is designed to look like the 4.3 BSD socket layer, so
that any programs designed to make use of Berkeley sockets will run on Linux
without any source-code changes. This interface is described in Section A.9.1.
The BSD socket interface is sufficiently general to represent network addresses
for a wide range of networking protocols. This single interface is used in Linux
to access not just those protocols implemented on standard BSD systems but all
the protocols supported by the system.



820  Chapter 18  The Linux System
     The  next      layer   of  software  is  the  protocol  stack,  which  is  similar  in
     organization to BSD's own framework. Whenever any networking data arrive at
     this layer, either from an application's socket or from a network-device driver,
     the data are expected to have been tagged with an identifier specifying which
     network protocol they contain. Protocols can communicate with one another
     if they desire; for example, within the Internet protocol set, separate protocols
     manage routing, error reporting, and reliable retransmission of lost data.
     The  protocol  layer       may  rewrite  packets,  create  new  packets,    split   or
     reassemble packets into fragments, or simply discard incoming data. Ulti-
     mately, once the protocol layer has finished processing a set of packets, it
     passes them on, either upward to the socket interface if the data are destined
     for a local connection or downward to a device driver if the data need to be
     transmitted remotely. The protocol layer decides to which socket or device it
     will send the packet.
     All communication between the layers of the networking stack is per-
     formed by passing single skbuff (socket buffer) structures. Each of these
     structures contains a set of pointers into a single continuous area of memory,
     representing a buffer inside which network packets can be constructed. The
     valid data in a skbuff do not need to start at the beginning of the skbuff's
     buffer, and they do not need to run to the end. The networking code can
     add data to or trim data from either end of the packet, as long as the result
     still fits into the skbuff. This capacity is especially important on modern
     microprocessors, where improvements in CPU speed have far outstripped the
     performance of main memory. The skbuff architecture allows flexibility in
     manipulating packet headers and checksums while avoiding any unnecessary
     data copying.
     The most important set of protocols in the Linux networking system is the
     TCP/IP protocol suite. This suite comprises a number of separate protocols.
     The IP protocol implements routing between different hosts anywhere on the
     network. On top of the routing protocol are the UDP, TCP, and ICMP protocols.
     The UDP protocol carries arbitrary individual datagrams between hosts. The
     TCP protocol implements reliable connections between hosts with guaranteed
     in-order delivery of packets and automatic retransmission of lost data. The
     ICMP protocol carries various error and status messages between hosts.
     Each packet (skbuff) arriving at the networking stack's protocol software
     is expected to be already tagged with an internal identifier indicating the
     protocol to which the packet is relevant. Different networking-device drivers
     encode the protocol type in different ways; thus, the protocol for incoming
     data must be identified in the device driver. The device driver uses a hash table
     of known networking-protocol identifiers to look up the appropriate protocol
     and passes the packet to that protocol. New protocols can be added to the hash
     table as kernel-loadable modules.
     Incoming IP packets are delivered to the IP driver. The job of this layer
     is to perform routing. After deciding where the packet is to be sent, the IP
     driver forwards the packet to the appropriate internal protocol driver to be
     delivered locally or injects it back into a selected network-device-driver queue
     to be forwarded to another host. It performs the routing decision using two
     tables: the persistent forwarding information base (FIB) and a cache of recent
     routing decisions. The FIB holds routing-configuration information and can
     specify routes based either on a specific destination address or on a wildcard



                                                                    18.11  Security         821
       representing multiple destinations. The FIB is organized as a set of hash tables
       indexed by destination address; the tables representing the most specific routes
       are always searched first. Successful lookups from this table are added to
       the route-caching table, which caches routes only by specific destination. No
       wildcards are stored in the cache, so lookups can be made quickly. An entry in
       the route cache expires after a fixed period with no hits.
               At various stages, the IP software passes packets to a separate section
       of code for firewall management--selective filtering of packets according
       to  arbitrary  criteria,  usually  for  security  purposes.  The    firewall  manager
       maintains a number of separate firewall chains and allows a skbuff to be
       matched against any chain. Chains are reserved for separate purposes: one is
       used for forwarded packets, one for packets being input to this host, and one
       for data generated at this host. Each chain is held as an ordered list of rules,
       where a rule specifies one of a number of possible firewall-decision functions
       plus some arbitrary data for matching purposes.
               Two other functions performed by the IP driver are disassembly and
       reassembly of large packets. If an outgoing packet is too large to be queued to
       a device, it is simply split up into smaller fragments, which are all queued to
       the driver. At the receiving host, these fragments must be reassembled. The IP
       driver maintains an ipfrag object for each fragment awaiting reassembly and
       an ipq for each datagram being assembled. Incoming fragments are matched
       against each known ipq. If a match is found, the fragment is added to it;
       otherwise, a new ipq is created. Once the final fragment has arrived for a
       ipq, a completely new skbuff is constructed to hold the new packet, and this
       packet is passed back into the IP driver.
               Packets identified by the IP as destined for this host are passed on to one
       of the other protocol drivers. The UDP and TCP protocols share a means of
       associating packets with source and destination sockets: each connected pair
       of sockets is uniquely identified by its source and destination addresses and
       by the source and destination port numbers. The socket lists are linked to
       hash tables keyed on these four address and port values for socket lookup on
       incoming packets. The TCP protocol has to deal with unreliable connections, so
       it maintains ordered lists of unacknowledged outgoing packets to retransmit
       after a timeout and of incoming out-of-order packets to be presented to the
       socket when the missing data have arrived.
18.11  Security
       Linux's security model is closely related to typical UNIX security mechanisms.
       The security concerns can be classified in two groups:
           1.  Authentication. Making sure that nobody can access the system without
               first proving that she has entry rights
           2.  Access control. Providing a mechanism for checking whether a user has
               the right to access a certain object and preventing access to objects as
               required



822  Chapter 18  The Linux System
     18.11.1    Authentication
     Authentication in UNIX has typically been performed through the use of a
     publicly readable password file. A user's password is combined with a random
     "salt" value, and the result is encoded with a one-way transformation function
     and stored in the password file. The use of the one-way function means that
     the original password cannot be deduced from the password file except by
     trial and error. When a user presents a password to the system, the password is
     recombined with the salt value stored in the password file and passed through
     the same one-way transformation. If the result matches the contents of the
     password file, then the password is accepted.
     Historically, UNIX implementations of this mechanism have had several
     drawbacks. Passwords were often limited to eight characters, and the number
     of possible salt values was so low that an attacker could easily combine a
     dictionary of commonly used passwords with every possible salt value and
     have a good chance of matching one or more passwords in the password
     file, gaining unauthorized access to any accounts compromised as a result.
     Extensions to the password mechanism have been introduced that keep the
     encrypted password secret in a file that is not publicly readable, that allow
     longer passwords, or that use more secure methods of encoding the password.
     Other authentication mechanisms have been introduced that limit the periods
     during which a user is permitted to connect to the system. Also, mechanisms
     exist to distribute authentication information to all the related systems in a
     network.
     A  new      security  mechanism         has  been  developed    by   UNIX     vendors  to
     address    authentication  problems.    The  pluggable      authentication    modules
     (PAM) system is based on a shared library that can be used by any system
     component that needs to authenticate users. An implementation of this system
     is available under Linux. PAM allows authentication modules to be loaded on
     demand as specified in a system-wide configuration file. If a new authentication
     mechanism is added at a later date, it can be added to the configuration file,
     and all system components will immediately be able to take advantage of it.
     PAM modules can specify authentication methods, account restrictions, session-
     setup functions, and password-changing functions (so that, when users change
     their passwords, all the necessary authentication mechanisms can be updated
     at once).
     18.11.2     Access Control
     Access control under UNIX systems, including Linux, is performed through the
     use of unique numeric identifiers. A user identifier (UID) identifies a single user
     or a single set of access rights. A group identifier (GID) is an extra identifier
     that can be used to identify rights belonging to more than one user.
     Access      control   is   applied  to  various    objects  in  the  system.  Every    file
     available in the system is protected by the standard access-control mecha-
     nism. In addition, other shared objects, such as shared-memory sections and
     semaphores, employ the same access system.
     Every object in a UNIX system under user and group access control has a
     single UID and a single GID associated with it. User processes also have a single
     UID, but they may have more than one GID. If a process's UID matches the UID
     of an object, then the process has user rights or owner rights to that object.



                                                            18.11  Security           823
If the UIDs do not match but any GID of the process matches the object's GID,
then group rights are conferred; otherwise, the process has world rights to the
object.
     Linux performs access control by assigning objects a protection mask that
specifies which access modes--read, write, or execute --are to be granted to
processes with owner, group, or world access. Thus, the owner of an object
might have full read, write, and execute access to a file; other users in a certain
group might be given read access but denied write access; and everybody else
might be given no access at all.
     The only exception is the privileged root UID. A process with this special
UID  is  granted  automatic  access    to  any  object  in  the  system,  bypassing
normal access checks. Such processes are also granted permission to perform
privileged  operations,  such     as  reading  any    physical  memory    or  opening
reserved network sockets. This mechanism allows the kernel to prevent normal
users from accessing these resources: most of the kernel's key internal resources
are implicitly owned by the root UID.
     Linux implements the standard UNIX setuid mechanism described in
Section A.3.2. This mechanism allows a program to run with privileges different
from those of the user running the program. For example, the lpr program
(which submits a job to a print queue) has access to the system's print queues
even if the user running that program does not. The UNIX implementation of
setuid distinguishes between a process's real and effective UID. The real
UID is that of the user running the program; the effective UID is that of the file's
owner.
     Under Linux, this mechanism is augmented in two ways. First, Linux
implements  the   POSIX  specification's       saved  user-id    mechanism,   which
allows a process to drop and reacquire its effective UID repeatedly. For security
reasons, a program may want to perform most of its operations in a safe mode,
waiving the privileges granted by its setuid status; but it may wish to perform
selected operations with all its privileges. Standard UNIX implementations
achieve this capacity only by swapping the real and effective UIDs. When this
is done, the previous effective UID is remembered, but the program's real UID
does not always correspond to the UID of the user running the program. Saved
UIDs allow a process to set its effective UID to its real UID and then return to
the previous value of its effective UID without having to modify the real UID at
any time.
     The second enhancement provided by Linux is the addition of a process
characteristic that grants just a subset of the rights of the effective UID. The
fsuid and fsgid process properties are used when access rights are granted
to files. The appropriate property is set every time the effective UID or GID is
set. However, the fsuid and fsgid can be set independently of the effective ids,
allowing a process to access files on behalf of another user without taking on the
identity of that other user in any other way. Specifically, server processes can
use this mechanism to serve files to a certain user without becoming vulnerable
to being killed or suspended by that user.
     Finally, Linux provides a mechanism for flexible passing of rights from
one program to another--a mechanism that has become common in modern
versions of UNIX. When a local network socket has been set up between any
two processes on the system, either of those processes may send to the other
process a file descriptor for one of its open files; the other process receives a



824  Chapter 18  The Linux System
     duplicate file descriptor for the same file. This mechanism allows a client to
     pass access to a single file selectively to some server process without granting
     that process any other privileges. For example, it is no longer necessary for a
     print server to be able to read all the files of a user who submits a new print
     job. The print client can simply pass the server file descriptors for any files to
     be printed, denying the server access to any of the user's other files.
18.12 Summary
     Linux is a modern, free operating system based on UNIX standards. It has been
     designed to run efficiently and reliably on common PC hardware; it also runs on
     a variety of other platforms, such as mobile phones. It provides a programming
     interface and user interface compatible with standard UNIX systems and can
     run a large number of UNIX applications, including an increasing number of
     commercially supported applications.
     Linux has not evolved in a vacuum. A complete Linux system includes
     many components that were developed independently of Linux. The core
     Linux operating-system kernel is entirely original, but it allows much existing
     free UNIX software to run, resulting in an entire UNIX-compatible operating
     system free from proprietary code.
     The Linux kernel is implemented as a traditional monolithic kernel for
     performance reasons, but it is modular enough in design to allow most drivers
     to be dynamically loaded and unloaded at run time.
     Linux is a multiuser system, providing protection between processes and
     running multiple processes according to a time-sharing scheduler. Newly
     created processes can share selective parts of their execution environment
     with their parent processes, allowing multithreaded programming. Interpro-
     cess communication is supported by both System V mechanisms--message
     queues, semaphores, and shared memory--and BSD's socket interface. Multi-
     ple networking protocols can be accessed simultaneously through the socket
     interface.
     The memory-management system uses page sharing and copy-on-write
     to minimize the duplication of data shared by different processes. Pages are
     loaded on demand when they are first referenced and are paged back out to
     backing store according to an LFU algorithm if physical memory needs to be
     reclaimed.
     To the user, the file system appears as a hierarchical directory tree that
     obeys UNIX semantics. Internally, Linux uses an abstraction layer to manage
     multiple file systems. Device-oriented, networked, and virtual file systems are
     supported. Device-oriented file systems access disk storage through a page
     cache that is unified with the virtual memory system.
Practice Exercises
     18.1  Dynamically loadable kernel modules give flexibility when drivers are
           added to a system, but do they have disadvantages too? Under what
           circumstances would a kernel be compiled into a single binary file, and
           when would it be better to keep it split into modules? Explain your
           answer.



                                                                     Exercises        825
18.2       Multithreading is a commonly used programming technique. Describe
           three different ways to implement threads, and compare these three
           methods with the Linux clone() mechanism. When might using each
           alternative mechanism be better or worse than using clones?
18.3       The Linux kernel does not allow paging out of kernel memory. What
           effect does this restriction have on the kernel's design? What are two
           advantages and two disadvantages of this design decision?
18.4       Discuss  three  advantages      of  dynamic   (shared)  linkage   of  libraries
           compared with static linkage. Describe two cases in which static linkage
           is preferable.
18.5       Compare the use of networking sockets with the use of shared memory
           as a mechanism for communicating data between processes on a single
           computer. What are the advantages of each method? When might each
           be preferred?
18.6       At  one  time,  UNIX       systems  used  disk-layout   optimizations    based
           on the rotation position of disk data, but modern implementations,
           including Linux, simply optimize for sequential data access. Why do
           they do so? Of what hardware characteristics does sequential access
           take advantage? Why is rotational optimization no longer so useful?
Exercises
18.7       What are the advantages and disadvantages of writing an operating
           system in a high-level language, such as C?
18.8       In what circumstances is the system-call sequence fork() exec() most
           appropriate? When is vfork() preferable?
18.9       What   socket   type   should   be  used  to  implement   an  intercomputer
           file-transfer program? What type should be used for a program that
           periodically    tests  to  see  whether   another  computer   is  up   on  the
           network? Explain your answer.
18.10      Linux  runs     on  a  variety  of  hardware  platforms.  What    steps  must
           Linux developers take to ensure that the system is portable to different
           processors and memory-management architectures and to minimize
           the amount of architecture-specific kernel code?
18.11      What are the advantages and disadvantages of making only some of the
           symbols defined inside a kernel accessible to a loadable kernel module?
18.12      What are the primary goals of the conflict-resolution mechanism used
           by the Linux kernel for loading kernel modules?
18.13      Discuss how the clone() operation supported by Linux is used to
           support both processes and threads.
18.14      Would you classify Linux threads as user-level threads or as kernel-level
           threads? Support your answer with the appropriate arguments.
18.15      What extra costs are incurred in the creation and scheduling of a
           process, compared with the cost of a cloned thread?



826  Chapter 18     The Linux System
     18.16  How does Linux's Completely Fair Scheduler (CFS) provide improved
            fairness over a traditional UNIX process scheduler? When is the fairness
            guaranteed?
     18.17  What are the two configurable variables of the Completely Fair Sched-
            uler (CFS)? What are the pros and cons of setting each of them to very
            small and very large values?
     18.18  The Linux scheduler implements "soft" real-time scheduling. What
            features necessary for certain real-time programming tasks are missing?
            How might they be added to the kernel? What are the costs (downsides)
            of such features?
     18.19  Under what circumstances would a user process request an operation
            that results in the allocation of a demand-zero memory region?
     18.20  What scenarios would cause a page of memory to be mapped into a user
            program's address space with the copy-on-write attribute enabled?
     18.21  In Linux, shared libraries perform many operations central to the
            operating system. What is the advantage of keeping this functionality
            out of the kernel? Are there any drawbacks? Explain your answer.
     18.22  What are the benefits of a journaling file system such as Linux's ext3?
            What are the costs? Why does ext3 provide the option to journal only
            metadata?
     18.23  The directory structure of a Linux operating system could include files
            corresponding to several different file systems, including the Linux
            /proc file system. How might the need to support different file-system
            types affect the structure of the Linux kernel?
     18.24  In what ways does the Linux setuid feature differ from the setuid
            feature SVR4?
     18.25  The Linux source code is freely and widely available over the Inter-
            net and from CD-ROM vendors. What are three implications of this
            availability for the security of the Linux system?
Bibliographical Notes
     The    Linux   system   is  a  product  of  the  Internet;  as   a  result,  much   of  the
     available documentation on Linux is available in some form on the Internet.
     The following key sites reference most of the useful information available:
     ·    The Linux Cross-Reference Page (LXR) (http://lxr.linux.no) maintains current
          listings  of  the  Linux  kernel,  browsable  via      the  Web  and    fully  cross-
          referenced.
     ·    The Kernel Hackers' Guide provides a helpful overview of the Linux kernel
          components and internals and is located at http://tldp.org/LDP/tlk/tlk.html.



                                                                Bibliography        827
·  The Linux Weekly News (LWN) (http://lwn.net) provides weekly Linux-
   related news, including a very well researched subsection on Linux kernel
   news.
   Many mailing lists devoted to Linux are also available. The most important
are maintained by a mailing-list manager that can be reached at the e-mail
address majordomo@vger.rutgers.edu. Send e-mail to this address with the
single line "help" in the mail's body for information on how to access the list
server and to subscribe to any lists.
   Finally, the Linux system itself can be obtained over the Internet. Complete
Linux  distributions     are  available  from  the   home  sites  of  the   companies
concerned,     and  the  Linux   community     also  maintains    archives  of  current
system components at several places on the Internet. The most important is
ftp://ftp.kernel.org/pub/linux.
   In addition to investigating Internet resources, you can read about the
internals of the Linux kernel in [Mauerer (2008)] and [Love (2010)].
Bibliography
[Love (2010)]     R. Love, Linux Kernel Development, Third Edition, Developer's
Library (2010).
[Mauerer (2008)]    W. Mauerer, Professional Linux Kernel Architecture, John Wiley
and Sons (2008).



