File-System Implementation


File -System                                                           12C H A P T E R
Implementation
      As we saw in Chapter 11, the file system provides the mechanism for on-line
      storage and access to file contents, including data and programs. The file system
      resides permanently on secondary storage, which is designed to hold a large
      amount of data permanently. This chapter is primarily concerned with issues
      surrounding file storage and access on the most common secondary-storage
      medium, the disk. We explore ways to structure file use, to allocate disk space,
      to recover freed space, to track the locations of data, and to interface other
      parts of the operating system to secondary storage. Performance issues are
      considered throughout the chapter.
      CHAPTER OBJECTIVES
      · To describe the details of implementing local file systems and directory
          structures.
      · To describe the implementation of remote file systems.
      · To discuss block allocation and free-block algorithms and trade-offs.
12.1  File-System Structure
      Disks  provide   most  of  the  secondary  storage  on    which  file  systems       are
      maintained. Two characteristics make them convenient for this purpose:
      1.     A disk can be rewritten in place; it is possible to read a block from the
             disk, modify the block, and write it back into the same place.
      2.     A disk can access directly any block of information it contains. Thus, it is
             simple to access any file either sequentially or randomly, and switching
             from one file to another requires only moving the read ­write heads and
             waiting for the disk to rotate.
      We discuss disk structure in great detail in Chapter 10.
          To improve I/O efficiency, I/O transfers between memory and disk are
      performed in units of blocks. Each block has one or more sectors. Depending
                                                                                           543



544  Chapter 12      File-System Implementation
     on the disk drive, sector size varies from 32 bytes to 4,096 bytes; the usual size
     is 512 bytes.
          File systems provide efficient and convenient access to the disk by allowing
     data to be stored, located, and retrieved easily. A file system poses two quite
     different design problems. The first problem is defining how the file system
     should look to the user. This task involves defining a file and its attributes,
     the operations allowed on a file, and the directory structure for organizing
     files. The second problem is creating algorithms and data structures to map the
     logical file system onto the physical secondary-storage devices.
          The file system itself is generally composed of many different levels. The
     structure shown in Figure 12.1 is an example of a layered design. Each level in
     the design uses the features of lower levels to create new features for use by
     higher levels.
          The I/O control level consists of device drivers and interrupt handlers
     to transfer information between the main memory and the disk system. A
     device driver can be thought of as a translator. Its input consists of high-
     level commands such as "retrieve block 123." Its output consists of low-level,
     hardware-specific instructions that are used by the hardware controller, which
     interfaces the I/O device to the rest of the system. The device driver usually
     writes specific bit patterns to special locations in the I/O controller's memory
     to tell the controller which device location to act on and what actions to take.
     The details of device drivers and the I/O infrastructure are covered in Chapter
     13.
          The  basic  file  system  needs  only  to     issue  generic  commands  to     the
     appropriate device driver to read and write physical blocks on the disk. Each
     physical block is identified by its numeric disk address (for example, drive 1,
     cylinder 73, track 2, sector 10). This layer also manages the memory buffers
     and caches that hold various file-system, directory, and data blocks. A block
     in the buffer is allocated before the transfer of a disk block can occur. When
     the buffer is full, the buffer manager must find more buffer memory or free
                                    application programs
                                    logical file system
                                    file-organization module
                                    basic file system
                                           I/O control
                                           devices
                            Figure 12.1    Layered file system.



                                            12.1       File-System Structure         545
up buffer space to allow a requested I/O to complete. Caches are used to hold
frequently used file-system metadata to improve performance, so managing
their contents is critical for optimum system performance.
     The file-organization module knows about files and their logical blocks,
as well as physical blocks. By knowing the type of file allocation used and
the location of the file, the file-organization module can translate logical block
addresses to physical block addresses for the basic file system to transfer.
Each file's logical blocks are numbered from 0 (or 1) through N. Since the
physical blocks containing the data usually do not match the logical numbers,
a translation is needed to locate each block. The file-organization module also
includes the free-space manager, which tracks unallocated blocks and provides
these blocks to the file-organization module when requested.
     Finally, the logical file system manages metadata information. Metadata
includes all of the file-system structure except the actual data (or contents of
the files). The logical file system manages the directory structure to provide
the file-organization module with the information the latter needs, given a
symbolic file name. It maintains file structure via file-control blocks. A file-
control block (FCB) (an inode in UNIX file systems) contains information about
the file, including ownership, permissions, and location of the file contents. The
logical file system is also responsible for protection, as discussed in Chaptrers
11 and 14.
     When a layered structure is used for file-system implementation, duplica-
tion of code is minimized. The I/O control and sometimes the basic file-system
code can be used by multiple file systems. Each file system can then have its
own logical file-system and file-organization modules. Unfortunately, layering
can introduce more operating-system overhead, which may result in decreased
performance. The use of layering, including the decision about how many
layers to use and what each layer should do, is a major challenge in designing
new systems.
     Many file systems are in use today, and most operating systems support
more    than  one.  For  example,  most     CD-ROMs    are  written    in  the  ISO  9660
format, a standard format agreed on by CD-ROM manufacturers. In addition
to removable-media file systems, each operating system has one or more disk-
based file systems. UNIX uses the UNIX file system (UFS), which is based on the
Berkeley Fast File System (FFS). Windows supports disk file-system formats of
FAT, FAT32, and NTFS (or Windows NT File System), as well as CD-ROM and DVD
file-system formats. Although Linux supports over forty different file systems,
the standard Linux file system is known as the extended file system, with
the most common versions being ext3 and ext4. There are also distributed file
systems in which a file system on a server is mounted by one or more client
computers across a network.
     File-system research continues to be an active area of operating-system
design  and   implementation.      Google   created    its  own  file  system   to   meet
the  company's      specific  storage  and  retrieval  needs,    which     include   high-
performance access from many clients across a very large number of disks.
Another interesting project is the FUSE file system, which provides flexibility in
file-system development and use by implementing and executing file systems
as user-level rather than kernel-level code. Using FUSE, a user can add a new
file system to a variety of operating systems and can use that file system to
manage her files.



546   Chapter 12  File-System Implementation
12.2  File-System Implementation
      As was described in Section 11.1.2, operating systems implement open()
      and close() systems calls for processes to request access to file contents.
      In this section, we delve into the structures and operations used to implement
      file-system operations.
      12.2.1  Overview
      Several on-disk and in-memory structures are used to implement a file system.
      These structures vary depending on the operating system and the file system,
      but some general principles apply.
         On disk, the file system may contain information about how to boot an
      operating system stored there, the total number of blocks, the number and
      location of free blocks, the directory structure, and individual files. Many of
      these structures are detailed throughout the remainder of this chapter. Here,
      we describe them briefly:
      ·  A boot control block (per volume) can contain information needed by the
         system to boot an operating system from that volume. If the disk does not
         contain an operating system, this block can be empty. It is typically the
         first block of a volume. In UFS, it is called the boot block. In NTFS, it is the
         partition boot sector.
      ·  A volume control block (per volume) contains volume (or partition)
         details, such as the number of blocks in the partition, the size of the blocks,
         a free-block count and free-block pointers, and a free-FCB count and FCB
         pointers. In UFS, this is called a superblock. In NTFS, it is stored in the
         master file table.
      ·  A directory structure (per file system) is used to organize the files. In UFS,
         this includes file names and associated inode numbers. In NTFS, it is stored
         in the master file table.
      ·  A    per-file  FCB  contains  many   details  about  the  file.  It  has  a  unique
         identifier number to allow association with a directory entry. In NTFS,
         this information is actually stored within the master file table, which uses
         a relational database structure, with a row per file.
         The in-memory information is used for both file-system management and
      performance improvement via caching. The data are loaded at mount time,
      updated during file-system operations, and discarded at dismount. Several
      types of structures may be included.
      ·  An in-memory mount table contains information about each mounted
         volume.
      ·  An in-memory directory-structure cache holds the directory information
         of recently accessed directories. (For directories at which volumes are
         mounted, it can contain a pointer to the volume table.)
      ·  The system-wide open-file table contains a copy of the FCB of each open
         file, as well as other information.



                                       12.2  File-System Implementation                 547
                     file permissions
                     file dates (create, access, write)
                     file owner, group, ACL
                     file size
                     file data blocks or pointers to file data blocks
                     Figure 12.2       A typical file-control block.
·  The per-process open-file table contains a pointer to the appropriate entry
   in the system-wide open-file table, as well as other information.
·  Buffers hold file-system blocks when they are being read from disk or
   written to disk.
   To create a new file, an application program calls the logical file system.
The logical file system knows the format of the directory structures. To create a
new file, it allocates a new FCB. (Alternatively, if the file-system implementation
creates all FCBs at file-system creation time, an FCB is allocated from the set
of free FCBs.) The system then reads the appropriate directory into memory,
updates it with the new file name and FCB, and writes it back to the disk. A
typical FCB is shown in Figure 12.2.
   Some operating systems, including UNIX, treat a directory exactly the same
as a file --one with a "type" field indicating that it is a directory. Other operating
systems, including Windows, implement separate system calls for files and
directories and treat directories as entities separate from files. Whatever the
larger structural issues, the logical file system can call the file-organization
module to map the directory I/O into disk-block numbers, which are passed
on to the basic file system and I/O control system.
   Now that a file has been created, it can be used for I/O. First, though, it
must be opened. The open() call passes a file name to the logical file system.
The open() system call first searches the system-wide open-file table to see
if the file is already in use by another process. If it is, a per-process open-file
table entry is created pointing to the existing system-wide open-file table. This
algorithm can save substantial overhead. If the file is not already open, the
directory structure is searched for the given file name. Parts of the directory
structure are usually cached in memory to speed directory operations. Once
the file is found, the FCB is copied into a system-wide open-file table in memory.
This table not only stores the FCB but also tracks the number of processes that
have the file open.
   Next, an entry is made in the per-process open-file table, with a pointer
to the entry in the system-wide open-file table and some other fields. These
other fields may include a pointer to the current location in the file (for the next
read() or write() operation) and the access mode in which the file is open.
The open() call returns a pointer to the appropriate entry in the per-process



548  Chapter 12    File-System Implementation
                                                                  directory structure
     open (file name)
                                  directory structure             file-control block
     user space                   kernel memory                   secondary storage
                                            (a)
                           index
                                                                  data blocks
     read (index)
                           per-process           system-wide      file-control block
                           open-file table       open-file table
     user space                   kernel memory                   secondary storage
                                            (b)
     Figure 12.3       In-memory file-system structures. (a) File open. (b) File read.
     file-system table. All file operations are then performed via this pointer. The
     file name may not be part of the open-file table, as the system has no use for
     it once the appropriate FCB is located on disk. It could be cached, though, to
     save time on subsequent opens of the same file. The name given to the entry
     varies. UNIX systems refer to it as a file descriptor; Windows refers to it as a
     file handle.
     When a process closes the file, the per-process table entry is removed, and
     the system-wide entry's open count is decremented. When all users that have
     opened the file close it, any updated metadata is copied back to the disk-based
     directory structure, and the system-wide open-file table entry is removed.
     Some systems complicate this scheme further by using the file system as an
     interface to other system aspects, such as networking. For example, in UFS, the
     system-wide open-file table holds the inodes and other information for files
     and directories. It also holds similar information for network connections and
     devices. In this way, one mechanism can be used for multiple purposes.
     The caching aspects of file-system structures should not be overlooked.
     Most systems keep all information about an open file, except for its actual data
     blocks, in memory. The BSD UNIX system is typical in its use of caches wherever
     disk I/O can be saved. Its average cache hit rate of 85 percent shows that these
     techniques are well worth implementing. The BSD UNIX system is described
     fully in Appendix A.
     The operating structures of a file-system implementation are summarized
     in Figure 12.3.



                               12.2  File-System Implementation                     549
12.2.2  Partitions and Mounting
The layout of a disk can have many variations, depending on the operating
system. A disk can be sliced into multiple partitions, or a volume can span
multiple partitions on multiple disks. The former layout is discussed here,
while the latter, which is more appropriately considered a form of RAID, is
covered in Section 10.7.
Each partition can be either "raw," containing no file system, or "cooked,"
containing a file system. Raw disk is used where no file system is appropriate.
UNIX swap space can use a raw partition, for example, since it uses its own
format on disk and does not use a file system. Likewise, some databases use raw
disk and format the data to suit their needs. Raw disk can also hold information
needed by disk RAID systems, such as bit maps indicating which blocks are
mirrored and which have changed and need to be mirrored. Similarly, raw
disk can contain a miniature database holding RAID configuration information,
such as which disks are members of each RAID set. Raw disk use is discussed
in Section 10.5.1.
Boot information can be stored in a separate partition, as described in
Section 10.5.2. Again, it has its own format, because at boot time the system
does not have the file-system code loaded and therefore cannot interpret the
file-system format. Rather, boot information is usually a sequential series of
blocks, loaded as an image into memory. Execution of the image starts at a
predefined location, such as the first byte. This boot loader in turn knows
enough about the file-system structure to be able to find and load the kernel
and start it executing. It can contain more than the instructions for how to boot
a specific operating system. For instance, many systems can be dual-booted,
allowing us to install multiple operating systems on a single system. How does
the system know which one to boot? A boot loader that understands multiple
file systems and multiple operating systems can occupy the boot space. Once
loaded, it can boot one of the operating systems available on the disk. The disk
can have multiple partitions, each containing a different type of file system and
a different operating system.
The root partition, which contains the operating-system kernel and some-
times other system files, is mounted at boot time. Other volumes can be
automatically mounted at boot or manually mounted later, depending on
the operating system. As part of a successful mount operation, the operating
system verifies that the device contains a valid file system. It does so by asking
the device driver to read the device directory and verifying that the directory
has the expected format. If the format is invalid, the partition must have
its consistency checked and possibly corrected, either with or without user
intervention. Finally, the operating system notes in its in-memory mount table
that a file system is mounted, along with the type of the file system. The details
of this function depend on the operating system.
Microsoft Windows­based systems mount each volume in a separate name
space, denoted by a letter and a colon. To record that a file system is mounted
at F:, for example, the operating system places a pointer to the file system in
a field of the device structure corresponding to F:. When a process specifies
the driver letter, the operating system finds the appropriate file-system pointer
and traverses the directory structures on that device to find the specified file



550  Chapter 12  File-System Implementation
     or directory. Later versions of Windows can mount a file system at any point
     within the existing directory structure.
           On UNIX, file systems can be mounted at any directory. Mounting is
     implemented by setting a flag in the in-memory copy of the inode for that
     directory. The flag indicates that the directory is a mount point. A field then
     points to an entry in the mount table, indicating which device is mounted there.
     The mount table entry contains a pointer to the superblock of the file system on
     that device. This scheme enables the operating system to traverse its directory
     structure, switching seamlessly among file systems of varying types.
     12.2.3      Virtual File Systems
     The previous section makes it clear that modern operating systems must
     concurrently support multiple types of file systems. But how does an operating
     system allow multiple types of file systems to be integrated into a directory
     structure? And how can users seamlessly move between file-system types
     as    they  navigate   the   file-system  space?  We    now     discuss   some   of  these
     implementation details.
           An obvious but suboptimal method of implementing multiple types of file
     systems is to write directory and file routines for each type. Instead, however,
     most operating systems, including UNIX, use object-oriented techniques to
     simplify,   organize,   and  modularize   the     implementation.    The  use    of  these
     methods allows very dissimilar file-system types to be implemented within
     the same structure, including network file systems, such as NFS. Users can
     access files contained within multiple file systems on the local disk or even on
     file systems available across the network.
           Data  structures  and  procedures   are     used  to  isolate  the  basic  system-
     call  functionality    from  the  implementation      details.  Thus,    the  file-system
     implementation consists of three major layers, as depicted schematically in
     Figure 12.4. The first layer is the file-system interface, based on the open(),
     read(), write(), and close() calls and on file descriptors.
           The second layer is called the virtual file system (VFS) layer. The VFS layer
     serves two important functions:
     1.    It separates file-system-generic operations from their implementation
           by defining a clean VFS interface. Several implementations for the VFS
           interface may coexist on the same machine, allowing transparent access
           to different types of file systems mounted locally.
     2.    It provides a mechanism for uniquely representing a file throughout a
           network. The VFS is based on a file-representation structure, called a
           vnode, that contains a numerical designator for a network-wide unique
           file. (UNIX inodes are unique within only a single file system.) This
           network-wide uniqueness is required for support of network file systems.
           The kernel maintains one vnode structure for each active node (file or
           directory).
     Thus, the VFS distinguishes local files from remote ones, and local files are
     further distinguished according to their file-system types.
           The VFS activates file-system-specific operations to handle local requests
     according to their file-system types and calls the NFS protocol procedures for



                                     12.2       File-System Implementation           551
                                  file-system interface
                                  VFS interface
   local file system              local file system       remote file system
          type 1                     type 2                                 type 1
               disk                       disk
                                                                            network
                     Figure 12.4  Schematic view of a virtual file system.
remote requests. File handles are constructed from the relevant vnodes and
are passed as arguments to these procedures. The layer implementing the
file-system type or the remote-file-system protocol is the third layer of the
architecture.
   Let's briefly examine the VFS architecture in Linux. The four main object
types defined by the Linux VFS are:
·  The inode object, which represents an individual file
·  The file object, which represents an open file
·  The superblock object, which represents an entire file system
·  The dentry object, which represents an individual directory entry
   For each of these four object types, the VFS defines a set of operations that
may be implemented. Every object of one of these types contains a pointer to
a function table. The function table lists the addresses of the actual functions
that implement the defined operations for that particular object. For example,
an abbreviated API for some of the operations for the file object includes:
·  int  open(.       .  .)--Open a file.
·  int  close(...)--Close an already-open file.
·  ssize  t    read(.   .     .)--Read from a file.
·  ssize  t    write(.     .      .)--Write to a file.
·  int  mmap(.       .  .)--Memory-map a file.



552   Chapter 12     File-System Implementation
      An implementation of the file object for a specific file type is required to imple-
      ment each function specified in the definition of the file object. (The complete
      definition of the file object is specified in the struct  file  operations, which
      is located in the file /usr/include/linux/fs.h.)
      Thus, the VFS software layer can perform an operation on one of these
      objects by calling the appropriate function from the object's function table,
      without having to know in advance exactly what kind of object it is dealing
      with. The VFS does not know, or care, whether an inode represents a disk file,
      a directory file, or a remote file. The appropriate function for that file's read()
      operation will always be at the same place in its function table, and the VFS
      software layer will call that function without caring how the data are actually
      read.
12.3  Directory Implementation
      The selection of directory-allocation and directory-management algorithms
      significantly  affects  the  efficiency,  performance,    and  reliability  of  the    file
      system. In this section, we discuss the trade-offs involved in choosing one
      of these algorithms.
      12.3.1  Linear List
      The simplest method of implementing a directory is to use a linear list of file
      names with pointers to the data blocks. This method is simple to program
      but time-consuming to execute. To create a new file, we must first search the
      directory to be sure that no existing file has the same name. Then, we add a
      new entry at the end of the directory. To delete a file, we search the directory for
      the named file and then release the space allocated to it. To reuse the directory
      entry, we can do one of several things. We can mark the entry as unused (by
      assigning it a special name, such as an all-blank name, or by including a used­
      unused bit in each entry), or we can attach it to a list of free directory entries. A
      third alternative is to copy the last entry in the directory into the freed location
      and to decrease the length of the directory. A linked list can also be used to
      decrease the time required to delete a file.
      The real disadvantage of a linear list of directory entries is that finding a
      file requires a linear search. Directory information is used frequently, and users
      will notice if access to it is slow. In fact, many operating systems implement a
      software cache to store the most recently used directory information. A cache
      hit avoids the need to constantly reread the information from disk. A sorted
      list allows a binary search and decreases the average search time. However, the
      requirement that the list be kept sorted may complicate creating and deleting
      files, since we may have to move substantial amounts of directory information
      to maintain a sorted directory. A more sophisticated tree data structure, such
      as a balanced tree, might help here. An advantage of the sorted list is that a
      sorted directory listing can be produced without a separate sort step.
      12.3.2  Hash Table
      Another data structure used for a file directory is a hash table. Here, a linear
      list stores the directory entries, but a hash data structure is also used. The hash
      table takes a value computed from the file name and returns a pointer to the file



                                              12.4  Allocation Methods                       553
      name in the linear list. Therefore, it can greatly decrease the directory search
      time. Insertion and deletion are also fairly straightforward, although some
      provision must be made for collisions--situations in which two file names
      hash to the same location.
      The major difficulties with a hash table are its generally fixed size and the
      dependence of the hash function on that size. For example, assume that we
      make a linear-probing hash table that holds 64 entries. The hash function
      converts file names into integers from 0 to 63 (for instance, by using the
      remainder of a division by 64). If we later try to create a 65th file, we must
      enlarge the directory hash table --say, to 128 entries. As a result, we need
      a new hash function that must map file names to the range 0 to 127, and we
      must reorganize the existing directory entries to reflect their new hash-function
      values.
      Alternatively, we can use a chained-overflow hash table. Each hash entry
      can be a linked list instead of an individual value, and we can resolve collisions
      by adding the new entry to the linked list. Lookups may be somewhat slowed,
      because searching for a name might require stepping through a linked list of
      colliding table entries. Still, this method is likely to be much faster than a linear
      search through the entire directory.
12.4  Allocation Methods
      The direct-access nature of disks gives us flexibility in the implementation of
      files. In almost every case, many files are stored on the same disk. The main
      problem is how to allocate space to these files so that disk space is utilized
      effectively and files can be accessed quickly. Three major methods of allocating
      disk space are in wide use: contiguous, linked, and indexed. Each method has
      advantages and disadvantages. Although some systems support all three, it is
      more common for a system to use one method for all files within a file-system
      type.
      12.4.1   Contiguous Allocation
      Contiguous allocation requires that each file occupy a set of contiguous blocks
      on the disk. Disk addresses define a linear ordering on the disk. With this
      ordering, assuming that only one job is accessing the disk, accessing block b +
      1 after block b normally requires no head movement. When head movement
      is needed (from the last sector of one cylinder to the first sector of the next
      cylinder), the head need only move from one track to the next. Thus, the
      number of disk seeks required for accessing contiguously allocated files is
      minimal, as is seek time when a seek is finally needed.
      Contiguous allocation of a file is defined by the disk address and length (in
      block units) of the first block. If the file is n blocks long and starts at location
      b, then it occupies blocks b, b + 1, b + 2, ..., b + n - 1. The directory entry for
      each file indicates the address of the starting block and the length of the area
      allocated for this file (Figure 12.5).
      Accessing a file that has been allocated contiguously is easy. For sequential
      access, the file system remembers the disk address of the last block referenced
      and, when necessary, reads the next block. For direct access to block i of a



554  Chapter 12  File-System Implementation
                                                                 directory
                     count                                 file    start    length
                 0   1               2         3        count      0        2
                                           f            tr         14       3
                 4   5               6         7        mail       19       6
                 8   9               10    11           list       28       4
                                           tr           f          6        2
                 12  13              14    15
                 16  17              18    19
                            mail
                 20  21              22    23
                 24  25              26    27
                                     list
                 28  29              30    31
                 Figure 12.5               Contiguous allocation of disk space.
     file that starts at block b, we can immediately access block b + i. Thus, both
     sequential and direct access can be supported by contiguous allocation.
     Contiguous  allocation                has    some  problems,  however.         One  difficulty  is
     finding space for a new file. The system chosen to manage free space determines
     how this task is accomplished; these management systems are discussed in
     Section 12.5. Any management system can be used, but some are slower than
     others.
     The contiguous-allocation problem can be seen as a particular application
     of the general dynamic storage-allocation problem discussed in Section 8.3,
     which involves how to satisfy a request of size n from a list of free holes. First
     fit and best fit are the most common strategies used to select a free hole from
     the set of available holes. Simulations have shown that both first fit and best fit
     are more efficient than worst fit in terms of both time and storage utilization.
     Neither first fit nor best fit is clearly best in terms of storage utilization, but
     first fit is generally faster.
     All these algorithms suffer from the problem of external fragmentation.
     As files are allocated and deleted, the free disk space is broken into little pieces.
     External fragmentation exists whenever free space is broken into chunks. It
     becomes a problem when the largest contiguous chunk is insufficient for a
     request; storage is fragmented into a number of holes, none of which is large
     enough to store the data. Depending on the total amount of disk storage and the
     average file size, external fragmentation may be a minor or a major problem.
     One strategy for preventing loss of significant amounts of disk space to
     external fragmentation is to copy an entire file system onto another disk. The
     original disk is then freed completely, creating one large contiguous free space.
     We then copy the files back onto the original disk by allocating contiguous
     space from this one large hole. This scheme effectively compacts all free space
     into one contiguous space, solving the fragmentation problem. The cost of this



                                        12.4             Allocation Methods           555
compaction is time, however, and the cost can be particularly high for large
hard disks. Compacting these disks may take hours and may be necessary on
a weekly basis. Some systems require that this function be done off-line, with
the file system unmounted. During this down time, normal system operation
generally cannot be permitted, so such compaction is avoided at all costs on
production machines. Most modern systems that need defragmentation can
perform it on-line during normal system operations, but the performance
penalty can be substantial.
Another problem with contiguous allocation is determining how much
space is needed for a file. When the file is created, the total amount of space
it will need must be found and allocated. How does the creator (program or
person) know the size of the file to be created? In some cases, this determination
may   be  fairly  simple  (copying  an  existing  file,  for  example).  In  general,
however, the size of an output file may be difficult to estimate.
If we allocate too little space to a file, we may find that the file cannot
be extended. Especially with a best-fit allocation strategy, the space on both
sides of the file may be in use. Hence, we cannot make the file larger in place.
Two possibilities then exist. First, the user program can be terminated, with
an appropriate error message. The user must then allocate more space and
run the program again. These repeated runs may be costly. To prevent them,
the user will normally overestimate the amount of space needed, resulting
in considerable wasted space. The other possibility is to find a larger hole,
copy the contents of the file to the new space, and release the previous space.
This series of actions can be repeated as long as space exists, although it can
be time consuming. The user need never be informed explicitly about what
is happening, however; the system continues despite the problem, although
more and more slowly.
Even if the total amount of space needed for a file is known in advance,
preallocation may be inefficient. A file that will grow slowly over a long period
(months or years) must be allocated enough space for its final size, even though
much of that space will be unused for a long time. The file therefore has a large
amount of internal fragmentation.
To minimize these drawbacks, some operating systems use a modified
contiguous-allocation scheme. Here, a contiguous chunk of space is allocated
initially. Then, if that amount proves not to be large enough, another chunk of
contiguous space, known as an extent, is added. The location of a file's blocks
is then recorded as a location and a block count, plus a link to the first block
of the next extent. On some systems, the owner of the file can set the extent
size, but this setting results in inefficiencies if the owner is incorrect. Internal
fragmentation can still be a problem if the extents are too large, and external
fragmentation can become a problem as extents of varying sizes are allocated
and deallocated. The commercial Veritas file system uses extents to optimize
performance. Veritas is a high-performance replacement for the standard UNIX
UFS.
12.4.2    Linked Allocation
Linked allocation solves all problems of contiguous allocation. With linked
allocation, each file is a linked list of disk blocks; the disk blocks may be
scattered anywhere on the disk. The directory contains a pointer to the first



556  Chapter 12  File-System Implementation
                                                   directory
                                             file           start  end
                                             jeep           9      25
                 0   1   1  2           3
                 4   5      6           7
                 8   9   1 10        2  11
                 12  13     14          15
                 16  17     18          19
                 20  21     22          23
                 24  25  -1 26          27
                 28  29     30          31
                 Figure 12.6            Linked allocation of disk space.
     and last blocks of the file. For example, a file of five blocks might start at block
     9 and continue at block 16, then block 1, then block 10, and finally block 25
     (Figure 12.6). Each block contains a pointer to the next block. These pointers
     are not made available to the user. Thus, if each block is 512 bytes in size, and
     a disk address (the pointer) requires 4 bytes, then the user sees blocks of 508
     bytes.
     To create a new file, we simply create a new entry in the directory. With
     linked allocation, each directory entry has a pointer to the first disk block of
     the file. This pointer is initialized to null (the end-of-list pointer value) to
     signify an empty file. The size field is also set to 0. A write to the file causes
     the free-space management system to find a free block, and this new block
     is written to and is linked to the end of the file. To read a file, we simply
     read blocks by following the pointers from block to block. There is no external
     fragmentation with linked allocation, and any free block on the free-space list
     can be used to satisfy a request. The size of a file need not be declared when the
     file is created. A file can continue to grow as long as free blocks are available.
     Consequently, it is never necessary to compact disk space.
     Linked allocation does have disadvantages, however. The major problem
     is that it can be used effectively only for sequential-access files. To find the
     ith block of a file, we must start at the beginning of that file and follow the
     pointers until we get to the ith block. Each access to a pointer requires a disk
     read, and some require a disk seek. Consequently, it is inefficient to support a
     direct-access capability for linked-allocation files.
     Another disadvantage is the space required for the pointers. If a pointer
     requires 4 bytes out of a 512-byte block, then 0.78 percent of the disk is being
     used for pointers, rather than for information. Each file requires slightly more
     space than it would otherwise.
     The usual solution to this problem is to collect blocks into multiples, called
     clusters, and to allocate clusters rather than blocks. For instance, the file system



                                              12.4     Allocation Methods             557
may define a cluster as four blocks and operate on the disk only in cluster
units. Pointers then use a much smaller percentage of the file's disk space.
This method allows the logical-to-physical block mapping to remain simple
but improves disk throughput (because fewer disk-head seeks are required)
and decreases the space needed for block allocation and free-list management.
The cost of this approach is an increase in internal fragmentation, because
more space is wasted when a cluster is partially full than when a block is
partially full. Clusters can be used to improve the disk-access time for many
other algorithms as well, so they are used in most file systems.
Yet another problem of linked allocation is reliability. Recall that the files
are linked together by pointers scattered all over the disk, and consider what
would happen if a pointer were lost or damaged. A bug in the operating-system
software or a disk hardware failure might result in picking up the wrong
pointer. This error could in turn result in linking into the free-space list or into
another file. One partial solution is to use doubly linked lists, and another is
to store the file name and relative block number in each block. However, these
schemes require even more overhead for each file.
An important variation on linked allocation is the use of a file-allocation
table (FAT). This simple but efficient method of disk-space allocation was used
by the MS-DOS operating system. A section of disk at the beginning of each
volume is set aside to contain the table. The table has one entry for each disk
block and is indexed by block number. The FAT is used in much the same
way as a linked list. The directory entry contains the block number of the
first block of the file. The table entry indexed by that block number contains
the block number of the next block in the file. This chain continues until it
reaches the last block, which has a special end-of-file value as the table entry.
An unused block is indicated by a table value of 0. Allocating a new block to
a file is a simple matter of finding the first 0-valued table entry and replacing
the previous end-of-file value with the address of the new block. The 0 is then
replaced with the end-of-file value. An illustrative example is the FAT structure
shown in Figure 12.7 for a file consisting of disk blocks 217, 618, and 339.
The FAT allocation scheme can result in a significant number of disk head
seeks, unless the FAT is cached. The disk head must move to the start of the
volume to read the FAT and find the location of the block in question, then
move to the location of the block itself. In the worst case, both moves occur for
each of the blocks. A benefit is that random-access time is improved, because
the disk head can find the location of any block by reading the information in
the FAT.
12.4.3    Indexed Allocation
Linked allocation solves the external-fragmentation and size-declaration prob-
lems  of  contiguous  allocation.  However,   in  the  absence    of  a  FAT,  linked
allocation cannot support efficient direct access, since the pointers to the blocks
are scattered with the blocks themselves all over the disk and must be retrieved
in order. Indexed allocation solves this problem by bringing all the pointers
together into one location: the index block.
Each file has its own index block, which is an array of disk-block addresses.
The ith entry in the index block points to the ith block of the file. The directory



558  Chapter 12  File-System Implementation
                 directory entry
                 test      ···           217
                 name                start block                  0
                                                            217      618
                                                            339
                                                            618      339
                                     number of disk blocks  ­1
                                                                     FAT
                                  Figure 12.7  File-allocation table.
     contains the address of the index block (Figure 12.8). To find and read the ith
     block, we use the pointer in the ith index-block entry. This scheme is similar to
     the paging scheme described in Section 8.5.
     When the file is created, all pointers in the index block are set to null.
     When the ith block is first written, a block is obtained from the free-space
     manager, and its address is put in the ith index-block entry.
     Indexed allocation supports direct access, without suffering from external
     fragmentation, because any free block on the disk can satisfy a request for more
     space. Indexed allocation does suffer from wasted space, however. The pointer
                                                                     directory
                                                            file     index block
                 0     1          2  3                      jeep          19
                 4     5          6  7
                 8     9   10        11
                                                                     9
                 12    13  14        15                              16
                                                                     1
                 16    17  18        19           19                 10
                                                                     25
                 20    21  22        23                              ­1
                                                                     ­1
                                                                     ­1
                 24    25  26        27
                 28    29  30        31
                       Figure 12.8       Indexed allocation of disk space.



                                               12.4  Allocation Methods             559
overhead of the index block is generally greater than the pointer overhead of
linked allocation. Consider a common case in which we have a file of only one
or two blocks. With linked allocation, we lose the space of only one pointer per
block. With indexed allocation, an entire index block must be allocated, even
if only one or two pointers will be non-null.
   This point raises the question of how large the index block should be. Every
file must have an index block, so we want the index block to be as small as
possible. If the index block is too small, however, it will not be able to hold
enough pointers for a large file, and a mechanism will have to be available to
deal with this issue. Mechanisms for this purpose include the following:
·  Linked scheme. An index block is normally one disk block. Thus, it can
   be read and written directly by itself. To allow for large files, we can link
   together several index blocks. For example, an index block might contain a
   small header giving the name of the file and a set of the first 100 disk-block
   addresses. The next address (the last word in the index block) is null (for
   a small file) or is a pointer to another index block (for a large file).
·  Multilevel index. A variant of linked representation uses a first-level index
   block to point to a set of second-level index blocks, which in turn point to
   the file blocks. To access a block, the operating system uses the first-level
   index to find a second-level index block and then uses that block to find the
   desired data block. This approach could be continued to a third or fourth
   level, depending on the desired maximum file size. With 4,096-byte blocks,
   we could store 1,024 four-byte pointers in an index block. Two levels of
   indexes allow 1,048,576 data blocks and a file size of up to 4 GB.
·  Combined scheme. Another alternative, used in UNIX-based file systems,
   is to keep the first, say, 15 pointers of the index block in the file's inode.
   The first 12 of these pointers point to direct blocks; that is, they contain
   addresses of blocks that contain data of the file. Thus, the data for small
   files (of no more than 12 blocks) do not need a separate index block. If the
   block size is 4 KB, then up to 48 KB of data can be accessed directly. The next
   three pointers point to indirect blocks. The first points to a single indirect
   block, which is an index block containing not data but the addresses of
   blocks that do contain data. The second points to a double indirect block,
   which contains the address of a block that contains the addresses of blocks
   that contain pointers to the actual data blocks. The last pointer contains
   the address of a triple indirect block. (A UNIX inode is shown in Figure
   12.9.)
   Under this method, the number of blocks that can be allocated to a file
   exceeds the amount of space addressable by the 4-byte file pointers used
   by many operating systems. A 32-bit file pointer reaches only 232 bytes,
   or 4 GB. Many UNIX and Linux implementations now support 64-bit file
   pointers, which allows files and file systems to be several exbibytes in size.
   The ZFS file system supports 128-bit file pointers.
   Indexed-allocation schemes suffer from some of the same performance
problems as does linked allocation. Specifically, the index blocks can be cached
in memory, but the data blocks may be spread all over a volume.



560  Chapter  12  File-System Implementation
                  mode
                  owners (2)
                  timestamps (3)
                                                    data
                  size block count
                                                    data
                                                    data
                                                    ·
                  direct blocks                     ·
                                    ·               ·
                                    ·               data
                                    ·
                                       ·            data              ·  data
                  single indirect      ·                              ·
                                       ·
                                                    data      ·       ·  data
                  double indirect
                                                              ·
                                                              ·
                  triple indirect                                     ·  data
                                                                      ·
                                                                      ·
                                                                         data
                                       Figure 12.9  The UNIX  inode.
     12.4.4   Performance
     The allocation methods that we have discussed vary in their storage efficiency
     and data-block access times. Both are important criteria in selecting the proper
     method or methods for an operating system to implement.
     Before selecting an allocation method, we need to determine how the
     systems will be used. A system with mostly sequential access should not use
     the same method as a system with mostly random access.
     For any type of access, contiguous allocation requires only one access to get
     a disk block. Since we can easily keep the initial address of the file in memory,
     we can calculate immediately the disk address of the ith block (or the next
     block) and read it directly.
     For linked allocation, we can also keep the address of the next block in
     memory and read it directly. This method is fine for sequential access; for
     direct access, however, an access to the ith block might require i disk reads. This
     problem indicates why linked allocation should not be used for an application
     requiring direct access.
     As a result, some systems support direct-access files by using contiguous
     allocation and sequential-access files by using linked allocation. For these
     systems, the type of access to be made must be declared when the file is created.
     A file created for sequential access will be linked and cannot be used for direct
     access. A file created for direct access will be contiguous and can support both
     direct access and sequential access, but its maximum length must be declared
     when it is created. In this case, the operating system must have appropriate
     data structures and algorithms to support both allocation methods. Files can be
     converted from one type to another by the creation of a new file of the desired
     type, into which the contents of the old file are copied. The old file may then
     be deleted and the new file renamed.



                                                12.5  Free-Space Management                    561
          Indexed allocation is more complex. If the index block is already in memory,
      then the access can be made directly. However, keeping the index block in
      memory requires considerable space. If this memory space is not available,
      then we may have to read first the index block and then the desired data
      block. For a two-level index, two index-block reads might be necessary. For an
      extremely large file, accessing a block near the end of the file would require
      reading in all the index blocks before the needed data block finally could
      be read. Thus, the performance of indexed allocation depends on the index
      structure, on the size of the file, and on the position of the block desired.
          Some systems combine contiguous allocation with indexed allocation by
      using contiguous allocation for small files (up to three or four blocks) and
      automatically switching to an indexed allocation if the file grows large. Since
      most files are small, and contiguous allocation is efficient for small files, average
      performance can be quite good.
          Many other optimizations are in use. Given the disparity between CPU
      speed   and  disk  speed,  it  is  not  unreasonable  to  add  thousands       of  extra
      instructions to the operating system to save just a few disk-head movements.
      Furthermore, this disparity is increasing over time, to the point where hundreds
      of  thousands  of  instructions    could  reasonably  be  used  to  optimize       head
      movements.
12.5  Free-Space Management
      Since disk space is limited, we need to reuse the space from deleted files for
      new files, if possible. (Write-once optical disks allow only one write to any
      given sector, and thus reuse is not physically possible.) To keep track of free
      disk space, the system maintains a free-space list. The free-space list records all
      free disk blocks--those not allocated to some file or directory. To create a file,
      we search the free-space list for the required amount of space and allocate that
      space to the new file. This space is then removed from the free-space list. When
      a file is deleted, its disk space is added to the free-space list. The free-space list,
      despite its name, may not be implemented as a list, as we discuss next.
      12.5.1  Bit Vector
      Frequently, the free-space list is implemented as a bit map or bit vector. Each
      block is represented by 1 bit. If the block is free, the bit is 1; if the block is
      allocated, the bit is 0.
          For example, consider a disk where blocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17,
      18, 25, 26, and 27 are free and the rest of the blocks are allocated. The free-space
      bit map would be
                         001111001111110001100000011100000 ...
          The main advantage of this approach is its relative simplicity and its
      efficiency in finding the first free block or n consecutive free blocks on the
      disk. Indeed, many computers supply bit-manipulation instructions that can
      be used effectively for that purpose. One technique for finding the first free
      block on a system that uses a bit-vector to allocate disk space is to sequentially
      check each word in the bit map to see whether that value is not 0, since a



562  Chapter 12  File-System Implementation
     0-valued word contains only 0 bits and represents a set of allocated blocks. The
     first non-0 word is scanned for the first 1 bit, which is the location of the first
     free block. The calculation of the block number is
     (number of bits per word) × (number of 0-value words) + offset of first 1 bit.
     Again, we see hardware features driving software functionality. Unfor-
     tunately, bit vectors are inefficient unless the entire vector is kept in main
     memory (and is written to disk occasionally for recovery needs). Keeping it in
     main memory is possible for smaller disks but not necessarily for larger ones.
     A 1.3-GB disk with 512-byte blocks would need a bit map of over 332 KB to
     track its free blocks, although clustering the blocks in groups of four reduces
     this number to around 83 KB per disk. A 1-TB disk with 4-KB blocks requires 256
     MB to store its bit map. Given that disk size constantly increases, the problem
     with bit vectors will continue to escalate as well.
     12.5.2  Linked List
     Another approach to free-space management is to link together all the free
     disk blocks, keeping a pointer to the first free block in a special location on the
     disk and caching it in memory. This first block contains a pointer to the next
     free disk block, and so on. Recall our earlier example (Section 12.5.1), in which
     blocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 18, 25, 26, and 27 were free and the
     rest of the blocks were allocated. In this situation, we would keep a pointer to
     block 2 as the first free block. Block 2 would contain a pointer to block 3, which
     would point to block 4, which would point to block 5, which would point to
     block 8, and so on (Figure 12.10). This scheme is not efficient; to traverse the
     list, we must read each block, which requires substantial I/O time. Fortunately,
                 free-space list  head
                                             0   1        2     3
                                             4   5        6     7
                                             8   9        10    11
                                             12  13       14    15
                                             16  17       18    19
                                             20  21       22    23
                                             24  25       26    27
                                             28  29       30    31
                 Figure 12.10     Linked  free-space      list  on disk.



                                        12.5     Free-Space Management               563
however, traversing the free list is not a frequent action. Usually, the operating
system simply needs a free block so that it can allocate that block to a file, so
the first block in the free list is used. The FAT method incorporates free-block
accounting into the allocation data structure. No separate method is needed.
12.5.3  Grouping
A modification of the free-list approach stores the addresses of n free blocks
in the first free block. The first n-1 of these blocks are actually free. The last
block contains the addresses of another n free blocks, and so on. The addresses
of a large number of free blocks can now be found quickly, unlike the situation
when the standard linked-list approach is used.
12.5.4  Counting
Another approach takes advantage of the fact that, generally, several contigu-
ous blocks may be allocated or freed simultaneously, particularly when space
is allocated with the contiguous-allocation algorithm or through clustering.
Thus, rather than keeping a list of n free disk addresses, we can keep the
address of the first free block and the number (n) of free contiguous blocks that
follow the first block. Each entry in the free-space list then consists of a disk
address and a count. Although each entry requires more space than would a
simple disk address, the overall list is shorter, as long as the count is generally
greater than 1. Note that this method of tracking free space is similar to the
extent method of allocating blocks. These entries can be stored in a balanced
tree, rather than a linked list, for efficient lookup, insertion, and deletion.
12.5.5  Space Maps
Oracle's ZFS file system (found in Solaris and other operating systems) was
designed to encompass huge numbers of files, directories, and even file systems
(in ZFS, we can create file-system hierarchies). On these scales, metadata I/O can
have a large performance impact. Consider, for example, that if the free-space
list is implemented as a bit map, bit maps must be modified both when blocks
are allocated and when they are freed. Freeing 1 GB of data on a 1-TB disk could
cause thousands of blocks of bit maps to be updated, because those data blocks
could be scattered over the entire disk. Clearly, the data structures for such a
system could be large and inefficient.
In its management of free space, ZFS uses a combination of techniques to
control the size of data structures and minimize the I/O needed to manage
those structures. First, ZFS creates metaslabs to divide the space on the device
into chunks of manageable size. A given volume may contain hundreds of
metaslabs. Each metaslab has an associated space map. ZFS uses the counting
algorithm to store information about free blocks. Rather than write counting
structures to disk, it uses log-structured file-system techniques to record them.
The space map is a log of all block activity (allocating and freeing), in time
order, in counting format. When ZFS decides to allocate or free space from a
metaslab, it loads the associated space map into memory in a balanced-tree
structure (for very efficient operation), indexed by offset, and replays the log
into that structure. The in-memory space map is then an accurate representation
of the allocated and free space in the metaslab. ZFS also condenses the map as



564   Chapter 12    File-System Implementation
      much as possible by combining contiguous free blocks into a single entry.
      Finally, the free-space list is updated on disk as part of the transaction-oriented
      operations of ZFS. During the collection and sorting phase, block requests can
      still occur, and ZFS satisfies these requests from the log. In essence, the log plus
      the balanced tree is the free list.
12.6  Efficiency and Performance
      Now     that  we  have  discussed    various  block-allocation  and  directory-
      management options, we can further consider their effect on performance
      and efficient disk use. Disks tend to represent a major bottleneck in system
      performance, since they are the slowest main computer component. In this
      section, we discuss a variety of techniques used to improve the efficiency and
      performance of secondary storage.
      12.6.1  Efficiency
      The efficient use of disk space depends heavily on the disk-allocation and
      directory algorithms in use. For instance, UNIX inodes are preallocated on
      a volume. Even an empty disk has a percentage of its space lost to inodes.
      However, by preallocating the inodes and spreading them across the volume,
      we improve the file system's performance. This improved performance results
      from the UNIX allocation and free-space algorithms, which try to keep a file's
      data blocks near that file's inode block to reduce seek time.
      As another example, let's reconsider the clustering scheme discussed in
      Section 12.4, which improves file-seek and file-transfer performance at the cost
      of internal fragmentation. To reduce this fragmentation, BSD UNIX varies the
      cluster size as a file grows. Large clusters are used where they can be filled, and
      small clusters are used for small files and the last cluster of a file. This system
      is described in Appendix A.
      The types of data normally kept in a file's directory (or inode) entry also
      require consideration. Commonly, a "last write date" is recorded to supply
      information to the user and to determine whether the file needs to be backed
      up. Some systems also keep a "last access date," so that a user can determine
      when the file was last read. The result of keeping this information is that,
      whenever the file is read, a field in the directory structure must be written
      to. That means the block must be read into memory, a section changed, and
      the block written back out to disk, because operations on disks occur only in
      block (or cluster) chunks. So any time a file is opened for reading, its directory
      entry must be read and written as well. This requirement can be inefficient for
      frequently accessed files, so we must weigh its benefit against its performance
      cost when designing a file system. Generally, every data item associated with
      a file needs to be considered for its effect on efficiency and performance.
      Consider, for instance, how efficiency is affected by the size of the pointers
      used to access data. Most systems use either 32-bit or 64-bit pointers throughout
      the operating system. Using 32-bit pointers limits the size of a file to 232, or 4
      GB. Using 64-bit pointers allows very large file sizes, but 64-bit pointers require



                     12.6                  Efficiency and Performance              565
more space to store. As a result, the allocation and free-space-management
methods (linked lists, indexes, and so on) use more disk space.
One of the difficulties in choosing a pointer size --or, indeed, any fixed
allocation size within an operating system--is planning for the effects of
changing technology. Consider that the IBM PC XT had a 10-MB hard drive
and an MS-DOS file system that could support only 32 MB. (Each FAT entry
was 12 bits, pointing to an 8-KB cluster.) As disk capacities increased, larger
disks had to be split into 32-MB partitions, because the file system could not
track blocks beyond 32 MB. As hard disks with capacities of over 100 MB became
common, the disk data structures and algorithms in MS-DOS had to be modified
to allow larger file systems. (Each FAT entry was expanded to 16 bits and later
to 32 bits.) The initial file-system decisions were made for efficiency reasons;
however, with the advent of MS-DOS Version 4, millions of computer users were
inconvenienced when they had to switch to the new, larger file system. Solaris'
ZFS file system uses 128-bit pointers, which theoretically should never need
to be extended. (The minimum mass of a device capable of storing 2128 bytes
using atomic-level storage would be about 272 trillion kilograms.)
As another example, consider the evolution of the Solaris operating system.
Originally, many data structures were of fixed length, allocated at system
startup. These structures included the process table and the open-file table.
When the process table became full, no more processes could be created. When
the file table became full, no more files could be opened. The system would fail
to provide services to users. Table sizes could be increased only by recompiling
the kernel and rebooting the system. With later releases of Solaris, almost all
kernel structures were allocated dynamically, eliminating these artificial limits
on system performance. Of course, the algorithms that manipulate these tables
are more complicated, and the operating system is a little slower because it
must dynamically allocate and deallocate table entries; but that price is the
usual one for more general functionality.
12.6.2  Performance
Even after the basic file-system algorithms have been selected, we can still
improve performance in several ways. As will be discussed in Chapter 13,
most disk controllers include local memory to form an on-board cache that is
large enough to store entire tracks at a time. Once a seek is performed, the
track is read into the disk cache starting at the sector under the disk head
(reducing latency time). The disk controller then transfers any sector requests
to the operating system. Once blocks make it from the disk controller into main
memory, the operating system may cache the blocks there.
Some systems maintain a separate section of main memory for a buffer
cache, where blocks are kept under the assumption that they will be used
again shortly. Other systems cache file data using a page cache. The page
cache uses virtual memory techniques to cache file data as pages rather than
as file-system-oriented blocks. Caching file data using virtual addresses is far
more efficient than caching through physical disk blocks, as accesses interface
with virtual memory rather than the file system. Several systems--including
Solaris, Linux, and Windows --use page caching to cache both process pages
and file data. This is known as unified virtual memory.



566  Chapter 12  File-System Implementation
                           memory-mapped I/O               I/O using
                                                       read( ) and write( )
                           page cache
                                             buffer cache
                                             file system
                           Figure 12.11  I/O without a unified buffer cache.
     Some versions of UNIX and Linux provide a unified buffer cache. To
     illustrate the benefits of the unified buffer cache, consider the two alternatives
     for opening and accessing a file. One approach is to use memory mapping
     (Section  9.7);  the  second  is    to  use  the  standard  system       calls  read()  and
     write(). Without a unified buffer cache, we have a situation similar to Figure
     12.11. Here, the read() and write() system calls go through the buffer cache.
     The memory-mapping call, however, requires using two caches--the page
     cache and the buffer cache. A memory mapping proceeds by reading in disk
     blocks from the file system and storing them in the buffer cache. Because the
     virtual memory system does not interface with the buffer cache, the contents
     of the file in the buffer cache must be copied into the page cache. This situation,
     known as double caching, requires caching file-system data twice. Not only
     does it waste memory but it also wastes significant CPU and I/O cycles due to
     the extra data movement within system memory. In addition, inconsistencies
     between the two caches can result in corrupt files. In contrast, when a unified
     buffer cache is provided, both memory mapping and the read() and write()
     system calls use the same page cache. This has the benefit of avoiding double
     caching, and it allows the virtual memory system to manage file-system data.
     The unified buffer cache is shown in Figure 12.12.
     Regardless of whether we are caching disk blocks or pages (or both), LRU
     (Section 9.4.4) seems a reasonable general-purpose algorithm for block or page
     replacement. However, the evolution of the Solaris page-caching algorithms
     reveals the difficulty in choosing an algorithm. Solaris allows processes and the
     page cache to share unused memory. Versions earlier than Solaris 2.5.1 made
     no distinction between allocating pages to a process and allocating them to
     the page cache. As a result, a system performing many I/O operations used
     most of the available memory for caching pages. Because of the high rates of
     I/O, the page scanner (Section 9.10.2) reclaimed pages from processes--rather
     than from the page cache--when free memory ran low. Solaris 2.6 and Solaris
     7 optionally implemented priority paging, in which the page scanner gives



                                        12.6  Efficiency and Performance           567
                     memory-mapped I/O           I/O using
                                              read( ) and write( )
                                   buffer cache
                                   file system
                     Figure 12.12  I/O using a unified buffer cache.
priority to process pages over the page cache. Solaris 8 applied a fixed limit to
process pages and the file-system page cache, preventing either from forcing
the other out of memory. Solaris 9 and 10 again changed the algorithms to
maximize memory use and minimize thrashing.
Another issue that can affect the performance of I/O is whether writes to
the file system occur synchronously or asynchronously. Synchronous writes
occur in the order in which the disk subsystem receives them, and the writes are
not buffered. Thus, the calling routine must wait for the data to reach the disk
drive before it can proceed. In an asynchronous write, the data are stored in
the cache, and control returns to the caller. Most writes are asynchronous.
However, metadata writes, among others, can be synchronous. Operating
systems frequently include a flag in the open system call to allow a process to
request that writes be performed synchronously. For example, databases use
this feature for atomic transactions, to assure that data reach stable storage in
the required order.
Some systems optimize their page cache by using different replacement
algorithms, depending on the access type of the file. A file being read or
written sequentially should not have its pages replaced in LRU order, because
the most recently used page will be used last, or perhaps never again. Instead,
sequential access can be optimized by techniques known as free-behind and
read-ahead. Free-behind removes a page from the buffer as soon as the next
page is requested. The previous pages are not likely to be used again and
waste buffer space. With read-ahead, a requested page and several subsequent
pages are read and cached. These pages are likely to be requested after the
current page is processed. Retrieving these data from the disk in one transfer
and caching them saves a considerable amount of time. One might think that
a track cache on the controller would eliminate the need for read-ahead on a
multiprogrammed system. However, because of the high latency and overhead
involved in making many small transfers from the track cache to main memory,
performing a read-ahead remains beneficial.
The page cache, the file system, and the disk drivers have some interesting
interactions. When data are written to a disk file, the pages are buffered in the
cache, and the disk driver sorts its output queue according to disk address.
These two actions allow the disk driver to minimize disk-head seeks and to



568   Chapter 12   File-System Implementation
      write data at times optimized for disk rotation. Unless synchronous writes are
      required, a process writing to disk simply writes into the cache, and the system
      asynchronously writes the data to disk when convenient. The user process sees
      very fast writes. When data are read from a disk file, the block I/O system does
      some read-ahead; however, writes are much more nearly asynchronous than
      are reads. Thus, output to the disk through the file system is often faster than
      is input for large transfers, counter to intuition.
12.7  Recovery
      Files and directories are kept both in main memory and on disk, and care must
      be taken to ensure that a system failure does not result in loss of data or in data
      inconsistency. We deal with these issues in this section. We also consider how
      a system can recover from such a failure.
      A system crash can cause inconsistencies among on-disk file-system data
      structures,  such  as  directory  structures,  free-block  pointers,  and  free         FCB
      pointers. Many file systems apply changes to these structures in place. A
      typical operation, such as creating a file, can involve many structural changes
      within the file system on the disk. Directory structures are modified, FCBs are
      allocated, data blocks are allocated, and the free counts for all of these blocks
      are decreased. These changes can be interrupted by a crash, and inconsistencies
      among the structures can result. For example, the free FCB count might indicate
      that an FCB had been allocated, but the directory structure might not point to
      the FCB. Compounding this problem is the caching that operating systems do
      to optimize I/O performance. Some changes may go directly to disk, while
      others may be cached. If the cached changes do not reach disk before a crash
      occurs, more corruption is possible.
      In addition to crashes, bugs in file-system implementation, disk controllers,
      and even user applications can corrupt a file system. File systems have varying
      methods to deal with corruption, depending on the file-system data structures
      and algorithms. We deal with these issues next.
      12.7.1  Consistency Checking
      Whatever the cause of corruption, a file system must first detect the problems
      and then correct them. For detection, a scan of all the metadata on each file
      system can confirm or deny the consistency of the system. Unfortunately, this
      scan can take minutes or hours and should occur every time the system boots.
      Alternatively, a file system can record its state within the file-system metadata.
      At the start of any metadata change, a status bit is set to indicate that the
      metadata is in flux. If all updates to the metadata complete successfully, the file
      system can clear that bit. If, however, the status bit remains set, a consistency
      checker is run.
      The consistency checker--a systems program such as fsck in UNIX --
      compares the data in the directory structure with the data blocks on disk
      and tries to fix any inconsistencies it finds. The allocation and free-space-
      management algorithms dictate what types of problems the checker can find
      and how successful it will be in fixing them. For instance, if linked allocation is
      used and there is a link from any block to its next block, then the entire file can be



                                                   12.7  Recovery                  569
reconstructed from the data blocks, and the directory structure can be recreated.
In contrast, the loss of a directory entry on an indexed allocation system can
be disastrous, because the data blocks have no knowledge of one another. For
this reason, UNIX caches directory entries for reads; but any write that results
in space allocation, or other metadata changes, is done synchronously, before
the corresponding data blocks are written. Of course, problems can still occur
if a synchronous write is interrupted by a crash.
12.7.2  Log-Structured File Systems
Computer scientists often find that algorithms and technologies originally
used in one area are equally useful in other areas. Such is the case with the
database log-based recovery algorithms. These logging algorithms have been
applied successfully to the problem of consistency checking. The resulting
implementations are known as log-based transaction-oriented (or journaling)
file systems.
Note that with the consistency-checking approach discussed in the pre-
ceding section, we essentially allow structures to break and repair them on
recovery. However, there are several problems with this approach. One is that
the inconsistency may be irreparable. The consistency check may not be able to
recover the structures, resulting in loss of files and even entire directories.
Consistency checking can require human intervention to resolve conflicts,
and that is inconvenient if no human is available. The system can remain
unavailable until the human tells it how to proceed. Consistency checking also
takes system and clock time. To check terabytes of data, hours of clock time
may be required.
The solution to this problem is to apply log-based recovery techniques to
file-system metadata updates. Both NTFS and the Veritas file system use this
method, and it is included in recent versions of UFS on Solaris. In fact, it is
becoming common on many operating systems.
Fundamentally, all metadata changes are written sequentially to a log.
Each set of operations for performing a specific task is a transaction. Once
the changes are written to this log, they are considered to be committed,
and the system call can return to the user process, allowing it to continue
execution. Meanwhile, these log entries are replayed across the actual file-
system structures. As the changes are made, a pointer is updated to indicate
which actions have completed and which are still incomplete. When an entire
committed transaction is completed, it is removed from the log file, which is
actually a circular buffer. A circular buffer writes to the end of its space and
then continues at the beginning, overwriting older values as it goes. We would
not want the buffer to write over data that had not yet been saved, so that
scenario is avoided. The log may be in a separate section of the file system or
even on a separate disk spindle. It is more efficient, but more complex, to have
it under separate read and write heads, thereby decreasing head contention
and seek times.
If the system crashes, the log file will contain zero or more transactions.
Any transactions it contains were not completed to the file system, even though
they were committed by the operating system, so they must now be completed.
The transactions can be executed from the pointer until the work is complete



570  Chapter 12  File-System Implementation
     so that the file-system structures remain consistent. The only problem occurs
     when a transaction was aborted--that is, was not committed before the system
     crashed. Any changes from such a transaction that were applied to the file
     system must be undone, again preserving the consistency of the file system.
     This recovery is all that is needed after a crash, eliminating any problems with
     consistency checking.
        A side benefit of using logging on disk metadata updates is that those
     updates proceed much faster than when they are applied directly to the on-
     disk data structures. The reason is found in the performance advantage of
     sequential I/O over random I/O. The costly synchronous random metadata
     writes are turned into much less costly synchronous sequential writes to the
     log-structured file system's logging area. Those changes, in turn, are replayed
     asynchronously via random writes to the appropriate structures. The overall
     result is a significant gain in performance of metadata-oriented operations,
     such as file creation and deletion.
     12.7.3  Other Solutions
     Another alternative to consistency checking is employed by Network Appli-
     ance's WAFL file system and the Solaris ZFS file system. These systems never
     overwrite blocks with new data. Rather, a transaction writes all data and meta-
     data changes to new blocks. When the transaction is complete, the metadata
     structures that pointed to the old versions of these blocks are updated to point
     to the new blocks. The file system can then remove the old pointers and the old
     blocks and make them available for reuse. If the old pointers and blocks are
     kept, a snapshot is created; the snapshot is a view of the file system before the
     last update took place. This solution should require no consistency checking if
     the pointer update is done atomically. WAFL does have a consistency checker,
     however, so some failure scenarios can still cause metadata corruption. (See
     Section 12.9 for details of the WAFL file system.)
        ZFS takes an even more innovative approach to disk consistency. It never
     overwrites blocks, just like WAFL. However, ZFS goes further and provides
     checksumming of all metadata and data blocks. This solution (when combined
     with RAID) assures that data are always correct. ZFS therefore has no consistency
     checker. (More details on ZFS are found in Section 10.7.6.)
     12.7.4  Backup and Restore
     Magnetic disks sometimes fail, and care must be taken to ensure that the data
     lost in such a failure are not lost forever. To this end, system programs can be
     used to back up data from disk to another storage device, such as a magnetic
     tape or other hard disk. Recovery from the loss of an individual file, or of an
     entire disk, may then be a matter of restoring the data from backup.
        To minimize the copying needed, we can use information from each file's
     directory entry. For instance, if the backup program knows when the last
     backup of a file was done, and the file's last write date in the directory indicates
     that the file has not changed since that date, then the file does not need to be
     copied again. A typical backup schedule may then be as follows:
     ·  Day 1. Copy to a backup medium all files from the disk. This is called a
        full backup.



                                                         12.8          NFS               571
      ·  Day 2. Copy to another medium all files changed since day 1. This is an
         incremental backup.
      ·  Day 3. Copy to another medium all files changed since day 2.
                                          .
                                          .
                                          .
      ·  Day N. Copy to another medium all files changed since day N- 1. Then
         go back to day 1.
         The new cycle can have its backup written over the previous set or onto a
      new set of backup media.
         Using this method, we can restore an entire disk by starting restores with
      the full backup and continuing through each of the incremental backups. Of
      course, the larger the value of N, the greater the number of media that must be
      read for a complete restore. An added advantage of this backup cycle is that
      we can restore any file accidentally deleted during the cycle by retrieving the
      deleted file from the backup of the previous day.
         The length of the cycle is a compromise between the amount of backup
      medium needed and the number of days covered by a restore. To decrease the
      number of tapes that must be read to do a restore, an option is to perform a
      full backup and then each day back up all files that have changed since the
      full backup. In this way, a restore can be done via the most recent incremental
      backup and the full backup, with no other incremental backups needed. The
      trade-off is that more files will be modified each day, so each successive
      incremental backup involves more files and more backup media.
         A user may notice that a particular file is missing or corrupted long after
      the damage was done. For this reason, we usually plan to take a full backup
      from time to time that will be saved "forever." It is a good idea to store these
      permanent backups far away from the regular backups to protect against
      hazard, such as a fire that destroys the computer and all the backups too.
      And if the backup cycle reuses media, we must take care not to reuse the
      media too many times--if the media wear out, it might not be possible to
      restore any data from the backups.
12.8  NFS
      Network file systems are commonplace. They are typically integrated with
      the overall directory structure and interface of the client system. NFS is a good
      example of a widely used, well implemented client­server network file system.
      Here, we use it as an example to explore the implementation details of network
      file systems.
         NFS is both an implementation and a specification of a software system for
      accessing remote files across LANs (or even WANs). NFS is part of ONC+, which
      most UNIX vendors and some PC operating systems support. The implementa-
      tion described here is part of the Solaris operating system, which is a modified
      version of UNIX SVR4. It uses either the TCP or UDP/IP protocol (depending on



572  Chapter 12  File-System Implementation
     U:                              S1:               S2:
                 usr                      usr                              usr
                              local            shared                           dir2
                                               dir1
                 Figure 12.13             Three independent file systems.
     the interconnecting network). The specification and the implementation are
     intertwined in our description of NFS. Whenever detail is needed, we refer to
     the Solaris implementation; whenever the description is general, it applies to
     the specification also.
     There are multiple versions of NFS, with the latest being Version 4. Here,
     we describe Version 3, as that is the one most commonly deployed.
     12.8.1  Overview
     NFS views a set of interconnected workstations as a set of independent machines
     with independent file systems. The goal is to allow some degree of sharing
     among these file systems (on explicit request) in a transparent manner. Sharing
     is based on a client­server relationship. A machine may be, and often is, both a
     client and a server. Sharing is allowed between any pair of machines. To ensure
     machine independence, sharing of a remote file system affects only the client
     machine and no other machine.
     So that a remote directory will be accessible in a transparent manner
     from a particular machine --say, from M1--a client of that machine must
     first carry out a mount operation. The semantics of the operation involve
     mounting a remote directory over a directory of a local file system. Once the
     mount operation is completed, the mounted directory looks like an integral
     subtree of the local file system, replacing the subtree descending from the
     local directory. The local directory becomes the name of the root of the newly
     mounted directory. Specification of the remote directory as an argument for the
     mount operation is not done transparently; the location (or host name) of the
     remote directory has to be provided. However, from then on, users on machine
     M1 can access files in the remote directory in a totally transparent manner.
     To illustrate file mounting, consider the file system depicted in Figure 12.13,
     where the triangles represent subtrees of directories that are of interest. The
     figure shows three independent file systems of machines named U, S1, and
     S2. At this point, on each machine, only the local files can be accessed. Figure
     12.14(a) shows the effects of mounting S1:/usr/shared over U:/usr/local.
     This figure depicts the view users on U have of their file system. After the
     mount is complete, they can access any file within the dir1 directory using the



                                                           12.8  NFS                 573
U:                                         U:
                       usr                     usr
                            local                   local
                                   dir1                    dir1
                       (a)                     (b)
Figure 12.14           Mounting in NFS. (a) Mounts. (b) Cascading mounts.
prefix /usr/local/dir1. The original directory /usr/local on that machine
is no longer visible.
Subject to access-rights accreditation, any file system, or any directory
within a file system, can be mounted remotely on top of any local directory.
Diskless workstations can even mount their own roots from servers. Cascading
mounts are also permitted in some NFS implementations. That is, a file system
can be mounted over another file system that is remotely mounted, not local. A
machine is affected by only those mounts that it has itself invoked. Mounting a
remote file system does not give the client access to other file systems that were,
by chance, mounted over the former file system. Thus, the mount mechanism
does not exhibit a transitivity property.
In Figure 12.14(b), we illustrate cascading mounts. The figure shows the
result of mounting S2:/usr/dir2 over U:/usr/local/dir1, which is already
remotely mounted from S1. Users can access files within dir2 on U using the
prefix /usr/local/dir1. If a shared file system is mounted over a user's home
directories on all machines in a network, the user can log into any workstation
and get their home environment. This property permits user mobility.
One of the design goals of NFS was to operate in a heterogeneous envi-
ronment of different machines, operating systems, and network architectures.
The NFS specification is independent of these media. This independence is
achieved through the use of RPC primitives built on top of an external data
representation (XDR) protocol used between two implementation-independent
interfaces. Hence, if the system's heterogeneous machines and file systems are
properly interfaced to NFS, file systems of different types can be mounted both
locally and remotely.
The NFS specification distinguishes between the services provided by a
mount mechanism and the actual remote-file-access services. Accordingly, two
separate protocols are specified for these services: a mount protocol and a
protocol for remote file accesses, the NFS protocol. The protocols are specified as
sets of RPCs. These RPCs are the building blocks used to implement transparent
remote file access.



574  Chapter 12  File-System Implementation
     12.8.2    The Mount Protocol
     The mount protocol establishes the initial logical connection between a server
     and a client. In Solaris, each machine has a server process, outside the kernel,
     performing the protocol functions.
          A  mount  operation  includes      the  name  of   the  remote   directory    to  be
     mounted and the name of the server machine storing it. The mount request
     is mapped to the corresponding RPC and is forwarded to the mount server
     running on the specific server machine. The server maintains an export list
     that specifies local file systems that it exports for mounting, along with names
     of machines that are permitted to mount them. (In Solaris, this list is the
     /etc/dfs/dfstab, which can be edited only by a superuser.) The specification
     can also include access rights, such as read only. To simplify the maintenance
     of export lists and mount tables, a distributed naming scheme can be used to
     hold this information and make it available to appropriate clients.
          Recall that any directory within an exported file system can be mounted
     remotely by an accredited machine. A component unit is such a directory. When
     the server receives a mount request that conforms to its export list, it returns to
     the client a file handle that serves as the key for further accesses to files within
     the mounted file system. The file handle contains all the information that the
     server needs to distinguish an individual file it stores. In UNIX terms, the file
     handle consists of a file-system identifier and an inode number to identify the
     exact mounted directory within the exported file system.
          The server also maintains a list of the client machines and the corresponding
     currently mounted directories. This list is used mainly for administrative
     purposes--for instance, for notifying all clients that the server is going down.
     Only through addition and deletion of entries in this list can the server state
     be affected by the mount protocol.
          Usually, a system has a static mounting preconfiguration that is established
     at boot time (/etc/vfstab in Solaris); however, this layout can be modified. In
     addition to the actual mount procedure, the mount protocol includes several
     other procedures, such as unmount and return export list.
     12.8.3    The NFS Protocol
     The  NFS  protocol  provides    a  set   of  RPCs  for  remote  file  operations.     The
     procedures support the following operations:
     ·    Searching for a file within a directory
     ·    Reading a set of directory entries
     ·    Manipulating links and directories
     ·    Accessing file attributes
     ·    Reading and writing files
     These procedures can be invoked only after a file handle for the remotely
     mounted directory has been established.
          The omission of open and close operations is intentional. A prominent
     feature of NFS servers is that they are stateless. Servers do not maintain
     information about their clients from one access to another. No parallels to



                                        12.8  NFS                                      575
UNIX's open-files table or file structures exist on the server side. Consequently,
each request has to provide a full set of arguments, including a unique file
identifier and an absolute offset inside the file for the appropriate operations.
The resulting design is robust; no special measures need be taken to recover
a server after a crash. File operations must be idempotent for this purpose,
that is, the same operation performed multiple times has the same effect as
if it were only performed once. To achieve idempotence, every NFS request
has a sequence number, allowing the server to determine if a request has been
duplicated or if any are missing.
Maintaining the list of clients that we mentioned seems to violate the
statelessness of the server. However, this list is not essential for the correct
operation of the client or the server, and hence it does not need to be restored
after a server crash. Consequently, it may include inconsistent data and is
treated as only a hint.
A further implication of the stateless-server philosophy and a result of the
synchrony of an RPC is that modified data (including indirection and status
blocks) must be committed to the server's disk before results are returned to
the client. That is, a client can cache write blocks, but when it flushes them to the
server, it assumes that they have reached the server's disks. The server must
write all NFS data synchronously. Thus, a server crash and recovery will be
invisible to a client; all blocks that the server is managing for the client will be
intact. The resulting performance penalty can be large, because the advantages
of caching are lost. Performance can be increased by using storage with its own
nonvolatile cache (usually battery-backed-up memory). The disk controller
acknowledges the disk write when the write is stored in the nonvolatile cache.
In essence, the host sees a very fast synchronous write. These blocks remain
intact even after a system crash and are written from this stable storage to disk
periodically.
A single NFS write procedure call is guaranteed to be atomic and is not
intermixed with other write calls to the same file. The NFS protocol, however,
does not provide concurrency-control mechanisms. A write() system call may
be broken down into several RPC writes, because each NFS write or read call
can contain up to 8 KB of data and UDP packets are limited to 1,500 bytes. As a
result, two users writing to the same remote file may get their data intermixed.
The claim is that, because lock management is inherently stateful, a service
outside the NFS should provide locking (and Solaris does). Users are advised
to coordinate access to shared files using mechanisms outside the scope of NFS.
NFS is integrated into the operating system via a VFS. As an illustration
of the architecture, let's trace how an operation on an already-open remote
file is handled (follow the example in Figure 12.15). The client initiates the
operation with a regular system call. The operating-system layer maps this
call to a VFS operation on the appropriate vnode. The VFS layer identifies the
file as a remote one and invokes the appropriate NFS procedure. An RPC call
is made to the NFS service layer at the remote server. This call is reinjected to
the VFS layer on the remote system, which finds that it is local and invokes
the appropriate file-system operation. This path is retraced to return the result.
An advantage of this architecture is that the client and the server are identical;
thus, a machine may be a client, or a server, or both. The actual service on each
server is performed by kernel threads.



576  Chapter 12    File-System Implementation
                     client                                                 server
                     system-calls interface
                     VFS interface                                      VFS interface
     other types of  UNIX file               NFS               NFS          UNIX file
     file systems    system                  client            server       system
                                             RPC/XDR           RPC/XDR
                     disk                                                   disk
                                                      network
                     Figure 12.15  Schematic view of the NFS architecture.
     12.8.4  Path-Name Translation
     Path-name translation in NFS involves the parsing of a path name such as
     /usr/local/dir1/file.txt into separate directory entries, or components:
     (1) usr, (2) local, and (3) dir1. Path-name translation is done by breaking the
     path into component names and performing a separate NFS lookup call for
     every pair of component name and directory vnode. Once a mount point is
     crossed, every component lookup causes a separate RPC to the server. This
     expensive path-name-traversal scheme is needed, since the layout of each
     client's logical name space is unique, dictated by the mounts the client has
     performed. It would be much more efficient to hand a server a path name
     and receive a target vnode once a mount point is encountered. At any point,
     however, there might be another mount point for the particular client of which
     the stateless server is unaware.
     So that lookup is fast, a directory-name-lookup cache on the client side
     holds the vnodes for remote directory names. This cache speeds up references
     to files with the same initial path name. The directory cache is discarded when
     attributes returned from the server do not match the attributes of the cached
     vnode.
     Recall that some implementations of NFS allow mounting a remote file
     system on top of another already-mounted remote file system (a cascading
     mount). When a client has a cascading mount, more than one server can be
     involved in a path-name traversal. However, when a client does a lookup on
     a directory on which the server has mounted a file system, the client sees the
     underlying directory instead of the mounted directory.



                                    12.9   Example: The WAFL File System                 577
      12.8.5  Remote Operations
      With the exception of opening and closing files, there is an almost one-to-one
      correspondence between the regular UNIX system calls for file operations and
      the NFS protocol RPCs. Thus, a remote file operation can be translated directly
      to the corresponding RPC. Conceptually, NFS adheres to the remote-service
      paradigm; but in practice, buffering and caching techniques are employed for
      the sake of performance. No direct correspondence exists between a remote
      operation and an RPC. Instead, file blocks and file attributes are fetched by the
      RPCs and are cached locally. Future remote operations use the cached data,
      subject to consistency constraints.
      There are two caches: the file-attribute (inode-information) cache and the
      file-blocks cache. When a file is opened, the kernel checks with the remote
      server to determine whether to fetch or revalidate the cached attributes. The
      cached file blocks are used only if the corresponding cached attributes are up
      to date. The attribute cache is updated whenever new attributes arrive from
      the server. Cached attributes are, by default, discarded after 60 seconds. Both
      read-ahead and delayed-write techniques are used between the server and the
      client. Clients do not free delayed-write blocks until the server confirms that
      the data have been written to disk. Delayed-write is retained even when a file
      is opened concurrently, in conflicting modes. Hence, UNIX semantics (Section
      11.5.3.1) are not preserved.
      Tuning the system for performance makes it difficult to characterize the
      consistency semantics of NFS. New files created on a machine may not be
      visible elsewhere for 30 seconds. Furthermore, writes to a file at one site may
      or may not be visible at other sites that have this file open for reading. New
      opens of a file observe only the changes that have already been flushed to the
      server. Thus, NFS provides neither strict emulation of UNIX semantics nor the
      session semantics of Andrew (Section 11.5.3.2). In spite of these drawbacks, the
      utility and good performance of the mechanism make it the most widely used
      multi-vendor-distributed system in operation.
12.9  Example: The WAFL File System
      Because disk I/O has such a huge impact on system performance, file-system
      design and implementation command quite a lot of attention from system
      designers. Some file systems are general purpose, in that they can provide
      reasonable performance and functionality for a wide variety of file sizes, file
      types, and I/O loads. Others are optimized for specific tasks in an attempt to
      provide better performance in those areas than general-purpose file systems.
      The write-anywhere file layout (WAFL) from Network Appliance is an example
      of this sort of optimization. WAFL is a powerful, elegant file system optimized
      for random writes.
      WAFL is used exclusively on network file servers produced by Network
      Appliance and is meant for use as a distributed file system. It can provide files
      to clients via the NFS, CIFS, ftp, and http protocols, although it was designed
      just for NFS and CIFS. When many clients use these protocols to talk to a file
      server, the server may see a very large demand for random reads and an even
      larger demand for random writes. The NFS and CIFS protocols cache data from
      read operations, so writes are of the greatest concern to file-server creators.



578  Chapter 12  File-System Implementation
     WAFL is used on file servers that include an NVRAM cache for writes.
     The WAFL designers took advantage of running on a specific architecture to
     optimize the file system for random I/O, with a stable-storage cache in front.
     Ease of use is one of the guiding principles of WAFL. Its creators also designed it
     to include a new snapshot functionality that creates multiple read-only copies
     of the file system at different points in time, as we shall see.
     The file system is similar to the Berkeley Fast File System, with many
     modifications. It is block-based and uses inodes to describe files. Each inode
     contains 16 pointers to blocks (or indirect blocks) belonging to the file described
     by the inode. Each file system has a root inode. All of the metadata lives in
     files. All inodes are in one file, the free-block map in another, and the free-inode
     map in a third, as shown in Figure 12.16. Because these are standard files, the
     data blocks are not limited in location and can be placed anywhere. If a file
     system is expanded by addition of disks, the lengths of the metadata files are
     automatically expanded by the file system.
     Thus, a WAFL file system is a tree of blocks with the root inode as its
     base. To take a snapshot, WAFL creates a copy of the root inode. Any file or
     metadata updates after that go to new blocks rather than overwriting their
     existing blocks. The new root inode points to metadata and data changed as a
     result of these writes. Meanwhile, the snapshot (the old root inode) still points
     to the old blocks, which have not been updated. It therefore provides access to
     the file system just as it was at the instant the snapshot was made --and takes
     very little disk space to do so. In essence, the extra disk space occupied by a
     snapshot consists of just the blocks that have been modified since the snapshot
     was taken.
     An important change from more standard file systems is that the free-block
     map has more than one bit per block. It is a bitmap with a bit set for each
     snapshot that is using the block. When all snapshots that have been using the
     block are deleted, the bit map for that block is all zeros, and the block is free to
     be reused. Used blocks are never overwritten, so writes are very fast, because
     a write can occur at the free block nearest the current head location. There are
     many other performance optimizations in WAFL as well.
     Many snapshots can exist simultaneously, so one can be taken each hour
     of the day and each day of the month. A user with access to these snapshots
     can access files as they were at any of the times the snapshots were taken.
     The snapshot facility is also useful for backups, testing, versioning, and so on.
                     root inode      ···
                     inode file                  ···
     free block map  free inode map          file in the file system...  ···
                     Figure 12.16    The WAFL file layout.



                                      12.9     Example: The WAFL File System        579
                          root inode
                          block A           B  C     D     E
                          (a) Before a snapshot.
                          root inode           new snapshot
                          block A        B     C     D     E
(b) After a snapshot, before any blocks change.
                          root inode        new snapshot
                          block A     B     C     D     E     D´
                          (c) After block D has changed to D´.
                          Figure 12.17      Snapshots in WAFL.
WAFL's snapshot facility is very efficient in that it does not even require that
copy-on-write copies of each data block be taken before the block is modified.
Other file systems provide snapshots, but frequently with less efficiency. WAFL
snapshots are depicted in Figure 12.17.
Newer versions of WAFL actually allow read ­write snapshots, known as
clones. Clones are also efficient, using the same techniques as shapshots. In
this case, a read-only snapshot captures the state of the file system, and a clone
refers back to that read-only snapshot. Any writes to the clone are stored in
new blocks, and the clone's pointers are updated to refer to the new blocks.
The original snapshot is unmodified, still giving a view into the file system as
it was before the clone was updated. Clones can also be promoted to replace
the original file system; this involves throwing out all of the old pointers and
any associated old blocks. Clones are useful for testing and upgrades, as the
original version is left untouched and the clone deleted when the test is done
or if the upgrade fails.
Another feature that naturally results from the WAFL file system implemen-
tation is replication, the duplication and synchronization of a set of data over a
network to another system. First, a snapshot of a WAFL file system is duplicated
to another system. When another snapshot is taken on the source system, it
is relatively easy to update the remote system just by sending over all blocks
contained in the new snapshot. These blocks are the ones that have changed



580  Chapter 12   File-System Implementation
     between the times the two snapshots were taken. The remote system adds these
     blocks to the file system and updates its pointers, and the new system then is a
     duplicate of the source system as of the time of the second snapshot. Repeating
     this process maintains the remote system as a nearly up-to-date copy of the first
     system. Such replication is used for disaster recovery. Should the first system
     be destroyed, most of its data are available for use on the remote system.
     Finally, we should note that the ZFS file system supports similarly efficient
     snapshots, clones, and replication.
12.10 Summary
     The file system resides permanently on secondary storage, which is designed to
     hold a large amount of data permanently. The most common secondary-storage
     medium is the disk.
     Physical disks may be segmented into partitions to control media use
     and to allow multiple, possibly varying, file systems on a single spindle.
     These file systems are mounted onto a logical file system architecture to make
     them available for use. File systems are often implemented in a layered or
     modular structure. The lower levels deal with the physical properties of storage
     devices. Upper levels deal with symbolic file names and logical properties of
     files. Intermediate levels map the logical file concepts into physical device
     properties.
     Any file-system type can have different structures and algorithms. A VFS
     layer allows the upper layers to deal with each file-system type uniformly. Even
     remote file systems can be integrated into the system's directory structure and
     acted on by standard system calls via the VFS interface.
     The various files can be allocated space on the disk in three ways: through
     contiguous, linked, or indexed allocation. Contiguous allocation can suffer
     from   external  fragmentation.  Direct  access  is  very  inefficient  with  linked
     allocation. Indexed allocation may require substantial overhead for its index
     block. These algorithms can be optimized in many ways. Contiguous space
     can be enlarged through extents to increase flexibility and to decrease external
     fragmentation. Indexed allocation can be done in clusters of multiple blocks
     to increase throughput and to reduce the number of index entries needed.
     Indexing in large clusters is similar to contiguous allocation with extents.
     Free-space allocation methods also influence the efficiency of disk-space
     use, the performance of the file system, and the reliability of secondary storage.
     The methods used include bit vectors and linked lists. Optimizations include
     grouping, counting, and the FAT, which places the linked list in one contiguous
     area.
     Directory-management routines must consider efficiency, performance,
     and reliability. A hash table is a commonly used method, as it is fast and
     efficient. Unfortunately, damage to the table or a system crash can result
     in inconsistency between the directory information and the disk's contents.
     A consistency checker can be used to repair the damage. Operating-system
     backup tools allow disk data to be copied to tape, enabling the user to recover
     from data or even disk loss due to hardware failure, operating system bug, or
     user error.



                                                                Practice Exercises       581
Network file systems, such as NFS, use client­server methodology to
allow users to access files and directories from remote machines as if they
were  on  local  file  systems.       System  calls  on  the    client  are  translated  into
network protocols and retranslated into file-system operations on the server.
Networking and multiple-client access create challenges in the areas of data
consistency and performance.
Due to the fundamental role that file systems play in system operation,
their performance and reliability are crucial. Techniques such as log structures
and caching help improve performance, while log structures and RAID improve
reliability. The WAFL file system is an example of optimization of performance
to match a specific I/O load.
Practice Exercises
12.1  Consider a file currently consisting of 100 blocks. Assume that the file-
      control block (and the index block, in the case of indexed allocation)
      is already in memory. Calculate how many disk I/O operations are
      required for contiguous, linked, and indexed (single-level) allocation
      strategies,      if,  for  one  block,  the    following  conditions   hold.  In   the
      contiguous-allocation case, assume that there is no room to grow at
      the beginning but there is room to grow at the end. Also assume that
      the block information to be added is stored in memory.
          a.  The block is added at the beginning.
          b.  The block is added in the middle.
          c.  The block is added at the end.
          d.  The block is removed from the beginning.
          e.  The block is removed from the middle.
          f.  The block is removed from the end.
12.2  What problems could occur if a system allowed a file system to be
      mounted simultaneously at more than one location?
12.3  Why must the bit map for file allocation be kept on mass storage, rather
      than in main memory?
12.4  Consider a system that supports the strategies of contiguous, linked,
      and indexed allocation. What criteria should be used in deciding which
      strategy is best utilized for a particular file?
12.5  One problem with contiguous allocation is that the user must preallo-
      cate enough space for each file. If the file grows to be larger than the
      space allocated for it, special actions must be taken. One solution to this
      problem is to define a file structure consisting of an initial contiguous
      area (of a specified size). If this area is filled, the operating system
      automatically defines an overflow area that is linked to the initial
      contiguous area. If the overflow area is filled, another overflow area
      is allocated. Compare this implementation of a file with the standard
      contiguous and linked implementations.



582  Chapter 12  File-System Implementation
     12.6   How do caches help improve performance? Why do systems not use
            more or larger caches if they are so useful?
     12.7   Why is it advantageous to the user for an operating system to dynami-
            cally allocate its internal tables? What are the penalties to the operating
            system for doing so?
     12.8   Explain how the VFS layer allows an operating system to support
            multiple types of file systems easily.
Exercises
     12.9   Consider  a  file  system  that  uses   a    modifed   contiguous-allocation
            scheme with support for extents. A file is a collection of extents, with
            each extent corresponding to a contiguous set of blocks. A key issue in
            such systems is the degree of variability in the size of the extents. What
            are the advantages and disadvantages of the following schemes?
            a.   All extents are of the same size, and the size is predetermined.
            b.   Extents can be of any size and are allocated dynamically.
            c.   Extents can be of a few fixed sizes, and these sizes are predeter-
                 mined.
     12.10  Contrast the performance of the three techniques for allocating disk
            blocks  (contiguous,  linked,    and    indexed)  for  both  sequential  and
            random file access.
     12.11  What are the advantages of the variant of linked allocation that uses a
            FAT to chain together the blocks of a file?
     12.12  Consider a system where free space is kept in a free-space list.
            a.   Suppose that the pointer to the free-space list is lost. Can the
                 system reconstruct the free-space list? Explain your answer.
            b.   Consider a file system similar to the one used by UNIX with
                 indexed allocation. How many disk I/O operations might be
                 required to read the contents of a small local file at /a/b/c?
                 Assume that none of the disk blocks is currently being cached.
            c.   Suggest a scheme to ensure that the pointer is never lost as a result
                 of memory failure.
     12.13  Some file systems allow disk storage to be allocated at different levels
            of granularity. For instance, a file system could allocate 4 KB of disk
            space as a single 4-KB block or as eight 512-byte blocks. How could
            we take advantage of this flexibility to improve performance? What
            modifications would have to be made to the free-space management
            scheme in order to support this feature?
     12.14  Discuss how performance optimizations for file systems might result
            in difficulties in maintaining the consistency of the systems in the event
            of computer crashes.



                                               Programming Problems                 583
12.15  Consider a file system on a disk that has both logical and physical
       block sizes of 512 bytes. Assume that the information about each
       file is already in memory. For each of the three allocation strategies
       (contiguous, linked, and indexed), answer these questions:
       a.  How is the logical-to-physical address mapping accomplished
           in this system? (For the indexed allocation, assume that a file is
           always less than 512 blocks long.)
       b.  If we are currently at logical block 10 (the last block accessed was
           block 10) and want to access logical block 4, how many physical
           blocks must be read from the disk?
12.16  Consider a file system that uses inodes to represent files. Disk blocks
       are 8 KB in size, and a pointer to a disk block requires 4 bytes. This file
       system has 12 direct disk blocks, as well as single, double, and triple
       indirect disk blocks. What is the maximum size of a file that can be
       stored in this file system?
12.17  Fragmentation on a storage device can be eliminated by recompaction
       of the information. Typical disk devices do not have relocation or base
       registers (such as those used when memory is to be compacted), so
       how can we relocate files? Give three reasons why recompacting and
       relocation of files are often avoided.
12.18  Assume  that  in  a  particular    augmentation  of  a  remote-file-access
       protocol, each client maintains a name cache that caches translations
       from file names to corresponding file handles. What issues should we
       take into account in implementing the name cache?
12.19  Explain why logging metadata       updates  ensures     recovery  of  a      file
       system after a file-system crash.
12.20  Consider the following backup scheme:
       ·   Day 1. Copy to a backup medium all files from the disk.
       ·   Day 2. Copy to another medium all files changed since day 1.
       ·   Day 3. Copy to another medium all files changed since day 1.
       This differs from the schedule given in Section 12.7.4 by having all
       subsequent backups copy all files modified since the first full backup.
       What are the benefits of this system over the one in Section 12.7.4?
       What are the drawbacks? Are restore operations made easier or more
       difficult? Explain your answer.
Programming Problems
       The following exercise examines the relationship between files and
       inodes on a UNIX or Linux system. On these systems, files are repre-
       sented with inodes. That is, an inode is a file (and vice versa). You can
       complete this exercise on the Linux virtual machine that is provided
       with this text. You can also complete the exercise on any Linux, UNIX, or



584  Chapter 12  File-System Implementation
            Mac OS X system, but it will require creating two simple text files named
            file1.txt and file3.txt whose contents are unique sentences.
     12.21  In   the  source  code  available  with  this  text,  open  file1.txt  and
            examine its contents. Next, obtain the inode number of this file with
            the command
                 ls -li file1.txt
            This will produce output similar to the following:
                 16980 -rw-r--r-- 2 os os 22 Sep 14 16:13 file1.txt
            where the inode number is boldfaced. (The inode number of file1.txt
            is likely to be different on your system.)
            The UNIX ln command creates a link between a source and target file.
            This command works as follows:
                 ln    [-s]   <source  file>     <target   file>
                UNIX provides two types of links: (1) hard links and (2) soft links.
            A hard link creates a separate target file that has the same inode as the
            source file. Enter the following command to create a hard link between
            file1.txt and file2.txt:
                 ln file1.txt file2.txt
            What are the inode values of file1.txt and file2.txt? Are they
            the same or different? Do the two files have the same --or different--
            contents?
            Next, edit file2.txt and change its contents. After you have done
            so, examine the contents of file1.txt. Are the contents of file1.txt
            and file2.txt the same or different?
            Next, enter the following command which removes file1.txt:
                 rm file1.txt
            Does file2.txt still exist as well?
            Now examine the man pages for both the rm and unlink commands.
            Afterwards, remove file2.txt by entering the command
                 strace       rm  file2.txt
            The strace command traces the execution of system calls as the
            command rm        file2.txt is run. What system call is used for removing
            file2.txt?
                A soft link (or symbolic link) creates a new file that "points" to the
            name of the file it is linking to. In the source code available with this text,
            create a soft link to file3.txt by entering the following command:
                 ln    -s  file3.txt   file4.txt
            After you have done so, obtain the inode numbers of file3.txt and
            file4.txt using the command
                 ls -li file*.txt



                                                                       Bibliography             585
          Are the inodes the same, or is each unique? Next, edit the contents
          of file4.txt. Have the contents of file3.txt been altered as well?
          Last, delete file3.txt. After you have done so, explain what happens
          when you attempt to edit file4.txt.
Bibliographical Notes
The  MS-DOS       FAT   system   is    explained     in  [Norton  and        Wilton  (1988)].   The
internals     of  the   BSD  UNIX      system   are  covered  in       full  in  [McKusick      and
Neville-Neil (2005)]. Details concerning file systems for Linux can be found in
[Love (2010)]. The Google file system is described in [Ghemawat et al. (2003)].
FUSE can be found at http://fuse.sourceforge.net.
     Log-structured file organizations for enhancing both performance and
consistency are discussed in [Rosenblum and Ousterhout (1991)], [Seltzer
et  al.   (1993)],  and      [Seltzer  et  al.  (1995)].     Algorithms      such    as  balanced
trees (and much more) are covered by [Knuth (1998)] and [Cormen et al.
(2009)].   [Silvers     (2000)]  discusses      implementing      the        page    cache  in  the
NetBSD operating system. The ZFS source code for space maps can be found at
http://src.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/
fs/zfs/space map.c.
     The network file system (NFS) is discussed in [Callaghan (2000)]. NFS Ver-
sion 4 is a standard described at http://www.ietf.org/rfc/rfc3530.txt. [Ouster-
hout (1991)] discusses the role of distributed state in networked file systems.
Log-structured designs for networked file systems are proposed in [Hartman
and Ousterhout (1995)] and [Thekkath et al. (1997)]. NFS and the UNIX file
system (UFS) are described in [Vahalia (1996)] and [Mauro and McDougall
(2007)]. The NTFS file system is explained in [Solomon (1998)]. The Ext3 file
system used in Linux is described in [Mauerer (2008)] and the WAFL file
system is covered in [Hitz et al. (1995)]. ZFS documentation can be found
at http://www.opensolaris.org/os/community/ZFS/docs.
Bibliography
[Callaghan (2000)]       B. Callaghan, NFS Illustrated, Addison-Wesley (2000).
[Cormen et al. (2009)]       T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,
    Introduction to Algorithms, Third Edition, MIT Press (2009).
[Ghemawat et al. (2003)]         S.    Ghemawat,         H.  Gobioff,  and   S.-T.   Leung,     "The
    Google File System", Proceedings of the ACM Symposium on Operating Systems
    Principles (2003).
[Hartman and Ousterhout (1995)]                J. H. Hartman and J. K. Ousterhout, "The
    Zebra Striped Network File System", ACM Transactions on Computer Systems,
    Volume 13, Number 3 (1995), pages 274­310.
[Hitz et al. (1995)]     D. Hitz, J. Lau, and M. Malcolm, "File System Design for an
    NFS File Server Appliance", Technical report, NetApp (1995).



586  Chapter 12     File-System Implementation
     [Knuth (1998)]    D. E. Knuth, The Art of Computer Programming, Volume 3: Sorting
     and Searching, Second Edition, Addison-Wesley (1998).
     [Love (2010)]     R. Love, Linux Kernel Development, Third Edition, Developer's
     Library (2010).
     [Mauerer (2008)]         W. Mauerer, Professional Linux Kernel Architecture, John Wiley
     and Sons (2008).
     [Mauro and McDougall (2007)]            J. Mauro and R. McDougall, Solaris Internals:
     Core Kernel Architecture, Prentice Hall (2007).
     [McKusick and Neville-Neil (2005)]            M. K. McKusick and G. V. Neville-Neil,
     The Design and Implementation of the FreeBSD UNIX Operating System, Addison
     Wesley (2005).
     [Norton and Wilton (1988)]          P.  Norton    and     R.  Wilton,  The  New    Peter  Norton
     Programmer's Guide to the IBM PC & PS/2, Microsoft Press (1988).
     [Ousterhout (1991)]       J. Ousterhout.      "The Role of Distributed State".         In CMU
     Computer Science: a 25th Anniversary Commemorative, R. F. Rashid, Ed., Addison-
     Wesley (1991).
     [Rosenblum and Ousterhout (1991)]             M. Rosenblum and J. K. Ousterhout, "The
     Design and Implementation of a Log-Structured File System", Proceedings of the
     ACM Symposium on Operating Systems Principles (1991), pages 1­15.
     [Seltzer et al. (1993)]   M. I. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin, "An
     Implementation of a Log-Structured File System for UNIX", USENIX Winter
     (1993), pages 307­326.
     [Seltzer et al. (1995)]   M.  I.    Seltzer,  K.  A.  Smith,   H.   Balakrishnan,  J.     Chang,
     S. McMains, and V. N. Padmanabhan, "File System Logging Versus Clustering:
     A Performance Comparison", USENIX Winter (1995), pages 249­264.
     [Silvers (2000)]  C. Silvers, "UBC: An Efficient Unified I/O and Memory Caching
     Subsystem for NetBSD", USENIX Annual Technical Conference -- FREENIX Track
     (2000).
     [Solomon (1998)]         D. A. Solomon, Inside Windows NT, Second Edition, Microsoft
     Press (1998).
     [Thekkath et al. (1997)]      C. A. Thekkath, T. Mann, and E. K. Lee, "Frangipani:
     A Scalable Distributed File System", Symposium on Operating Systems Principles
     (1997), pages 224­237.
     [Vahalia (1996)]     U.   Vahalia,  Unix      Internals:  The  New     Frontiers,  Prentice  Hall
     (1996).
