Virtual Memory


Virtual                                                          9C H A P T E R
Memory
     In Chapter 8, we discussed various memory-management strategies used in
     computer systems. All these strategies have the same goal: to keep many
     processes in memory simultaneously to allow multiprogramming. However,
     they tend to require that an entire process be in memory before it can execute.
        Virtual memory is a technique that allows the execution of processes
     that are not completely in memory. One major advantage of this scheme is
     that programs can be larger than physical memory. Further, virtual memory
     abstracts main memory into an extremely large, uniform array of storage,
     separating logical memory as viewed by the user from physical memory.
     This  technique  frees  programmers   from  the  concerns   of  memory-storage
     limitations. Virtual memory also allows processes to share files easily and
     to implement shared memory. In addition, it provides an efficient mechanism
     for process creation. Virtual memory is not easy to implement, however, and
     may substantially decrease performance if it is used carelessly. In this chapter,
     we discuss virtual memory in the form of demand paging and examine its
     complexity and cost.
     CHAPTER OBJECTIVES
     ·     To describe the benefits of a virtual memory system.
     ·     To explain the concepts of demand paging, page-replacement algorithms,
           and allocation of page frames.
     ·     To discuss the principles of the working-set model.
     ·     To examine the relationship between shared memory and memory-mapped
           files.
     ·     To explore how kernel memory is managed.
9.1  Background
     The memory-management algorithms outlined in Chapter 8 are necessary
     because of one basic requirement: The instructions being executed must be
                                                                                        397



398  Chapter 9  Virtual Memory
     in physical memory. The first approach to meeting this requirement is to place
     the entire logical address space in physical memory. Dynamic loading can help
     to ease this restriction, but it generally requires special precautions and extra
     work by the programmer.
        The  requirement    that  instructions  must  be   in  physical  memory   to    be
     executed seems both necessary and reasonable; but it is also unfortunate, since
     it limits the size of a program to the size of physical memory. In fact, an
     examination of real programs shows us that, in many cases, the entire program
     is not needed. For instance, consider the following:
     ·  Programs often have code to handle unusual error conditions. Since these
        errors seldom, if ever, occur in practice, this code is almost never executed.
     ·  Arrays, lists, and tables are often allocated more memory than they actually
        need. An array may be declared 100 by 100 elements, even though it is
        seldom larger than 10 by 10 elements. An assembler symbol table may
        have room for 3,000 symbols, although the average program has less than
        200 symbols.
     ·  Certain options and features of a program may be used rarely. For instance,
        the routines on U.S. government computers that balance the budget have
        not been used in many years.
     Even in those cases where the entire program is needed, it may not all be
     needed at the same time.
        The ability to execute a program that is only partially in memory would
     confer many benefits:
     ·  A program would no longer be constrained by the amount of physical
        memory that is available. Users would be able to write programs for an
        extremely large virtual address space, simplifying the programming task.
     ·  Because  each  user    program  could   take  less     physical  memory,  more
        programs could be run at the same time, with a corresponding increase in
        CPU utilization and throughput but with no increase in response time or
        turnaround time.
     ·  Less I/O would be needed to load or swap user programs into memory, so
        each user program would run faster.
     Thus, running a program that is not entirely in memory would benefit both
     the system and the user.
        Virtual memory involves the separation of logical memory as perceived
     by users from physical memory. This separation allows an extremely large
     virtual memory to be provided for programmers when only a smaller physical
     memory is available (Figure 9.1). Virtual memory makes the task of program-
     ming much easier, because the programmer no longer needs to worry about
     the amount of physical memory available; she can concentrate instead on the
     problem to be programmed.
        The virtual address space of a process refers to the logical (or virtual) view
     of how a process is stored in memory. Typically, this view is that a process
     begins at a certain logical address--say, address 0--and exists in contiguous
     memory, as shown in Figure 9.2. Recall from Chapter 8, though, that in fact



                                                9.1     Background                     399
         page 0
         page 1
         page 2
         ·
         ·
         ·
                  memory
                  map
         page v                physical
         virtual               memory
         memory
Figure 9.1        Diagram showing virtual memory that is larger than physical memory.
physical memory may be organized in page frames and that the physical page
frames assigned to a process may not be contiguous. It is up to the memory-
management unit (MMU) to map logical pages to physical page frames in
memory.
Note in Figure 9.2 that we allow the heap to grow upward in memory as
it is used for dynamic memory allocation. Similarly, we allow for the stack to
                          Max
                               stack
                               heap
                               data
                               code
                          0
                  Figure 9.2   Virtual address  space.



400  Chapter 9  Virtual Memory
                stack                                                  stack
                shared library           shared                shared library
                                         pages
                heap                                                   heap
                data                                                   data
                code                                                   code
                     Figure 9.3  Shared library using virtual memory.
     grow downward in memory through successive function calls. The large blank
     space (or hole) between the heap and the stack is part of the virtual address
     space but will require actual physical pages only if the heap or stack grows.
     Virtual address spaces that include holes are known as sparse address spaces.
     Using a sparse address space is beneficial because the holes can be filled as the
     stack or heap segments grow or if we wish to dynamically link libraries (or
     possibly other shared objects) during program execution.
        In addition to separating logical memory from physical memory, virtual
     memory allows files and memory to be shared by two or more processes
     through page sharing (Section 8.5.4). This leads to the following benefits:
     ·  System libraries can be shared by several processes through mapping of the
        shared object into a virtual address space. Although each process considers
        the libraries to be part of its virtual address space, the actual pages where
        the libraries reside in physical memory are shared by all the processes
        (Figure 9.3). Typically, a library is mapped read-only into the space of each
        process that is linked with it.
     ·  Similarly, processes can share memory. Recall from Chapter 3 that two
        or more processes can communicate through the use of shared memory.
        Virtual memory allows one process to create a region of memory that it can
        share with another process. Processes sharing this region consider it part
        of their virtual address space, yet the actual physical pages of memory are
        shared, much as is illustrated in Figure 9.3.
     ·  Pages can be shared during process creation with the fork() system call,
        thus speeding up process creation.
     We further explore these--and other--benefits of virtual memory later in
     this chapter. First, though, we discuss implementing virtual memory through
     demand paging.



                                                      9.2  Demand Paging               401
9.2  Demand Paging
     Consider how an executable program might be loaded from disk into memory.
     One option is to load the entire program in physical memory at program
     execution time. However, a problem with this approach is that we may not
     initially need the entire program in memory. Suppose a program starts with
     a list of available options from which the user is to select. Loading the entire
     program into memory results in loading the executable code for all options,
     regardless of whether or not an option is ultimately selected by the user. An
     alternative strategy is to load pages only as they are needed. This technique is
     known as demand paging and is commonly used in virtual memory systems.
     With demand-paged virtual memory, pages are loaded only when they are
     demanded during program execution. Pages that are never accessed are thus
     never loaded into physical memory.
     A demand-paging system is similar to a paging system with swapping
     (Figure 9.4) where processes reside in secondary memory (usually a disk).
     When we want to execute a process, we swap it into memory. Rather than
     swapping the entire process into memory, though, we use a lazy swapper.
     A lazy swapper never swaps a page into memory unless that page will be
     needed. In the context of a demand-paging system, use of the term "swapper"
     is technically incorrect. A swapper manipulates entire processes, whereas a
     pager is concerned with the individual pages of a process. We thus use "pager,"
     rather than "swapper," in connection with demand paging.
     program                      swap out            0    1      2     3
     A                                                4    5      6     7
                                                      8    9      10    11
                                                      12   13     14    15
     program                                          16   17     18    19
     B                                   swap in
                                                      20   21     22    23
                 main
                 memory
     Figure 9.4  Transfer  of  a  paged memory    to  contiguous  disk  space.



402  Chapter 9  Virtual Memory
     9.2.1     Basic Concepts
     When a process is to be swapped in, the pager guesses which pages will be
     used before the process is swapped out again. Instead of swapping in a whole
     process, the pager brings only those pages into memory. Thus, it avoids reading
     into memory pages that will not be used anyway, decreasing the swap time
     and the amount of physical memory needed.
     With this scheme, we need some form of hardware support to distinguish
     between the pages that are in memory and the pages that are on the disk.
     The valid ­invalid bit scheme described in Section 8.5.3 can be used for this
     purpose. This time, however, when this bit is set to "valid," the associated page
     is both legal and in memory. If the bit is set to "invalid," the page either is not
     valid (that is, not in the logical address space of the process) or is valid but
     is currently on the disk. The page-table entry for a page that is brought into
     memory is set as usual, but the page-table entry for a page that is not currently
     in memory is either simply marked invalid or contains the address of the page
     on disk. This situation is depicted in Figure 9.5.
     Notice that marking a page invalid will have no effect if the process never
     attempts to access that page. Hence, if we guess right and page in all pages
     that are actually needed and only those pages, the process will run exactly as
     though we had brought in all pages. While the process executes and accesses
     pages that are memory resident, execution proceeds normally.
                                                    0
                                                    1
            0   A                                   2
                                     valid­invalid
            1   B             frame        bit      3
            2   C                    0  4  v        4   A
            3   D                    1     i        5
                                     2  6  v
            4   E                    3     i        6   C                         A  B
            5   F                    4     i        7
                                     5  9  v                                C     D  E
            6   G                    6     i        8
            7   H                    7     i        9   F                   F     G  H
                logical              page  table    10
               memory
                                                    11
                                                    12
                                                    13
                                                    14
                                                    15
                                                    physical memory
                Figure   9.5  Page table when       some pages are not  in  main  memory.



                                                          9.2  Demand Paging      403
                                        3  page is on
                                           backing store
            operating
            system
                                           2
            reference                      trap
            1
    load M                           i
            6
            restart      page table
            instruction
                                           free frame
                         5                                     4
                         reset page                       bring in
                         table                            missing page
                                              physical
                                              memory
               Figure 9.6            Steps in handling a page fault.
    But what happens if the process tries to access a page that was not brought
into memory? Access to a page marked invalid causes a page fault. The paging
hardware, in translating the address through the page table, will notice that
the invalid bit is set, causing a trap to the operating system. This trap is the
result of the operating system's failure to bring the desired page into memory.
The procedure for handling this page fault is straightforward (Figure 9.6):
1.  We check an internal table (usually kept with the process control block)
    for this process to determine whether the reference was a valid or an
    invalid memory access.
2.  If the reference was invalid, we terminate the process. If it was valid but
    we have not yet brought in that page, we now page it in.
3.  We find a free frame (by taking one from the free-frame list, for example).
4.  We schedule a disk operation to read the desired page into the newly
    allocated frame.
5.  When the disk read is complete, we modify the internal table kept with
    the process and the page table to indicate that the page is now in memory.
6.  We restart the instruction that was interrupted by the trap. The process
    can now access the page as though it had always been in memory.
    In the extreme case, we can start executing a process with no pages in
memory. When the operating system sets the instruction pointer to the first



404  Chapter 9     Virtual Memory
     instruction of the process, which is on a non-memory-resident page, the process
     immediately faults for the page. After this page is brought into memory, the
     process continues to execute, faulting as necessary until every page that it
     needs is in memory. At that point, it can execute with no more faults. This
     scheme is pure demand paging: never bring a page into memory until it is
     required.
         Theoretically, some programs could access several new pages of memory
     with each instruction execution (one page for the instruction and many for
     data), possibly causing multiple page faults per instruction. This situation
     would result in unacceptable system performance. Fortunately, analysis of
     running processes shows that this behavior is exceedingly unlikely. Programs
     tend to have locality of reference, described in Section 9.6.1, which results in
     reasonable performance from demand paging.
         The hardware to support demand paging is the same as the hardware for
     paging and swapping:
     ·   Page table. This table has the ability to mark an entry invalid through a
         valid ­invalid bit or a special value of protection bits.
     ·   Secondary memory. This memory holds those pages that are not present
         in main memory. The secondary memory is usually a high-speed disk. It is
         known as the swap device, and the section of disk used for this purpose is
         known as swap space. Swap-space allocation is discussed in Chapter 10.
         A crucial requirement for demand paging is the ability to restart any
     instruction after a page fault. Because we save the state (registers, condition
     code, instruction counter) of the interrupted process when the page fault
     occurs, we must be able to restart the process in exactly the same place and
     state, except that the desired page is now in memory and is accessible. In most
     cases, this requirement is easy to meet. A page fault may occur at any memory
     reference. If the page fault occurs on the instruction fetch, we can restart by
     fetching the instruction again. If a page fault occurs while we are fetching an
     operand, we must fetch and decode the instruction again and then fetch the
     operand.
         As a worst-case example, consider a three-address instruction such as ADD
     the content of A to B, placing the result in C. These are the steps to execute this
     instruction:
     1.  Fetch and decode the instruction (ADD).
     2.  Fetch A.
     3.  Fetch B.
     4.  Add A and B.
     5.  Store the sum in C.
         If we fault when we try to store in C (because C is in a page not currently
     in memory), we will have to get the desired page, bring it in, correct the
     page table, and restart the instruction. The restart will require fetching the
     instruction again, decoding it again, fetching the two operands again, and
     then adding again. However, there is not much repeated work (less than one



                                                 9.2  Demand Paging                 405
complete instruction), and the repetition is necessary only when a page fault
occurs.
    The  major    difficulty  arises  when  one  instruction  may  modify  several
different locations. For example, consider the IBM System 360/370 MVC (move
character) instruction, which can move up to 256 bytes from one location to
another (possibly overlapping) location. If either block (source or destination)
straddles a page boundary, a page fault might occur after the move is partially
done. In addition, if the source and destination blocks overlap, the source
block may have been modified, in which case we cannot simply restart the
instruction.
    This problem can be solved in two different ways. In one solution, the
microcode computes and attempts to access both ends of both blocks. If a page
fault is going to occur, it will happen at this step, before anything is modified.
The move can then take place; we know that no page fault can occur, since all
the relevant pages are in memory. The other solution uses temporary registers
to hold the values of overwritten locations. If there is a page fault, all the old
values are written back into memory before the trap occurs. This action restores
memory to its state before the instruction was started, so that the instruction
can be repeated.
    This is by no means the only architectural problem resulting from adding
paging to an existing architecture to allow demand paging, but it illustrates
some of the difficulties involved. Paging is added between the CPU and the
memory in a computer system. It should be entirely transparent to the user
process. Thus, people often assume that paging can be added to any system.
Although this assumption is true for a non-demand-paging environment,
where a page fault represents a fatal error, it is not true where a page fault
means only that an additional page must be brought into memory and the
process restarted.
9.2.2    Performance of Demand Paging
Demand paging can significantly affect the performance of a computer system.
To see why, let's compute the effective access time for a demand-paged
memory. For most computer systems, the memory-access time, denoted ma,
ranges from 10 to 200 nanoseconds. As long as we have no page faults, the
effective access time is equal to the memory access time. If, however, a page
fault occurs, we must first read the relevant page from disk and then access the
desired word.
    Let p be the probability of a page fault (0  p  1). We would expect p to
be close to zero--that is, we would expect to have only a few page faults. The
effective access time is then
         effective access time = (1 - p) × ma + p × page fault time.
    To compute the effective access time, we must know how much time is
needed to service a page fault. A page fault causes the following sequence to
occur:
1.  Trap to the operating system.
2.  Save the user registers and process state.



406  Chapter 9  Virtual Memory
        3.  Determine that the interrupt was a page fault.
        4.  Check that the page reference was legal and determine the location of the
            page on the disk.
        5.  Issue a read from the disk to a free frame:
            a.  Wait in a queue for this device until the read request is serviced.
            b.  Wait for the device seek and/or latency time.
            c.  Begin the transfer of the page to a free frame.
        6.  While waiting, allocate the CPU to some other user (CPU scheduling,
            optional).
        7.  Receive an interrupt from the disk I/O subsystem (I/O completed).
        8.  Save the registers and process state for the other user (if step 6 is executed).
        9.  Determine that the interrupt was from the disk.
     10.    Correct the page table and other tables to show that the desired page is
            now in memory.
     11.    Wait for the CPU to be allocated to this process again.
     12.    Restore the user registers, process state, and new page table, and then
            resume the interrupted instruction.
     Not all of these steps are necessary in every case. For example, we are assuming
     that, in step 6, the CPU is allocated to another process while the I/O occurs.
     This arrangement allows multiprogramming to maintain CPU utilization but
     requires additional time to resume the page-fault service routine when the I/O
     transfer is complete.
            In any case, we are faced with three major components of the page-fault
     service time:
        1.  Service the page-fault interrupt.
        2.  Read in the page.
        3.  Restart the process.
            The first and third tasks can be reduced, with careful coding, to several
     hundred instructions. These tasks may take from 1 to 100 microseconds each.
     The page-switch time, however, will probably be close to 8 milliseconds.
     (A typical hard disk has an average latency of 3 milliseconds, a seek of
     5  milliseconds,   and    a  transfer  time  of  0.05  milliseconds.  Thus,  the  total
     paging time is about 8 milliseconds, including hardware and software time.)
     Remember also that we are looking at only the device-service time. If a queue
     of processes is waiting for the device, we have to add device-queueing time as
     we wait for the paging device to be free to service our request, increasing even
     more the time to swap.



                                                      9.2  Demand Paging               407
    With an average page-fault service time of 8 milliseconds and a memory-
access time of 200 nanoseconds, the effective access time in nanoseconds
is
            effective access time = (1 - p) × (200) + p (8 milliseconds)
                            = (1 - p) × 200 + p × 8,000,000
                            = 200 + 7,999,800 × p.
    We see, then, that the effective access time is directly proportional to the
page-fault rate. If one access out of 1,000 causes a page fault, the effective access
time is 8.2 microseconds. The computer will be slowed down by a factor of 40
because of demand paging! If we want performance degradation to be less
than 10 percent, we need to keep the probability of page faults at the following
level:
                            220 > 200 + 7,999,800 × p,
                            20 > 7,999,800 × p,
                            p < 0.0000025.
That is, to keep the slowdown due to paging at a reasonable level, we can
allow fewer than one memory access out of 399,990 to page-fault. In sum,
it is important to keep the page-fault rate low in a demand-paging system.
Otherwise,  the  effective  access  time  increases,  slowing  process    execution
dramatically.
    An additional aspect of demand paging is the handling and overall use
of swap space. Disk I/O to swap space is generally faster than that to the file
system. It is a faster file system because swap space is allocated in much larger
blocks, and file lookups and indirect allocation methods are not used (Chapter
10). The system can therefore gain better paging throughput by copying an
entire file image into the swap space at process startup and then performing
demand paging from the swap space. Another option is to demand pages
from the file system initially but to write the pages to swap space as they are
replaced. This approach will ensure that only needed pages are read from the
file system but that all subsequent paging is done from swap space.
    Some systems attempt to limit the amount of swap space used through
demand paging of binary files. Demand pages for such files are brought directly
from the file system. However, when page replacement is called for, these
frames can simply be overwritten (because they are never modified), and the
pages can be read in from the file system again if needed. Using this approach,
the file system itself serves as the backing store. However, swap space must still
be used for pages not associated with a file (known as anonymous memory);
these pages include the stack and heap for a process. This method appears to
be a good compromise and is used in several systems, including Solaris and
BSD UNIX.
    Mobile operating systems typically do not support swapping. Instead,
these systems demand-page from the file system and reclaim read-only pages
(such as code) from applications if memory becomes constrained. Such data
can be demand-paged from the file system if it is later needed. Under iOS,
anonymous memory pages are never reclaimed from an application unless the
application is terminated or explicitly releases the memory.



408  Chapter 9  Virtual Memory
9.3  Copy-on-Write
     In Section 9.2, we illustrated how a process can start quickly by demand-paging
     in the page containing the first instruction. However, process creation using the
     fork() system call may initially bypass the need for demand paging by using
     a technique similar to page sharing (covered in Section 8.5.4). This technique
     provides rapid process creation and minimizes the number of new pages that
     must be allocated to the newly created process.
     Recall that the fork() system call creates a child process that is a duplicate
     of its parent. Traditionally, fork() worked by creating a copy of the parent's
     address space for the child, duplicating the pages belonging to the parent.
     However, considering that many child processes invoke the exec() system
     call immediately after creation, the copying of the parent's address space may
     be unnecessary. Instead, we can use a technique known as copy-on-write,
     which works by allowing the parent and child processes initially to share the
     same pages. These shared pages are marked as copy-on-write pages, meaning
     that if either process writes to a shared page, a copy of the shared page is
     created. Copy-on-write is illustrated in Figures 9.7 and 9.8, which show the
     contents of the physical memory before and after process 1 modifies page C.
     For example, assume that the child process attempts to modify a page
     containing portions of the stack, with the pages set to be copy-on-write. The
     operating system will create a copy of this page, mapping it to the address space
     of the child process. The child process will then modify its copied page and not
     the page belonging to the parent process. Obviously, when the copy-on-write
     technique is used, only the pages that are modified by either process are copied;
     all unmodified pages can be shared by the parent and child processes. Note, too,
     that only pages that can be modified need be marked as copy-on-write. Pages
     that cannot be modified (pages containing executable code) can be shared by
     the parent and child. Copy-on-write is a common technique used by several
     operating systems, including Windows XP, Linux, and Solaris.
     When it is determined that a page is going to be duplicated using copy-
     on-write, it is important to note the location from which the free page will
     be allocated. Many operating systems provide a pool of free pages for such
     requests. These free pages are typically allocated when the stack or heap for a
     process must expand or when there are copy-on-write pages to be managed.
                                      physical
                process1              memory                             process2
                                      page A
                                      page B
                                      page C
                          Figure 9.7  Before process 1 modifies page C.



                                                      9.4  Page Replacement                 409
                                     physical
     process1                        memory                            process2
                                     page A
                                     page B
                                     page C
                                     Copy of page  C
               Figure 9.8            After process 1 modifies page C.
     Operating systems typically allocate these pages using a technique known as
     zero-fill-on-demand. Zero-fill-on-demand pages have been zeroed-out before
     being allocated, thus erasing the previous contents.
     Several versions of UNIX (including Solaris and Linux) provide a variation
     of the fork() system call--vfork() (for virtual memory fork)--that operates
     differently from fork() with copy-on-write. With vfork(), the parent process
     is suspended, and the child process uses the address space of the parent.
     Because vfork() does not use copy-on-write, if the child process changes
     any pages of the parent's address space, the altered pages will be visible to the
     parent once it resumes. Therefore, vfork() must be used with caution to ensure
     that the child process does not modify the address space of the parent. vfork()
     is intended to be used when the child process calls exec() immediately after
     creation. Because no copying of pages takes place, vfork() is an extremely
     efficient method of process creation and is sometimes used to implement UNIX
     command-line shell interfaces.
9.4  Page Replacement
     In our earlier discussion of the page-fault rate, we assumed that each page
     faults at most once, when it is first referenced. This representation is not strictly
     accurate, however. If a process of ten pages actually uses only half of them, then
     demand paging saves the I/O necessary to load the five pages that are never
     used. We could also increase our degree of multiprogramming by running
     twice as many processes. Thus, if we had forty frames, we could run eight
     processes, rather than the four that could run if each required ten frames (five
     of which were never used).
     If we increase our degree of multiprogramming, we are over-allocating
     memory. If we run six processes, each of which is ten pages in size but actually
     uses only five pages, we have higher CPU utilization and throughput, with
     ten frames to spare. It is possible, however, that each of these processes, for a
     particular data set, may suddenly try to use all ten of its pages, resulting in a
     need for sixty frames when only forty are available.
     Further, consider that system memory is not used only for holding program
     pages. Buffers for I/O also consume a considerable amount of memory. This use



410  Chapter 9  Virtual Memory
                                      valid­invalid
                0  H           frame       bit       0  monitor
     PC         1  load M             3    v         1
                2  J                  4    v         2  D
                                      5    v
                3  M                       i         3  H                 B
                logical memory        page table     4  load M
                   for user 1         for user 1
                                                     5  J
                                                     6  A
                                      valid­invalid  7  E                    M
                0  A           frame       bit
                                                        physical
                1  B                  6    v            memory
                2  D                       i
                                      2    v
                3  E                  7    v
                logical memory        page table
                   for user 2         for user 2
                               Figure 9.9     Need for page replacement.
     can increase the strain on memory-placement algorithms. Deciding how much
     memory to allocate to I/O and how much to program pages is a significant
     challenge. Some systems allocate a fixed percentage of memory for I/O buffers,
     whereas others allow both user processes and the I/O subsystem to compete
     for all system memory.
     Over-allocation of memory manifests itself as follows. While a user process
     is executing, a page fault occurs. The operating system determines where the
     desired page is residing on the disk but then finds that there are no free frames
     on the free-frame list; all memory is in use (Figure 9.9).
     The operating system has several options at this point. It could terminate
     the user process. However, demand paging is the operating system's attempt to
     improve the computer system's utilization and throughput. Users should not
     be aware that their processes are running on a paged system--paging should
     be logically transparent to the user. So this option is not the best choice.
     The operating system could instead swap out a process, freeing all its
     frames and reducing the level of multiprogramming. This option is a good one
     in certain circumstances, and we consider it further in Section 9.6. Here, we
     discuss the most common solution: page replacement.
     9.4.1  Basic Page Replacement
     Page replacement takes the following approach. If no frame is free, we find
     one that is not currently being used and free it. We can free a frame by writing
     its contents to swap space and changing the page table (and all other tables) to
     indicate that the page is no longer in memory (Figure 9.10). We can now use
     the freed frame to hold the page for which the process faulted. We modify the
     page-fault service routine to include page replacement:



                                                   9.4       Page Replacement          411
    frame  valid­invalid bit
                                                   page out
                        change                     victim
           0  i     2  to invalid                  page
                                                1
           f  v
                    4              f  victim
                    reset page
        page table     table for                3
                    new page                       page in
                                                   desired
                                                   page
                                      physical
                                      memory
                              Figure 9.10  Page replacement.
1.  Find the location of the desired page on the disk.
2.  Find a free frame:
    a.     If there is a free frame, use it.
    b.     If there is no free frame, use a page-replacement algorithm to select
           a victim frame.
    c.     Write the victim frame to the disk; change the page and frame tables
           accordingly.
3.  Read the desired page into the newly freed frame; change the page and
    frame tables.
4.  Continue the user process from where the page fault occurred.
Notice that, if no frames are free, two page transfers (one out and one in)
are required. This situation effectively doubles the page-fault service time and
increases the effective access time accordingly.
    We can reduce this overhead by using a modify bit (or dirty bit). When
this scheme is used, each page or frame has a modify bit associated with it in
the hardware. The modify bit for a page is set by the hardware whenever any
byte in the page is written into, indicating that the page has been modified.
When we select a page for replacement, we examine its modify bit. If the bit
is set, we know that the page has been modified since it was read in from the
disk. In this case, we must write the page to the disk. If the modify bit is not set,
however, the page has not been modified since it was read into memory. In this
case, we need not write the memory page to the disk: it is already there. This
technique also applies to read-only pages (for example, pages of binary code).



412  Chapter 9   Virtual Memory
     Such pages cannot be modified; thus, they may be discarded when desired.
     This scheme can significantly reduce the time required to service a page fault,
     since it reduces I/O time by one-half if the page has not been modified.
     Page replacement is basic to demand paging. It completes the separation
     between logical memory and physical memory. With this mechanism, an
     enormous virtual memory can be provided for programmers on a smaller
     physical memory. With no demand paging, user addresses are mapped into
     physical addresses, and the two sets of addresses can be different. All the
     pages of a process still must be in physical memory, however. With demand
     paging, the size of the logical address space is no longer constrained by physical
     memory. If we have a user process of twenty pages, we can execute it in ten
     frames simply by using demand paging and using a replacement algorithm to
     find a free frame whenever necessary. If a page that has been modified is to be
     replaced, its contents are copied to the disk. A later reference to that page will
     cause a page fault. At that time, the page will be brought back into memory,
     perhaps replacing some other page in the process.
     We must solve two major problems to implement demand paging: we must
     develop a frame-allocation algorithm and a page-replacement algorithm.
     That is, if we have multiple processes in memory, we must decide how many
     frames to allocate to each process; and when page replacement is required,
     we must select the frames that are to be replaced. Designing appropriate
     algorithms to solve these problems is an important task, because disk I/O
     is so expensive. Even slight improvements in demand-paging methods yield
     large gains in system performance.
     There are many different page-replacement algorithms. Every operating
     system   probably  has      its  own   replacement  scheme.       How       do  we     select  a
     particular replacement algorithm? In general, we want the one with the lowest
     page-fault rate.
     We evaluate an algorithm by running it on a particular string of memory
     references and computing the number of page faults. The string of memory
     references  is   called  a  reference  string.  We  can           generate  reference  strings
     artificially (by using a random-number generator, for example), or we can trace
     a given system and record the address of each memory reference. The latter
     choice produces a large number of data (on the order of 1 million addresses
     per second). To reduce the number of data, we use two facts.
     First, for a given page size (and the page size is generally fixed by the
     hardware or system), we need to consider only the page number, rather than
     the entire address. Second, if we have a reference to a page p, then any references
     to page p that immediately follow will never cause a page fault. Page p will
     be in memory after the first reference, so the immediately following references
     will not fault.
     For example, if we trace a particular process, we might record the following
     address sequence:
              0100, 0432, 0101, 0612, 0102, 0103, 0104, 0101, 0611, 0102, 0103,
              0104, 0101, 0610, 0102, 0103, 0104, 0101, 0609, 0102, 0105
     At 100 bytes per page, this sequence is reduced to the following reference
     string:
                                      1, 4, 1, 6, 1, 6, 1, 6, 1, 6, 1



                                                9.4          Page Replacement             413
                       16
number of page faults  14
                       12
                       10
                       8
                       6
                       4
                       2
                           1            2  3    4            5                         6
                                           number of frames
                           Figure 9.11  Graph of page faults versus number of frames.
To determine the number of page faults for a particular reference string and
page-replacement algorithm, we also need to know the number of page frames
available. Obviously, as the number of frames available increases, the number
of page faults decreases. For the reference string considered previously, for
example, if we had three or more frames, we would have only three faults--
one fault for the first reference to each page. In contrast, with only one frame
available, we would have a replacement with every reference, resulting in
eleven faults. In general, we expect a curve such as that in Figure 9.11. As the
number of frames increases, the number of page faults drops to some minimal
level. Of course, adding physical memory increases the number of frames.
We next illustrate several page-replacement algorithms. In doing so, we
use the reference string
                           7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
for a memory with three frames.
9.4.2                  FIFO Page Replacement
The simplest page-replacement algorithm is a first-in, first-out (FIFO) algorithm.
A FIFO replacement algorithm associates with each page the time when that
page was brought into memory. When a page must be replaced, the oldest
page is chosen. Notice that it is not strictly necessary to record the time when
a page is brought in. We can create a FIFO queue to hold all pages in memory.
We replace the page at the head of the queue. When a page is brought into
memory, we insert it at the tail of the queue.
For our example reference string, our three frames are initially empty. The
first three references (7, 0, 1) cause page faults and are brought into these empty
frames. The next reference (2) replaces page 7, because page 7 was brought in
first. Since 0 is the next reference and 0 is already in memory, we have no fault
for this reference. The first reference to 3 results in replacement of page 0, since
it is now first in line. Because of this replacement, the next reference, to 0, will



414  Chapter 9      Virtual Memory
     reference string
     7     0     1     2     0  3  0      4     2     3     0     3  2    1  2     0  1  7  0  1
        7     7     7     2     2     2      4     4     4     0          0     0        7  7     7
              0     0     0     3     3      3     2     2     2          1     1        1  0     0
                    1     1     1     0      0     0     3     3          3     2        2  2     1
     page frames
                             Figure 9.12     FIFO page-replacement algorithm.
     fault. Page 1 is then replaced by page 0. This process continues as shown in
     Figure 9.12. Every time a fault occurs, we show which pages are in our three
     frames. There are fifteen faults altogether.
        The FIFO page-replacement algorithm is easy to understand and program.
     However, its performance is not always good. On the one hand, the page
     replaced may be an initialization module that was used a long time ago and is
     no longer needed. On the other hand, it could contain a heavily used variable
     that was initialized early and is in constant use.
        Notice that, even if we select for replacement a page that is in active use,
     everything still works correctly. After we replace an active page with a new
     one, a fault occurs almost immediately to retrieve the active page. Some other
     page must be replaced to bring the active page back into memory. Thus, a bad
     replacement choice increases the page-fault rate and slows process execution.
     It does not, however, cause incorrect execution.
        To illustrate the problems that are possible with a FIFO page-replacement
     algorithm, consider the following reference string:
                                      1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5
     Figure 9.13 shows the curve of page faults for this reference string versus the
     number of available frames. Notice that the number of faults for four frames
     (ten) is greater than the number of faults for three frames (nine)! This most
     unexpected result is known as Belady's anomaly: for some page-replacement
     algorithms, the page-fault rate may increase as the number of allocated frames
     increases. We would expect that giving more memory to a process would
     improve its performance. In some early research, investigators noticed that
     this assumption was not always true. Belady's anomaly was discovered as a
     result.
     9.4.3       Optimal Page Replacement
     One result of the discovery of Belady's anomaly was the search for an optimal
     page-replacement algorithm--the algorithm that has the lowest page-fault
     rate of all algorithms and will never suffer from Belady's anomaly. Such an
     algorithm does exist and has been called OPT or MIN. It is simply this:
           Replace the page that will not be used for the longest period of time.
     Use of this page-replacement algorithm guarantees the lowest possible page-
     fault rate for a fixed number of frames.



                                                                              9.4   Page Replacement               415
                                16
         number of page faults  14
                                12
                                10
                                   8
                                   6
                                   4
                                   2
                                               1         2        3        4        5         6           7
                                                               number of frames
      Figure 9.13                           Page-fault curve for FIFO replacement on a reference string.
For example, on our sample reference string, the optimal page-replacement
algorithm would yield nine page faults, as shown in Figure 9.14. The first three
references cause faults that fill the three empty frames. The reference to page
2 replaces page 7, because page 7 will not be used until reference 18, whereas
page 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces
page 1, as page 1 will be the last of the three pages in memory to be referenced
again. With only nine page faults, optimal replacement is much better than
a FIFO algorithm, which results in fifteen faults. (If we ignore the first three,
which all algorithms must suffer, then optimal replacement is twice as good as
FIFO replacement.) In fact, no replacement algorithm can process this reference
string in three frames with fewer than nine faults.
Unfortunately,                              the   optimal      page-replacement     algorithm    is  difficult     to
implement, because it requires future knowledge of the reference string. (We
encountered a similar situation with the SJF CPU-scheduling algorithm in
Section 6.3.2.) As a result, the optimal algorithm is used mainly for comparison
studies. For instance, it may be useful to know that, although a new algorithm
is not optimal, it is within 12.3 percent of optimal at worst and within 4.7
percent on average.
reference string
7     0                         1     2     0  3      0  4     2  3  0     3     2  1     2   0  1        7     0  1
   7     7                         7     2        2         2           2              2                     7
         0                         0     0        0         4           0              0                     0
                                   1     1        3         3           3              1                     1
page frames
                                         Figure 9.14     Optimal page-replacement algorithm.



416  Chapter 9       Virtual Memory
     9.4.4        LRU Page Replacement
     If the optimal algorithm is not feasible, perhaps an approximation of the
     optimal algorithm is possible. The key distinction between the FIFO and OPT
     algorithms (other than looking backward versus forward in time) is that the
     FIFO algorithm uses the time when a page was brought into memory, whereas
     the OPT algorithm uses the time when a page is to be used. If we use the recent
     past as an approximation of the near future, then we can replace the page that
     has not been used for the longest period of time. This approach is the least
     recently used (LRU) algorithm.
         LRU replacement associates with each page the time of that page's last use.
     When a page must be replaced, LRU chooses the page that has not been used
     for the longest period of time. We can think of this strategy as the optimal
     page-replacement algorithm looking backward in time, rather than forward.
     (Strangely, if we let SR be the reverse of a reference string S, then the page-fault
     rate for the OPT algorithm on S is the same as the page-fault rate for the OPT
     algorithm on SR. Similarly, the page-fault rate for the LRU algorithm on S is the
     same as the page-fault rate for the LRU algorithm on SR.)
         The result of applying LRU replacement to our example reference string is
     shown in Figure 9.15. The LRU algorithm produces twelve faults. Notice that
     the first five faults are the same as those for optimal replacement. When the
     reference to page 4 occurs, however, LRU replacement sees that, of the three
     frames in memory, page 2 was used least recently. Thus, the LRU algorithm
     replaces page 2, not knowing that page 2 is about to be used. When it then faults
     for page 2, the LRU algorithm replaces page 3, since it is now the least recently
     used of the three pages in memory. Despite these problems, LRU replacement
     with twelve faults is much better than FIFO replacement with fifteen.
         The      LRU      policy     is  often     used     as     a  page-replacement          algorithm      and
     is  considered        to     be  good.     The    major     problem     is  how      to  implement         LRU
     replacement. An LRU page-replacement algorithm may require substantial
     hardware assistance. The problem is to determine an order for the frames
     defined by the time of last use. Two implementations are feasible:
     ·   Counters. In the simplest case, we associate with each page-table entry a
         time-of-use field and add to the CPU a logical clock or counter. The clock is
         incremented for every memory reference. Whenever a reference to a page
         is made, the contents of the clock register are copied to the time-of-use
         field in the page-table entry for that page. In this way, we always have
     reference string
         7     0     1     2      0   3      0   4     2     3      0     3  2   1     2      0     1  7     0  1
            7     7     7      2          2         4     4      4     0            1            1        1
                  0     0      0          0         0     0      3     3            3            0        0
                        1      1          3         3     2      2     2            2            2        7
         page frames
                                  Figure  9.15   LRU page-replacement algorithm.



                                             9.4  Page Replacement                417
   the "time" of the last reference to each page. We replace the page with the
   smallest time value. This scheme requires a search of the page table to find
   the LRU page and a write to memory (to the time-of-use field in the page
   table) for each memory access. The times must also be maintained when
   page tables are changed (due to CPU scheduling). Overflow of the clock
   must be considered.
·  Stack. Another approach to implementing LRU replacement is to keep
   a stack of page numbers. Whenever a page is referenced, it is removed
   from the stack and put on the top. In this way, the most recently used
   page is always at the top of the stack and the least recently used page is
   always at the bottom (Figure 9.16). Because entries must be removed from
   the middle of the stack, it is best to implement this approach by using a
   doubly linked list with a head pointer and a tail pointer. Removing a page
   and putting it on the top of the stack then requires changing six pointers
   at worst. Each update is a little more expensive, but there is no search for
   a replacement; the tail pointer points to the bottom of the stack, which is
   the LRU page. This approach is particularly appropriate for software or
   microcode implementations of LRU replacement.
   Like optimal replacement, LRU replacement does not suffer from Belady's
anomaly. Both belong to a class of page-replacement algorithms, called stack
algorithms, that can never exhibit Belady's anomaly. A stack algorithm is an
algorithm for which it can be shown that the set of pages in memory for n
frames is always a subset of the set of pages that would be in memory with n
+ 1 frames. For LRU replacement, the set of pages in memory would be the n
most recently referenced pages. If the number of frames is increased, these n
pages will still be the most recently referenced and so will still be in memory.
   Note that neither implementation of LRU would be conceivable without
hardware assistance beyond the standard TLB registers. The updating of the
clock fields or stack must be done for every memory reference. If we were
to use an interrupt for every reference to allow software to update such data
structures, it would slow every memory reference by a factor of at least ten,
   reference string
   4            7       0  7  1  0  1     2  1    2     7     1  2
                   2                   7
                                                     a     b
                   1                   2
                   0                   1
                   7                   0
                   4                   4
                stack               stack
                before              after
                   a                   b
   Figure 9.16  Use of a stack to record the most recent page references.



418  Chapter 9   Virtual Memory
     hence slowing every user process by a factor of ten. Few systems could tolerate
     that level of overhead for memory management.
     9.4.5    LRU-Approximation Page Replacement
     Few computer systems provide sufficient hardware support for true LRU page
     replacement. In fact, some systems provide no hardware support, and other
     page-replacement algorithms (such as a FIFO algorithm) must be used. Many
     systems provide some help, however, in the form of a reference bit. The
     reference bit for a page is set by the hardware whenever that page is referenced
     (either a read or a write to any byte in the page). Reference bits are associated
     with each entry in the page table.
          Initially, all bits are cleared (to 0) by the operating system. As a user process
     executes, the bit associated with each page referenced is set (to 1) by the
     hardware. After some time, we can determine which pages have been used and
     which have not been used by examining the reference bits, although we do not
     know the order of use. This information is the basis for many page-replacement
     algorithms that approximate LRU replacement.
     9.4.5.1  Additional-Reference-Bits Algorithm
     We can gain additional ordering information by recording the reference bits at
     regular intervals. We can keep an 8-bit byte for each page in a table in memory.
     At regular intervals (say, every 100 milliseconds), a timer interrupt transfers
     control to the operating system. The operating system shifts the reference bit
     for each page into the high-order bit of its 8-bit byte, shifting the other bits right
     by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the
     history of page use for the last eight time periods. If the shift register contains
     00000000, for example, then the page has not been used for eight time periods.
     A page that is used at least once in each period has a shift register value of
     11111111. A page with a history register value of 11000100 has been used more
     recently than one with a value of 01110111. If we interpret these 8-bit bytes
     as unsigned integers, the page with the lowest number is the LRU page, and
     it can be replaced. Notice that the numbers are not guaranteed to be unique,
     however. We can either replace (swap out) all pages with the smallest value or
     use the FIFO method to choose among them.
          The number of bits of history included in the shift register can be varied,
     of course, and is selected (depending on the hardware available) to make
     the updating as fast as possible. In the extreme case, the number can be
     reduced to zero, leaving only the reference bit itself. This algorithm is called
     the second-chance page-replacement algorithm.
     9.4.5.2  Second-Chance Algorithm
     The  basic  algorithm  of  second-chance  replacement  is  a  FIFO  replacement
     algorithm. When a page has been selected, however, we inspect its reference
     bit. If the value is 0, we proceed to replace this page; but if the reference bit
     is set to 1, we give the page a second chance and move on to select the next
     FIFO page. When a page gets a second chance, its reference bit is cleared, and
     its arrival time is reset to the current time. Thus, a page that is given a second
     chance will not be replaced until all other pages have been replaced (or given



                                                      9.4  Page Replacement         419
                 reference       pages                     reference       pages
                 bits                                      bits
                 0                                         0
                 0                                         0
         next    1                                         0
         victim
                 1                                         0
                 0                                         0
                 ...             ...                       ...             ...
                 1                                         1
                 1                                         1
                 circular queue of pages                   circular queue of pages
                            (a)                                       (b)
         Figure 9.17   Second-chance (clock) page-replacement algorithm.
second chances). In addition, if a page is used often enough to keep its reference
bit set, it will never be replaced.
    One way to implement the second-chance algorithm (sometimes referred
to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on
the clock) indicates which page is to be replaced next. When a frame is needed,
the pointer advances until it finds a page with a 0 reference bit. As it advances,
it clears the reference bits (Figure 9.17). Once a victim page is found, the page
is replaced, and the new page is inserted in the circular queue in that position.
Notice that, in the worst case, when all bits are set, the pointer cycles through
the whole queue, giving each page a second chance. It clears all the reference
bits before selecting the next page for replacement. Second-chance replacement
degenerates to FIFO replacement if all bits are set.
9.4.5.3  Enhanced Second-Chance Algorithm
We can enhance the second-chance algorithm by considering the reference bit
and the modify bit (described in Section 9.4.1) as an ordered pair. With these
two bits, we have the following four possible classes:
1.  (0, 0) neither recently used nor modified --best page to replace
2.  (0, 1) not recently used but modified --not quite as good, because the
    page will need to be written out before replacement



420  Chapter 9  Virtual Memory
     3.    (1, 0) recently used but clean--probably will be used again soon
     4.    (1, 1) recently used and modified--probably will be used again soon, and
           the page will be need to be written out to disk before it can be replaced
     Each page is in one of these four classes. When page replacement is called for,
     we use the same scheme as in the clock algorithm; but instead of examining
     whether the page to which we are pointing has the reference bit set to 1,
     we examine the class to which that page belongs. We replace the first page
     encountered in the lowest nonempty class. Notice that we may have to scan
     the circular queue several times before we find a page to be replaced.
           The major difference between this algorithm and the simpler clock algo-
     rithm is that here we give preference to those pages that have been modified
     in order to reduce the number of I/Os required.
     9.4.6  Counting-Based Page Replacement
     There are many other algorithms that can be used for page replacement. For
     example, we can keep a counter of the number of references that have been
     made to each page and develop the following two schemes.
     ·     The least frequently used (LFU) page-replacement algorithm requires that
           the page with the smallest count be replaced. The reason for this selection is
           that an actively used page should have a large reference count. A problem
           arises, however, when a page is used heavily during the initial phase of
           a process but then is never used again. Since it was used heavily, it has a
           large count and remains in memory even though it is no longer needed.
           One solution is to shift the counts right by 1 bit at regular intervals, forming
           an exponentially decaying average usage count.
     ·     The most frequently used (MFU) page-replacement algorithm is based
           on the argument that the page with the smallest count was probably just
           brought in and has yet to be used.
     As you might expect, neither MFU nor LFU replacement is common. The
     implementation of these algorithms is expensive, and they do not approximate
     OPT replacement well.
     9.4.7  Page-Buffering Algorithms
     Other procedures are often used in addition to a specific page-replacement
     algorithm. For example, systems commonly keep a pool of free frames. When
     a page fault occurs, a victim frame is chosen as before. However, the desired
     page is read into a free frame from the pool before the victim is written out. This
     procedure allows the process to restart as soon as possible, without waiting
     for the victim page to be written out. When the victim is later written out, its
     frame is added to the free-frame pool.
           An expansion of this idea is to maintain a list of modified pages. Whenever
     the paging device is idle, a modified page is selected and is written to the disk.
     Its modify bit is then reset. This scheme increases the probability that a page
     will be clean when it is selected for replacement and will not need to be written
     out.



                                               9.5    Allocation of Frames                 421
          Another modification is to keep a pool of free frames but to remember
     which page was in each frame. Since the frame contents are not modified when
     a frame is written to the disk, the old page can be reused directly from the
     free-frame pool if it is needed before that frame is reused. No I/O is needed in
     this case. When a page fault occurs, we first check whether the desired page is
     in the free-frame pool. If it is not, we must select a free frame and read into it.
          This technique is used in the VAX/VMS system along with a FIFO replace-
     ment algorithm. When the FIFO replacement algorithm mistakenly replaces a
     page that is still in active use, that page is quickly retrieved from the free-frame
     pool, and no I/O is necessary. The free-frame buffer provides protection against
     the relatively poor, but simple, FIFO replacement algorithm. This method is
     necessary because the early versions of VAX did not implement the reference
     bit correctly.
          Some versions of the UNIX system use this method in conjunction with
     the second-chance algorithm. It can be a useful augmentation to any page-
     replacement algorithm, to reduce the penalty incurred if the wrong victim
     page is selected.
     9.4.8  Applications and Page Replacement
     In certain cases, applications accessing data through the operating system's
     virtual memory perform worse than if the operating system provided no
     buffering at all. A typical example is a database, which provides its own
     memory management and I/O buffering. Applications like this understand
     their memory use and disk use better than does an operating system that is
     implementing algorithms for general-purpose use. If the operating system is
     buffering I/O and the application is doing so as well, however, then twice the
     memory is being used for a set of I/O.
          In another example, data warehouses frequently perform massive sequen-
     tial disk reads, followed by computations and writes. The LRU algorithm would
     be removing old pages and preserving new ones, while the application would
     more likely be reading older pages than newer ones (as it starts its sequential
     reads again). Here, MFU would actually be more efficient than LRU.
          Because of such problems, some operating systems give special programs
     the ability to use a disk partition as a large sequential array of logical blocks,
     without any file-system data structures. This array is sometimes called the raw
     disk, and I/O to this array is termed raw I/O. Raw I/O bypasses all the file-
     system services, such as file I/O demand paging, file locking, prefetching, space
     allocation, file names, and directories. Note that although certain applications
     are  more  efficient  when  implementing  their  own  special-purpose  storage
     services on a raw partition, most applications perform better when they use
     the regular file-system services.
9.5  Allocation of Frames
     We turn next to the issue of allocation. How do we allocate the fixed amount
     of free memory among the various processes? If we have 93 free frames and
     two processes, how many frames does each process get?
          The simplest case is the single-user system. Consider a single-user system
     with 128 KB of memory composed of pages 1 KB in size. This system has 128



422  Chapter 9   Virtual Memory
     frames. The operating system may take 35 KB, leaving 93 frames for the user
     process. Under pure demand paging, all 93 frames would initially be put on
     the free-frame list. When a user process started execution, it would generate a
     sequence of page faults. The first 93 page faults would all get free frames from
     the free-frame list. When the free-frame list was exhausted, a page-replacement
     algorithm would be used to select one of the 93 in-memory pages to be replaced
     with the 94th, and so on. When the process terminated, the 93 frames would
     once again be placed on the free-frame list.
     There are many variations on this simple strategy. We can require that the
     operating system allocate all its buffer and table space from the free-frame list.
     When this space is not in use by the operating system, it can be used to support
     user paging. We can try to keep three free frames reserved on the free-frame list
     at all times. Thus, when a page fault occurs, there is a free frame available to
     page into. While the page swap is taking place, a replacement can be selected,
     which is then written to the disk as the user process continues to execute.
     Other variants are also possible, but the basic strategy is clear: the user process
     is allocated any free frame.
     9.5.1  Minimum Number of Frames
     Our strategies for the allocation of frames are constrained in various ways. We
     cannot, for example, allocate more than the total number of available frames
     (unless there is page sharing). We must also allocate at least a minimum number
     of frames. Here, we look more closely at the latter requirement.
     One reason for allocating at least a minimum number of frames involves
     performance. Obviously, as the number of frames allocated to each process
     decreases, the page-fault rate increases, slowing process execution. In addition,
     remember that, when a page fault occurs before an executing instruction
     is complete, the instruction must be restarted. Consequently, we must have
     enough frames to hold all the different pages that any single instruction can
     reference.
     For example, consider a machine in which all memory-reference instruc-
     tions may reference only one memory address. In this case, we need at least one
     frame for the instruction and one frame for the memory reference. In addition,
     if one-level indirect addressing is allowed (for example, a load instruction on
     page 16 can refer to an address on page 0, which is an indirect reference to page
     23), then paging requires at least three frames per process. Think about what
     might happen if a process had only two frames.
     The minimum number of frames is defined by the computer architecture.
     For example, the move instruction for the PDP-11 includes more than one word
     for some addressing modes, and thus the instruction itself may straddle two
     pages. In addition, each of its two operands may be indirect references, for a
     total of six frames. Another example is the IBM 370 MVC instruction. Since the
     instruction is from storage location to storage location, it takes 6 bytes and can
     straddle two pages. The block of characters to move and the area to which it
     is to be moved can each also straddle two pages. This situation would require
     six frames. The worst case occurs when the MVC instruction is the operand of
     an EXECUTE instruction that straddles a page boundary; in this case, we need
     eight frames.



                                                          9.5      Allocation of Frames           423
    The   worst-case   scenario  occurs         in  computer           architectures      that  allow
multiple levels of indirection (for example, each 16-bit word could contain
a 15-bit address plus a 1-bit indirect indicator). Theoretically, a simple load
instruction could reference an indirect address that could reference an indirect
address (on another page) that could also reference an indirect address (on yet
another page), and so on, until every page in virtual memory had been touched.
Thus, in the worst case, the entire virtual memory must be in physical memory.
To overcome this difficulty, we must place a limit on the levels of indirection (for
example, limit an instruction to at most 16 levels of indirection). When the first
indirection occurs, a counter is set to 16; the counter is then decremented for
each successive indirection for this instruction. If the counter is decremented to
0, a trap occurs (excessive indirection). This limitation reduces the maximum
number of memory references per instruction to 17, requiring the same number
of frames.
    Whereas the minimum number of frames per process is defined by the
architecture, the maximum number is defined by the amount of available
physical memory. In between, we are still left with significant choice in frame
allocation.
9.5.2    Allocation Algorithms
The easiest way to split m frames among n processes is to give everyone an
equal share, m/n frames (ignoring frames needed by the operating system
for the moment). For instance, if there are 93 frames and five processes, each
process will get 18 frames. The three leftover frames can be used as a free-frame
buffer pool. This scheme is called equal allocation.
    An alternative is to recognize that various processes will need differing
amounts of memory. Consider a system with a 1-KB frame size. If a small
student process of 10 KB and an interactive database of 127 KB are the only
two processes running in a system with 62 free frames, it does not make much
sense to give each process 31 frames. The student process does not need more
than 10 frames, so the other 21 are, strictly speaking, wasted.
    To solve this problem, we can use proportional allocation, in which we
allocate available memory to each process according to its size. Let the size of
the virtual memory for process pi be si , and define
                                        S=          si .
Then, if the total number of available          frames         is  m,  we  allocate   ai  frames  to
process pi , where ai is approximately
                                 ai = si /S × m.
Of  course,  we  must  adjust  each     ai  to  be  an    integer      that  is  greater  than    the
minimum number of frames required by the instruction set, with a sum not
exceeding m.
    With     proportional  allocation,      we  would          split   62  frames    between    two
processes, one of 10 pages and one of 127 pages, by allocating 4 frames and 57
frames, respectively, since
                               10/137 × 62  4, and
                               127/137 × 62  57.



424  Chapter 9  Virtual Memory
     In this way, both processes share the available frames according to their
     "needs," rather than equally.
     In both equal and proportional allocation, of course, the allocation may
     vary according to the multiprogramming level. If the multiprogramming level
     is increased, each process will lose some frames to provide the memory needed
     for the new process. Conversely, if the multiprogramming level decreases, the
     frames that were allocated to the departed process can be spread over the
     remaining processes.
     Notice that, with either equal or proportional allocation, a high-priority
     process is treated the same as a low-priority process. By its definition, however,
     we may want to give the high-priority process more memory to speed its
     execution, to the detriment of low-priority processes. One solution is to use
     a proportional allocation scheme wherein the ratio of frames depends not on
     the relative sizes of processes but rather on the priorities of processes or on a
     combination of size and priority.
     9.5.3  Global versus Local Allocation
     Another important factor in the way frames are allocated to the various
     processes is page replacement. With multiple processes competing for frames,
     we can classify page-replacement algorithms into two broad categories: global
     replacement and local replacement. Global replacement allows a process to
     select a replacement frame from the set of all frames, even if that frame is
     currently allocated to some other process; that is, one process can take a frame
     from another. Local replacement requires that each process select from only its
     own set of allocated frames.
     For example, consider an allocation scheme wherein we allow high-priority
     processes to select frames from low-priority processes for replacement. A
     process can select a replacement from among its own frames or the frames
     of any lower-priority process. This approach allows a high-priority process to
     increase its frame allocation at the expense of a low-priority process. With a
     local replacement strategy, the number of frames allocated to a process does not
     change. With global replacement, a process may happen to select only frames
     allocated to other processes, thus increasing the number of frames allocated to
     it (assuming that other processes do not choose its frames for replacement).
     One problem with a global replacement algorithm is that a process cannot
     control its own page-fault rate. The set of pages in memory for a process
     depends not only on the paging behavior of that process but also on the paging
     behavior of other processes. Therefore, the same process may perform quite
     differently (for example, taking 0.5 seconds for one execution and 10.3 seconds
     for the next execution) because of totally external circumstances. Such is not
     the case with a local replacement algorithm. Under local replacement, the
     set of pages in memory for a process is affected by the paging behavior of
     only that process. Local replacement might hinder a process, however, by
     not making available to it other, less used pages of memory. Thus, global
     replacement generally results in greater system throughput and is therefore
     the more commonly used method.
     9.5.4  Non-Uniform Memory Access
     Thus far in our coverage of virtual memory, we have assumed that all main
     memory is created equal--or at least that it is accessed equally. On many



                                                                9.6  Thrashing           425
     computer systems, that is not the case. Often, in systems with multiple CPUs
     (Section 1.3.2), a given CPU can access some sections of main memory faster
     than it can access others. These performance differences are caused by how
     CPUs and memory are interconnected in the system. Frequently, such a system
     is made up of several system boards, each containing multiple CPUs and some
     memory. The system boards are interconnected in various ways, ranging from
     system buses to high-speed network connections like InfiniBand. As you might
     expect, the CPUs on a particular board can access the memory on that board with
     less delay than they can access memory on other boards in the system. Systems
     in which memory access times vary significantly are known collectively as
     non-uniform memory access (NUMA) systems, and without exception, they
     are slower than systems in which memory and CPUs are located on the same
     motherboard.
     Managing which page frames are stored at which locations can significantly
     affect performance in NUMA systems. If we treat memory as uniform in such
     a system, CPUs may wait significantly longer for memory access than if we
     modify memory allocation algorithms to take NUMA into account. Similar
     changes must be made to the scheduling system. The goal of these changes is
     to have memory frames allocated "as close as possible" to the CPU on which
     the process is running. The definition of "close" is "with minimum latency,"
     which typically means on the same system board as the CPU.
     The algorithmic changes consist of having the scheduler track the last CPU
     on which each process ran. If the scheduler tries to schedule each process onto
     its previous CPU, and the memory-management system tries to allocate frames
     for the process close to the CPU on which it is being scheduled, then improved
     cache hits and decreased memory access times will result.
     The picture is more complicated once threads are added. For example, a
     process with many running threads may end up with those threads scheduled
     on many different system boards. How is the memory to be allocated in this
     case? Solaris solves the problem by creating lgroups (for "latency groups") in
     the kernel. Each lgroup gathers together close CPUs and memory. In fact, there
     is a hierarchy of lgroups based on the amount of latency between the groups.
     Solaris tries to schedule all threads of a process and allocate all memory of a
     process within an lgroup. If that is not possible, it picks nearby lgroups for the
     rest of the resources needed. This practice minimizes overall memory latency
     and maximizes CPU cache hit rates.
9.6  Thrashing
     If the number of frames allocated to a low-priority process falls below the
     minimum number required by the computer architecture, we must suspend
     that process's execution. We should then page out its remaining pages, freeing
     all its allocated frames. This provision introduces a swap-in, swap-out level of
     intermediate CPU scheduling.
     In fact, look at any process that does not have "enough" frames. If the
     process does not have the number of frames it needs to support pages in
     active use, it will quickly page-fault. At this point, it must replace some page.
     However, since all its pages are in active use, it must replace a page that will
     be needed again right away. Consequently, it quickly faults again, and again,
     and again, replacing pages that it must bring back in immediately.



426  Chapter 9  Virtual Memory
     This high paging activity is called thrashing. A process is thrashing if it is
     spending more time paging than executing.
     9.6.1  Cause of Thrashing
     Thrashing results in severe performance problems. Consider the following
     scenario, which is based on the actual behavior of early paging systems.
     The operating system monitors CPU utilization. If CPU utilization is too low,
     we increase the degree of multiprogramming by introducing a new process
     to the system. A global page-replacement algorithm is used; it replaces pages
     without regard to the process to which they belong. Now suppose that a process
     enters a new phase in its execution and needs more frames. It starts faulting and
     taking frames away from other processes. These processes need those pages,
     however, and so they also fault, taking frames from other processes. These
     faulting processes must use the paging device to swap pages in and out. As
     they queue up for the paging device, the ready queue empties. As processes
     wait for the paging device, CPU utilization decreases.
     The CPU scheduler sees the decreasing CPU utilization and increases the
     degree of multiprogramming as a result. The new process tries to get started by
     taking frames from running processes, causing more page faults and a longer
     queue for the paging device. As a result, CPU utilization drops even further,
     and the CPU scheduler tries to increase the degree of multiprogramming even
     more. Thrashing has occurred, and system throughput plunges. The page-
     fault rate increases tremendously. As a result, the effective memory-access
     time increases. No work is getting done, because the processes are spending
     all their time paging.
     This phenomenon is illustrated in Figure 9.18, in which CPU utilization
     is plotted against the degree of multiprogramming. As the degree of multi-
     programming increases, CPU utilization also increases, although more slowly,
     until a maximum is reached. If the degree of multiprogramming is increased
     even further, thrashing sets in, and CPU utilization drops sharply. At this point,
     to increase CPU utilization and stop thrashing, we must decrease the degree of
     multiprogramming.
                CPU utilization                              thrashing
                                 degree of multiprogramming
                                 Figure 9.18  Thrashing.



                                                           9.6  Thrashing             427
We can limit the effects of thrashing by using a local replacement algorithm
(or priority replacement algorithm). With local replacement, if one process
starts thrashing, it cannot steal frames from another process and cause the latter
to thrash as well. However, the problem is not entirely solved. If processes are
thrashing, they will be in the queue for the paging device most of the time. The
average service time for a page fault will increase because of the longer average
queue for the paging device. Thus, the effective access time will increase even
for a process that is not thrashing.
To prevent thrashing, we must provide a process with as many frames as
it needs. But how do we know how many frames it "needs"? There are several
techniques. The working-set strategy (Section 9.6.2) starts by looking at how
many frames a process is actually using. This approach defines the locality
model of process execution.
The locality model states that, as a process executes, it moves from locality
to locality. A locality is a set of pages that are actively used together (Figure
9.19). A program is generally composed of several different localities, which
may overlap.
For example, when a function is called, it defines a new locality. In this
locality, memory references are made to the instructions of the function call, its
local variables, and a subset of the global variables. When we exit the function,
the process leaves this locality, since the local variables and instructions of the
function are no longer in active use. We may return to this locality later.
Thus, we see that localities are defined by the program structure and its
data structures. The locality model states that all programs will exhibit this
basic memory reference structure. Note that the locality model is the unstated
principle behind the caching discussions so far in this book. If accesses to any
types of data were random rather than patterned, caching would be useless.
Suppose we allocate enough frames to a process to accommodate its current
locality. It will fault for the pages in its locality until all these pages are in
memory; then, it will not fault again until it changes localities. If we do not
allocate enough frames to accommodate the size of the current locality, the
process will thrash, since it cannot keep in memory all the pages that it is
actively using.
9.6.2     Working-Set Model
As mentioned, the working-set model is based on the assumption of locality.
This model uses a parameter,   , to define the working-set window. The idea
is to examine the most recent         page references. The set of pages in the most
recent    page references is the working set (Figure 9.20). If a page is in active
use, it will be in the working set. If it is no longer being used, it will drop from
the working set  time units after its last reference. Thus, the working set is an
approximation of the program's locality.
For example, given the sequence of memory references shown in Figure
9.20, if  = 10 memory references, then the working set at time t1 is {1, 2, 5,
6, 7}. By time t2, the working set has changed to {3, 4}.
The accuracy of the working set depends on the selection of     . If         is too
small, it will not encompass the entire locality; if  is too large, it may overlap
several localities. In the extreme, if    is infinite, the working set is the set of
pages touched during the process execution.



428  Chapter 9           Virtual Memory
                     34
                     32
                     30
                     28
     memory address  26
                     24
                     22
     page numbers    20
                     18
                         execution time
                         Figure 9.19     Locality in a memory-reference pattern.
     The most important property of the working set, then, is its size. If we
     compute the working-set size, WSSi , for each process in the system, we can
     then consider that
                                         D=  WSSi ,
     where D is the total demand for frames. Each process is actively using the pages
     in its working set. Thus, process i needs WSSi frames. If the total demand is
     greater than the total number of available frames (D > m), thrashing will occur,
     because some processes will not have enough frames.
     Once                has been selected, use of the working-set model is simple. The
     operating system monitors the working set of each process and allocates to



                                                                              9.6   Thrashing           429
page       reference table
...        26157777516                2  3  4  1  2  3  44434      344    41  323  4443444     .  .  .
                                                        
                                  t1                                      t2
           WS(t 1) = {1,2,5,6,7}                        WS(t 2) =  {3,4}
                                  Figure 9.20           Working-set model.
that working set enough frames to provide it with its working-set size. If there
are enough extra frames, another process can be initiated. If the sum of the
working-set sizes increases, exceeding the total number of available frames,
the operating system selects a process to suspend. The process's pages are
written out (swapped), and its frames are reallocated to other processes. The
suspended process can be restarted later.
This working-set strategy prevents thrashing while keeping the degree of
multiprogramming as high as possible. Thus, it optimizes CPU utilization. The
difficulty with the working-set model is keeping track of the working set. The
working-set window is a moving window. At each memory reference, a new
reference appears at one end, and the oldest reference drops off the other end.
A page is in the working set if it is referenced anywhere in the working-set
window.
We can approximate the working-set model with a fixed-interval timer
interrupt  and  a   reference         bit.     For      example,   assume     that  equals 10,000
references and that we can cause a timer interrupt every 5,000 references.
When we get a timer interrupt, we copy and clear the reference-bit values for
each page. Thus, if a page fault occurs, we can examine the current reference
bit and two in-memory bits to determine whether a page was used within the
last 10,000 to 15,000 references. If it was used, at least one of these bits will be
on. If it has not been used, these bits will be off. Pages with at least one bit on
will be considered to be in the working set.
Note that this arrangement is not entirely accurate, because we cannot
tell where, within an interval of 5,000, a reference occurred. We can reduce the
uncertainty by increasing the number of history bits and the frequency of inter-
rupts (for example, 10 bits and interrupts every 1,000 references). However, the
cost to service these more frequent interrupts will be correspondingly higher.
9.6.3    Page-Fault Frequency
The working-set model is successful, and knowledge of the working set can
be useful for prepaging (Section 9.9.1), but it seems a clumsy way to control
thrashing. A strategy that uses the page-fault frequency (PFF) takes a more
direct approach.
The specific problem is how to prevent thrashing. Thrashing has a high
page-fault rate. Thus, we want to control the page-fault rate. When it is too
high, we know that the process needs more frames. Conversely, if the page-fault
rate is too low, then the process may have too many frames. We can establish
upper and lower bounds on the desired page-fault rate (Figure 9.21). If the
actual page-fault rate exceeds the upper limit, we allocate the process another



430  Chapter 9        Virtual Memory
     page-fault rate                                             increase number
                                                                 of frames
                                                         upper bound
                                                         lower bound
                                                                 decrease number
                                                                 of frames
                                       number of frames
                      Figure 9.21         Page-fault frequency.
     frame. If the page-fault rate falls below the lower limit, we remove a frame
     from the process. Thus, we can directly measure and control the page-fault
     rate to prevent thrashing.
     As with the working-set strategy, we may have to swap out a process. If the
     page-fault rate increases and no free frames are available, we must select some
     process and swap it out to backing store. The freed frames are then distributed
     to processes with high page-fault rates.
     9.6.4            Concluding Remarks
     Practically speaking, thrashing and the resulting swapping have a disagreeably
     large impact on performance. The current best practice in implementing a
     computer facility is to include enough physical memory, whenever possible,
     to avoid thrashing and swapping. From smartphones through mainframes,
     providing enough memory to keep all working sets in memory concurrently,
     except under extreme conditions, gives the best user experience.
9.7  Memory-Mapped Files
     Consider a sequential read of a file on disk using the standard system calls
     open(), read(), and write(). Each file access requires a system call and disk
     access. Alternatively, we can use the virtual memory techniques discussed
     so far to treat file I/O as routine memory accesses. This approach, known as
     memory mapping a file, allows a part of the virtual address space to be logically
     associated with the file. As we shall see, this can lead to significant performance
     increases.
     9.7.1            Basic Mechanism
     Memory mapping a file is accomplished by mapping a disk block to a page (or
     pages) in memory. Initial access to the file proceeds through ordinary demand
     paging, resulting in a page fault. However, a page-sized portion of the file is
     read from the file system into a physical page (some systems may opt to read



                                             9.7        Memory-Mapped Files        431
             WORKING SETS AND PAGE-FAULT RATES
There is a direct relationship between the working set of a process and its
page-fault rate. Typically, as shown in Figure 9.20, the working set of a process
changes over time as references to data and code sections move from one
locality to another. Assuming there is sufficient memory to store the working
set of a process (that is, the process is not thrashing), the page-fault rate of
the process will transition between peaks and valleys over time. This general
behavior is shown below:
                                       working set
       1
page
fault
rate
       0
                                       time
A peak in the page-fault rate occurs when we begin demand-paging a new
locality. However, once the working set of this new locality is in memory,
the page-fault rate falls. When the process moves to a new working set, the
page-fault rate rises toward a peak once again, returning to a lower rate once
the new working set is loaded into memory. The span of time between the
start of one peak and the start of the next peak represents the transition from
one working set to another.
in more than a page-sized chunk of memory at a time). Subsequent reads and
writes to the file are handled as routine memory accesses. Manipulating files
through memory rather than incurring the overhead of using the read() and
write() system calls simplifies and speeds up file access and usage.
Note   that  writes     to  the  file  mapped       in  memory  are  not  necessarily
immediate (synchronous) writes to the file on disk. Some systems may choose
to update the physical file when the operating system periodically checks
whether the page in memory has been modified. When the file is closed, all the
memory-mapped data are written back to disk and removed from the virtual
memory of the process.
Some operating systems provide memory mapping only through a specific
system call and use the standard system calls to perform all other file I/O.
However, some systems choose to memory-map a file regardless of whether
the file was specified as memory-mapped. Let's take Solaris as an example. If
a file is specified as memory-mapped (using the mmap() system call), Solaris
maps the file into the address space of the process. If a file is opened and
accessed using ordinary system calls, such as open(), read(), and write(),



432  Chapter 9  Virtual Memory
                                                              1
                                                              2
                                                              3
                1                                             4
                2                          3                  5
                3                                             6
                4
                5                          6
                6
     process A                             1                  process B
     virtual memory                        5                  virtual memory
                                           4
                                           2
                                   physical memory
                                1    2  3     4    5  6
                                        disk file
                        Figure 9.22     Memory-mapped files.
     Solaris still memory-maps the file; however, the file is mapped to the kernel
     address space. Regardless of how the file is opened, then, Solaris treats all
     file I/O as memory-mapped, allowing file access to take place via the efficient
     memory subsystem.
     Multiple processes may be allowed to map the same file concurrently,
     to allow sharing of data. Writes by any of the processes modify the data in
     virtual memory and can be seen by all others that map the same section of
     the file. Given our earlier discussions of virtual memory, it should be clear
     how the sharing of memory-mapped sections of memory is implemented:
     the virtual memory map of each sharing process points to the same page of
     physical memory--the page that holds a copy of the disk block. This memory
     sharing is illustrated in Figure 9.22. The memory-mapping system calls can
     also support copy-on-write functionality, allowing processes to share a file in
     read-only mode but to have their own copies of any data they modify. So that
     access to the shared data is coordinated, the processes involved might use one
     of the mechanisms for achieving mutual exclusion described in Chapter 5.
     Quite often, shared memory is in fact implemented by memory mapping
     files. Under this scenario, processes can communicate using shared memory
     by having the communicating processes memory-map the same file into their
     virtual address spaces. The memory-mapped file serves as the region of shared
     memory between the communicating processes (Figure 9.23). We have already
     seen this in Section 3.4.1, where a POSIX shared memory object is created and
     each communicating process memory-maps the object into its address space.
     In the following section, we illustrate support in the Windows API for shared
     memory using memory-mapped files.



                                             9.7     Memory-Mapped Files          433
       process1                                                       process2
         shared               memory-mapped
         memory               file
                              shared
                              memory
                                                                      shared
                                                                      memory
                 Figure 9.23  Shared memory using memory-mapped I/O.
9.7.2    Shared Memory in the Windows API
The general outline for creating a region of shared memory using memory-
mapped files in the Windows API involves first creating a file mapping for the
file to be mapped and then establishing a view of the mapped file in a process's
virtual address space. A second process can then open and create a view of
the mapped file in its virtual address space. The mapped file represents the
shared-memory object that will enable communication to take place between
the processes.
We next illustrate these steps in more detail. In this example, a producer
process  first   creates  a   shared-memory  object  using  the  memory-mapping
features available in the Windows API. The producer then writes a message
to shared memory. After that, a consumer process opens a mapping to the
shared-memory object and reads the message written by the consumer.
To establish a memory-mapped file, a process first opens the file to be
mapped with the CreateFile() function, which returns a HANDLE to the
opened file. The process then creates a mapping of this file HANDLE using
the CreateFileMapping() function. Once the file mapping is established, the
process then establishes a view of the mapped file in its virtual address space
with the MapViewOfFile() function. The view of the mapped file represents
the portion of the file being mapped in the virtual address space of the process
--the entire file or only a portion of it may be mapped. We illustrate this
sequence in the program shown in Figure 9.24. (We eliminate much of the error
checking for code brevity.)
The call to CreateFileMapping() creates a named shared-memory object
called SharedObject. The consumer process will communicate using this
shared-memory segment by creating a mapping to the same named object.
The producer then creates a view of the memory-mapped file in its virtual
address space. By passing the last three parameters the value 0, it indicates
that the mapped view is the entire file. It could instead have passed values
specifying an offset and size, thus creating a view containing only a subsection
of the file. (It is important to note that the entire mapping may not be loaded
into memory when the mapping is established. Rather, the mapped file may be
demand-paged, thus bringing pages into memory only as they are accessed.)
The MapViewOfFile() function returns a pointer to the shared-memory object;
any accesses to this memory location are thus accesses to the memory-mapped



434  Chapter 9    Virtual Memory
     #include    <windows.h>
     #include    <stdio.h>
     int  main(int       argc,  char    *argv[])
     {
        HANDLE      hFile,     hMapFile;
        LPVOID      lpMapAddress;
        hFile    =   CreateFile("temp.txt",              /*     file  name  */
            GENERIC READ | GENERIC WRITE, /* read/write access */
            0,   /*  no   sharing     of    the  file       */
            NULL,    /*   default     security       */
            OPEN    ALWAYS,     /*  open    new     or  existing      file  */
            FILE    ATTRIBUTE   NORMAL,         /*   routine    file      attributes  */
            NULL);   /*   no    file    template        */
        hMapFile     =   CreateFileMapping(hFile,               /*    file  handle    */
            NULL,    /*   default     security       */
            PAGE    READWRITE,      /*  read/write          access    to  mapped  pages   */
            0,   /*  map  entire      file  */
            0,
            TEXT("SharedObject"));               /*  named      shared    memory  object  */
        lpMapAddress        =  MapViewOfFile(hMapFile,                /*  mapped  object  handle   */
            FILE    MAP  ALL   ACCESS,      /*   read/write     access      */
            0,   /*  mapped     view    of  entire       file   */
            0,
            0);
        /*  write    to   shared    memory       */
        sprintf(lpMapAddress,"Shared memory message");
        UnmapViewOfFile(lpMapAddress);
        CloseHandle(hFile);
        CloseHandle(hMapFile);
     }
            Figure 9.24   Producer writing to shared memory using the Windows API.
     file. In this instance, the producer process writes the message "Shared              memory
     message" to shared memory.
        A program illustrating how the consumer process establishes a view of
     the named shared-memory object is shown in Figure 9.25. This program is
     somewhat simpler than the one shown in Figure 9.24, as all that is necessary
     is for the process to create a mapping to the existing named shared-memory
     object. The consumer process must also create a view of the mapped file, just
     as the producer process did in the program in Figure 9.24. The consumer then
     reads from shared memory the message "Shared                     memory    message" that was
     written by the producer process.



                                                9.7   Memory-Mapped Files                435
#include <windows.h>
#include <stdio.h>
int  main(int      argc,  char  *argv[])
{
   HANDLE    hMapFile;
   LPVOID    lpMapAddress;
   hMapFile     =  OpenFileMapping(FILE MAP ALL ACCESS,           /*  R/W  access        */
       FALSE,     /*  no  inheritance       */
       TEXT("SharedObject"));           /*  name  of  mapped  file    object   */
   lpMapAddress       =   MapViewOfFile(hMapFile,         /*  mapped  object   handle         */
       FILE  MAP   ALL    ACCESS,   /*  read/write    access  */
       0,   /*  mapped    view  of  entire      file  */
       0,
       0);
   /*  read     from  shared    memory  */
   printf("Read       message   %s",    lpMapAddress);
   UnmapViewOfFile(lpMapAddress);
   CloseHandle(hMapFile);
}
     Figure 9.25   Consumer reading from shared memory using the Windows API.
   Finally, both processes remove the view of the mapped file with a call to
UnmapViewOfFile(). We provide a programming exercise at the end of this
chapter using shared memory with memory mapping in the Windows API.
9.7.3  Memory-Mapped I/O
In the case of I/O, as mentioned in Section 1.2.1, each I/O controller includes
registers to hold commands and the data being transferred. Usually, special I/O
instructions allow data transfers between these registers and system memory.
To allow more convenient access to I/O devices, many computer architectures
provide memory-mapped I/O. In this case, ranges of memory addresses are
set aside and are mapped to the device registers. Reads and writes to these
memory addresses cause the data to be transferred to and from the device
registers. This method is appropriate for devices that have fast response times,
such as video controllers. In the IBM PC, each location on the screen is mapped
to a memory location. Displaying text on the screen is almost as easy as writing
the text into the appropriate memory-mapped locations.
   Memory-mapped I/O is also convenient for other devices, such as the serial
and parallel ports used to connect modems and printers to a computer. The
CPU transfers data through these kinds of devices by reading and writing a few
device registers, called an I/O port. To send out a long string of bytes through a
memory-mapped serial port, the CPU writes one data byte to the data register
and sets a bit in the control register to signal that the byte is available. The device



436  Chapter 9  Virtual Memory
     takes the data byte and then clears the bit in the control register to signal that
     it is ready for the next byte. Then the CPU can transfer the next byte. If the
     CPU uses polling to watch the control bit, constantly looping to see whether
     the device is ready, this method of operation is called programmed I/O (PIO).
     If the CPU does not poll the control bit, but instead receives an interrupt when
     the device is ready for the next byte, the data transfer is said to be interrupt
     driven.
9.8  Allocating Kernel Memory
     When a process running in user mode requests additional memory, pages
     are allocated from the list of free page frames maintained by the kernel.
     This list is typically populated using a page-replacement algorithm such as
     those discussed in Section 9.4 and most likely contains free pages scattered
     throughout physical memory, as explained earlier. Remember, too, that if a
     user process requests a single byte of memory, internal fragmentation will
     result, as the process will be granted an entire page frame.
         Kernel memory is often allocated from a free-memory pool different from
     the list used to satisfy ordinary user-mode processes. There are two primary
     reasons for this:
     1.  The kernel requests memory for data structures of varying sizes, some of
         which are less than a page in size. As a result, the kernel must use memory
         conservatively and attempt to minimize waste due to fragmentation. This
         is especially important because many operating systems do not subject
         kernel code or data to the paging system.
     2.  Pages allocated to user-mode processes do not necessarily have to be in
         contiguous physical memory. However, certain hardware devices interact
         directly with physical memory--without the benefit of a virtual memory
         interface--and consequently may require memory residing in physically
         contiguous pages.
     In the following sections, we examine two strategies for managing free memory
     that is assigned to kernel processes: the "buddy system" and slab allocation.
     9.8.1    Buddy System
     The buddy system allocates memory from a fixed-size segment consisting of
     physically contiguous pages. Memory is allocated from this segment using a
     power-of-2 allocator, which satisfies requests in units sized as a power of 2
     (4 KB, 8 KB, 16 KB, and so forth). A request in units not appropriately sized is
     rounded up to the next highest power of 2. For example, a request for 11 KB is
     satisfied with a 16-KB segment.
         Let's consider a simple example. Assume the size of a memory segment
     is initially 256 KB and the kernel requests 21 KB of memory. The segment is
     initially divided into two buddies--which we will call AL and AR --each 128
     KB in size. One of these buddies is further divided into two 64-KB buddies--
     BL and BR. However, the next-highest power of 2 from 21 KB is 32 KB so either
     BL or BR is again divided into two 32-KB buddies, CL and CR. One of these



                                            9.8     Allocating Kernel Memory        437
                                   physically contiguous pages
                                            256 KB
                           128 KB                               128 KB
                               AL                               AR
                64 KB              64 KB
                       BL               BR
                32 KB      32 KB
                CL         CR
                           Figure 9.26      Buddy system allocation.
buddies is used to satisfy the 21-KB request. This scheme is illustrated in Figure
9.26, where CL is the segment allocated to the 21-KB request.
An advantage of the buddy system is how quickly adjacent buddies can be
combined to form larger segments using a technique known as coalescing. In
Figure 9.26, for example, when the kernel releases the CL unit it was allocated,
the system can coalesce CL and CR into a 64-KB segment. This segment, BL, can
in turn be coalesced with its buddy BR to form a 128-KB segment. Ultimately,
we can end up with the original 256-KB segment.
The obvious drawback to the buddy system is that rounding up to the
next highest power of 2 is very likely to cause fragmentation within allocated
segments. For example, a 33-KB request can only be satisfied with a 64-
KB segment. In fact, we cannot guarantee that less than 50 percent of the
allocated unit will be wasted due to internal fragmentation. In the following
section, we explore a memory allocation scheme where no space is lost due to
fragmentation.
9.8.2  Slab Allocation
A second strategy for allocating kernel memory is known as slab allocation. A
slab is made up of one or more physically contiguous pages. A cache consists of
one or more slabs. There is a single cache for each unique kernel data structure
--for example, a separate cache for the data structure representing process
descriptors, a separate cache for file objects, a separate cache for semaphores,
and so forth. Each cache is populated with objects that are instantiations of the
kernel data structure the cache represents. For example, the cache representing
semaphores  stores     instances   of   semaphore   objects,        the  cache  representing
process descriptors stores instances of process descriptor objects, and so forth.
The relationship among slabs, caches, and objects is shown in Figure 9.27. The
figure shows two kernel objects 3 KB in size and three objects 7 KB in size, each
stored in a separate cache.



438  Chapter  9   Virtual Memory
                  kernel objects              caches             slabs
                  3-KB
                 objects
                                                                         physically
                                                                         contiguous
                                                                         pages
                  7-KB
                 objects
                                    Figure 9.27       Slab allocation.
            The slab-allocation algorithm uses caches to store kernel objects. When a
     cache is created, a number of objects--which are initially marked as free--are
     allocated to the cache. The number of objects in the cache depends on the size
     of the associated slab. For example, a 12-KB slab (made up of three continguous
     4-KB pages) could store six 2-KB objects. Initially, all objects in the cache are
     marked as free. When a new object for a kernel data structure is needed, the
     allocator can assign any free object from the cache to satisfy the request. The
     object assigned from the cache is marked as used.
            Let's consider a scenario in which the kernel requests memory from the
     slab allocator for an object representing a process descriptor. In Linux systems,
     a  process   descriptor    is  of   the  type    struct  task      struct,  which  requires
     approximately 1.7 KB of memory. When the Linux kernel creates a new task,
     it requests the necessary memory for the struct             task   struct object from its
     cache.  The  cache   will  fulfill  the  request using   a  struct  task    struct  object
     that has already been allocated in a slab and is marked as free.
            In Linux, a slab may be in one of three possible states:
        1.  Full. All objects in the slab are marked as used.
        2.  Empty. All objects in the slab are marked as free.
        3.  Partial. The slab consists of both used and free objects.
     The slab allocator first attempts to satisfy the request with a free object in a
     partial slab. If none exists, a free object is assigned from an empty slab. If no
     empty slabs are available, a new slab is allocated from contiguous physical
     pages and assigned to a cache; memory for the object is allocated from this
     slab.
            The slab allocator provides two main benefits:
        1.  No memory is wasted due to fragmentation. Fragmentation is not an
            issue because each unique kernel data structure has an associated cache,
            and each cache is made up of one or more slabs that are divided into



                                                         9.9   Other Considerations          439
           chunks the size of the objects being represented. Thus, when the kernel
           requests memory for an object, the slab allocator returns the exact amount
           of memory required to represent the object.
     2.    Memory requests can be satisfied quickly. The slab allocation scheme
           is thus particularly effective for managing memory when objects are
           frequently allocated and deallocated, as is often the case with requests
           from the kernel. The act of allocating--and releasing--memory can be
           a time-consuming process. However, objects are created in advance and
           thus can be quickly allocated from the cache. Furthermore, when the
           kernel has finished with an object and releases it, it is marked as free and
           returned to its cache, thus making it immediately available for subsequent
           requests from the kernel.
         The slab allocator first appeared in the Solaris 2.4 kernel. Because of its
     general-purpose nature, this allocator is now also used for certain user-mode
     memory requests in Solaris. Linux originally used the buddy system; however,
     beginning with Version 2.2, the Linux kernel adopted the slab allocator.
         Recent distributions of Linux now include two other kernel memory allo-
     cators--the SLOB and SLUB allocators. (Linux refers to its slab implementation
     as SLAB.)
         The SLOB allocator is designed for systems with a limited amount of
     memory, such as embedded systems. SLOB (which stands for Simple List of
     Blocks) works by maintaining three lists of objects: small (for objects less than
     256 bytes), medium (for objects less than 1,024 bytes), and large (for objects
     less than 1,024 bytes). Memory requests are allocated from an object on an
     appropriately sized list using a first-fit policy.
         Beginning with Version 2.6.24, the SLUB allocator replaced SLAB as the
     default allocator for the Linux kernel. SLUB addresses performance issues
     with   slab  allocation  by  reducing    much     of   the  overhead   required    by   the
     SLAB   allocator.  One   change  is  to  move     the    metadata  that     is  stored  with
     each   slab  under  SLAB     allocation  to  the    page    structure  the  Linux  kernel
     uses for each page. Additionally, SLUB removes the per-CPU queues that the
     SLAB allocator maintains for objects in each cache. For systems with a large
     number of processors, the amount of memory allocated to these queues was
     not insignificant. Thus, SLUB provides better performance as the number of
     processors on a system increases.
9.9  Other Considerations
     The major decisions that we make for a paging system are the selections of
     a replacement algorithm and an allocation policy, which we discussed earlier
     in this chapter. There are many other considerations as well, and we discuss
     several of them here.
     9.9.1  Prepaging
     An obvious property of pure demand paging is the large number of page faults
     that occur when a process is started. This situation results from trying to get the
     initial locality into memory. The same situation may arise at other times. For



440  Chapter 9  Virtual Memory
     instance, when a swapped-out process is restarted, all its pages are on the disk,
     and each must be brought in by its own page fault. Prepaging is an attempt to
     prevent this high level of initial paging. The strategy is to bring into memory at
     one time all the pages that will be needed. Some operating systems--notably
     Solaris--prepage the page frames for small files.
     In a system using the working-set model, for example, we could keep with
     each process a list of the pages in its working set. If we must suspend a process
     (due to an I/O wait or a lack of free frames), we remember the working set for
     that process. When the process is to be resumed (because I/O has finished or
     enough free frames have become available), we automatically bring back into
     memory its entire working set before restarting the process.
     Prepaging may offer an advantage in some cases. The question is simply
     whether the cost of using prepaging is less than the cost of servicing the
     corresponding page faults. It may well be the case that many of the pages
     brought back into memory by prepaging will not be used.
     Assume that s pages are prepaged and a fraction  of these s pages is
     actually used (0    1). The question is whether the cost of the s *  saved
     page faults is greater or less than the cost of prepaging s * (1 - ) unnecessary
     pages. If  is close to 0, prepaging loses; if  is close to 1, prepaging wins.
     9.9.2  Page Size
     The designers of an operating system for an existing machine seldom have
     a choice concerning the page size. However, when new machines are being
     designed, a decision regarding the best page size must be made. As you might
     expect, there is no single best page size. Rather, there is a set of factors that
     support various sizes. Page sizes are invariably powers of 2, generally ranging
     from 4,096 (212) to 4,194,304 (222) bytes.
     How do we select a page size? One concern is the size of the page table. For
     a given virtual memory space, decreasing the page size increases the number
     of pages and hence the size of the page table. For a virtual memory of 4 MB
     (222), for example, there would be 4,096 pages of 1,024 bytes but only 512 pages
     of 8,192 bytes. Because each active process must have its own copy of the page
     table, a large page size is desirable.
     Memory is better utilized with smaller pages, however. If a process is
     allocated memory starting at location 00000 and continuing until it has as much
     as it needs, it probably will not end exactly on a page boundary. Thus, a part
     of the final page must be allocated (because pages are the units of allocation)
     but will be unused (creating internal fragmentation). Assuming independence
     of process size and page size, we can expect that, on the average, half of the
     final page of each process will be wasted. This loss is only 256 bytes for a page
     of 512 bytes but is 4,096 bytes for a page of 8,192 bytes. To minimize internal
     fragmentation, then, we need a small page size.
     Another problem is the time required to read or write a page. I/O time is
     composed of seek, latency, and transfer times. Transfer time is proportional to
     the amount transferred (that is, the page size)--a fact that would seem to argue
     for a small page size. However, as we shall see in Section 10.1.1, latency and
     seek time normally dwarf transfer time. At a transfer rate of 2 MB per second,
     it takes only 0.2 milliseconds to transfer 512 bytes. Latency time, though, is
     perhaps 8 milliseconds, and seek time 20 milliseconds. Of the total I/O time



                                                 9.9  Other Considerations              441
(28.2 milliseconds), therefore, only 1 percent is attributable to the actual transfer.
Doubling the page size increases I/O time to only 28.4 milliseconds. It takes 28.4
milliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read
the same amount as two pages of 512 bytes each. Thus, a desire to minimize
I/O time argues for a larger page size.
With a smaller page size, though, total I/O should be reduced, since locality
will be improved. A smaller page size allows each page to match program
locality more accurately. For example, consider a process 200 KB in size, of
which only half (100 KB) is actually used in an execution. If we have only one
large page, we must bring in the entire page, a total of 200 KB transferred and
allocated. If instead we had pages of only 1 byte, then we could bring in only
the 100 KB that are actually used, resulting in only 100 KB transferred and
allocated. With a smaller page size, then, we have better resolution, allowing
us to isolate only the memory that is actually needed. With a larger page size,
we must allocate and transfer not only what is needed but also anything else
that happens to be in the page, whether it is needed or not. Thus, a smaller
page size should result in less I/O and less total allocated memory.
But did you notice that with a page size of 1 byte, we would have a page
fault for each byte? A process of 200 KB that used only half of that memory
would generate only one page fault with a page size of 200 KB but 102,400 page
faults with a page size of 1 byte. Each page fault generates the large amount
of overhead needed for processing the interrupt, saving registers, replacing a
page, queueing for the paging device, and updating tables. To minimize the
number of page faults, we need to have a large page size.
Other factors must be considered as well (such as the relationship between
page size and sector size on the paging device). The problem has no best
answer. As we have seen, some factors (internal fragmentation, locality) argue
for a small page size, whereas others (table size, I/O time) argue for a large
page size. Nevertheless, the historical trend is toward larger page sizes, even
for mobile systems. Indeed, the first edition of Operating System Concepts (1983)
used 4,096 bytes as the upper bound on page sizes, and this value was the most
common page size in 1990. Modern systems may now use much larger page
sizes, as we will see in the following section.
9.9.3  TLB Reach
In Chapter 8, we introduced the hit ratio of the TLB. Recall that the hit ratio
for the TLB refers to the percentage of virtual address translations that are
resolved in the TLB rather than the page table. Clearly, the hit ratio is related
to the number of entries in the TLB, and the way to increase the hit ratio is
by increasing the number of entries in the TLB. This, however, does not come
cheaply, as the associative memory used to construct the TLB is both expensive
and power hungry.
Related to the hit ratio is a similar metric: the TLB reach. The TLB reach refers
to the amount of memory accessible from the TLB and is simply the number
of entries multiplied by the page size. Ideally, the working set for a process is
stored in the TLB. If it is not, the process will spend a considerable amount of
time resolving memory references in the page table rather than the TLB. If we
double the number of entries in the TLB, we double the TLB reach. However,



442  Chapter 9  Virtual Memory
     for some memory-intensive applications, this may still prove insufficient for
     storing the working set.
     Another approach for increasing the TLB reach is to either increase the
     size of the page or provide multiple page sizes. If we increase the page size
     --say, from 8 KB to 32 KB--we quadruple the TLB reach. However, this may
     lead to an increase in fragmentation for some applications that do not require
     such a large page size. Alternatively, an operating system may provide several
     different page sizes. For example, the UltraSPARC supports page sizes of 8 KB,
     64 KB, 512 KB, and 4 MB. Of these available pages sizes, Solaris uses both 8-KB
     and 4-MB page sizes. And with a 64-entry TLB, the TLB reach for Solaris ranges
     from 512 KB with 8-KB pages to 256 MB with 4-MB pages. For the majority of
     applications, the 8-KB page size is sufficient, although Solaris maps the first 4 MB
     of kernel code and data with two 4-MB pages. Solaris also allows applications
     --such as databases--to take advantage of the large 4-MB page size.
     Providing support for multiple page sizes requires the operating system
     --not hardware --to manage the TLB. For example, one of the fields in a TLB
     entry must indicate the size of the page frame corresponding to the TLB entry.
     Managing the TLB in software and not hardware comes at a cost in performance.
     However, the increased hit ratio and TLB reach offset the performance costs.
     Indeed, recent trends indicate a move toward software-managed TLBs and
     operating-system support for multiple page sizes.
     9.9.4  Inverted Page Tables
     Section 8.6.3 introduced the concept of the inverted page table. The purpose
     of this form of page management is to reduce the amount of physical memory
     needed to track virtual-to-physical address translations. We accomplish this
     savings by creating a table that has one entry per page of physical memory,
     indexed by the pair <process-id, page-number>.
     Because they keep information about which virtual memory page is stored
     in each physical frame, inverted page tables reduce the amount of physical
     memory needed to store this information. However, the inverted page table
     no longer contains complete information about the logical address space of a
     process, and that information is required if a referenced page is not currently
     in memory. Demand paging requires this information to process page faults.
     For the information to be available, an external page table (one per process)
     must be kept. Each such table looks like the traditional per-process page table
     and contains information on where each virtual page is located.
     But do external page tables negate the utility of inverted page tables? Since
     these tables are referenced only when a page fault occurs, they do not need to
     be available quickly. Instead, they are themselves paged in and out of memory
     as necessary. Unfortunately, a page fault may now cause the virtual memory
     manager to generate another page fault as it pages in the external page table it
     needs to locate the virtual page on the backing store. This special case requires
     careful handling in the kernel and a delay in the page-lookup processing.
     9.9.5  Program Structure
     Demand paging is designed to be transparent to the user program. In many
     cases, the user is completely unaware of the paged nature of memory. In other



                                                        9.9   Other Considerations        443
cases, however, system performance can be improved if the user (or compiler)
has an awareness of the underlying demand paging.
Let's look at a contrived but informative example. Assume that pages are
128 words in size. Consider a C program whose function is to initialize to 0
each element of a 128-by-128 array. The following code is typical:
                int    i,  j;
                int[128][128] data;
                for    (j  =   0;     j  <  128;    j++)
                     for    (i     =  0;    i  <  128;  i++)
                           data[i][j]          =  0;
Notice  that  the    array     is  stored      row    major;  that  is,  the  array  is  stored
data[0][0], data[0][1], · · ·, data[0][127], data[1][0], data[1][1], · · ·,
data[127][127]. For pages of 128 words, each row takes one page. Thus,
the preceding code zeros one word in each page, then another word in each
page, and so on. If the operating system allocates fewer than 128 frames to the
entire program, then its execution will result in 128 × 128 = 16,384 page faults.
In contrast, suppose we change the code to
                int    i,  j;
                int[128][128] data;
                for    (i  =   0;     i  <  128;    i++)
                     for    (j     =  0;    j  <  128;  j++)
                           data[i][j]          =  0;
This code zeros all the words on one page before starting the next page,
reducing the number of page faults to 128.
Careful       selection    of   data     structures     and   programming     structures  can
increase locality and hence lower the page-fault rate and the number of pages in
the working set. For example, a stack has good locality, since access is always
made to the top. A hash table, in contrast, is designed to scatter references,
producing bad locality. Of course, locality of reference is just one measure of
the efficiency of the use of a data structure. Other heavily weighted factors
include search speed, total number of memory references, and total number of
pages touched.
At a later stage, the compiler and loader can have a significant effect on
paging. Separating code and data and generating reentrant code means that
code pages can be read-only and hence will never be modified. Clean pages
do not have to be paged out to be replaced. The loader can avoid placing
routines across page boundaries, keeping each routine completely in one page.
Routines that call each other many times can be packed into the same page.
This packaging is a variant of the bin-packing problem of operations research:
try to pack the variable-sized load segments into the fixed-sized pages so that
interpage references are minimized. Such an approach is particularly useful
for large page sizes.



444  Chapter 9  Virtual Memory
     9.9.6  I/O Interlock and Page Locking
     When demand paging is used, we sometimes need to allow some of the pages
     to be locked in memory. One such situation occurs when I/O is done to or from
     user (virtual) memory. I/O is often implemented by a separate I/O processor.
     For example, a controller for a USB storage device is generally given the number
     of bytes to transfer and a memory address for the buffer (Figure 9.28). When
     the transfer is complete, the CPU is interrupted.
     We must be sure the following sequence of events does not occur: A process
     issues an I/O request and is put in a queue for that I/O device. Meanwhile, the
     CPU is given to other processes. These processes cause page faults, and one of
     them, using a global replacement algorithm, replaces the page containing the
     memory buffer for the waiting process. The pages are paged out. Some time
     later, when the I/O request advances to the head of the device queue, the I/O
     occurs to the specified address. However, this frame is now being used for a
     different page belonging to another process.
     There are two common solutions to this problem. One solution is never to
     execute I/O to user memory. Instead, data are always copied between system
     memory and user memory. I/O takes place only between system memory
     and the I/O device. To write a block on tape, we first copy the block to system
     memory and then write it to tape. This extra copying may result in unacceptably
     high overhead.
     Another solution is to allow pages to be locked into memory. Here, a lock
     bit is associated with every frame. If the frame is locked, it cannot be selected
     for replacement. Under this approach, to write a block on tape, we lock into
     memory the pages containing the block. The system can then continue as
     usual. Locked pages cannot be replaced. When the I/O is complete, the pages
     are unlocked.
                         buffer
                                                        disk drive
            Figure 9.28  The reason why frames used for I/O must be in memory.



                                             9.10  Operating-System Examples              445
      Lock bits are used in various situations. Frequently, some or all of the
      operating-system   kernel  is  locked  into  memory.  Many      operating  systems
      cannot tolerate a page fault caused by the kernel or by a specific kernel module,
      including the one performing memory management. User processes may also
      need to lock pages into memory. A database process may want to manage
      a chunk of memory, for example, moving blocks between disk and memory
      itself because it has the best knowledge of how it is going to use its data. Such
      pinning of pages in memory is fairly common, and most operating systems
      have a system call allowing an application to request that a region of its logical
      address space be pinned. Note that this feature could be abused and could
      cause stress on the memory-management algorithms. Therefore, an application
      frequently requires special privileges to make such a request.
      Another use for a lock bit involves normal page replacement. Consider
      the following sequence of events: A low-priority process faults. Selecting a
      replacement frame, the paging system reads the necessary page into memory.
      Ready to continue, the low-priority process enters the ready queue and waits
      for the CPU. Since it is a low-priority process, it may not be selected by the
      CPU scheduler for a time. While the low-priority process waits, a high-priority
      process faults. Looking for a replacement, the paging system sees a page that
      is in memory but has not been referenced or modified: it is the page that the
      low-priority process just brought in. This page looks like a perfect replacement:
      it is clean and will not need to be written out, and it apparently has not been
      used for a long time.
      Whether the high-priority process should be able to replace the low-priority
      process is a policy decision. After all, we are simply delaying the low-priority
      process for the benefit of the high-priority process. However, we are wasting
      the effort spent to bring in the page for the low-priority process. If we decide
      to prevent replacement of a newly brought-in page until it can be used at least
      once, then we can use the lock bit to implement this mechanism. When a page
      is selected for replacement, its lock bit is turned on. It remains on until the
      faulting process is again dispatched.
      Using a lock bit can be dangerous: the lock bit may get turned on but
      never turned off. Should this situation occur (because of a bug in the operating
      system, for example), the locked frame becomes unusable. On a single-user
      system, the overuse of locking would hurt only the user doing the locking.
      Multiuser systems must be less trusting of users. For instance, Solaris allows
      locking "hints," but it is free to disregard these hints if the free-frame pool
      becomes too small or if an individual process requests that too many pages be
      locked in memory.
9.10  Operating-System Examples
      In this section, we describe how Windows     and      Solaris   implement  virtual
      memory.
      9.10.1   Windows
      Windows implements virtual memory using demand paging with clustering.
      Clustering handles page faults by bringing in not only the faulting page but also



446  Chapter 9    Virtual Memory
     several pages following the faulting page. When a process is first created, it is
     assigned a working-set minimum and maximum. The working-set minimum
     is the minimum number of pages the process is guaranteed to have in memory.
     If sufficient memory is available, a process may be assigned as many pages as
     its working-set maximum. (In some circumstances, a process may be allowed
     to exceed its working-set maximum.) The virtual memory manager maintains a
     list of free page frames. Associated with this list is a threshold value that is used
     to indicate whether sufficient free memory is available. If a page fault occurs for
     a process that is below its working-set maximum, the virtual memory manager
     allocates a page from this list of free pages. If a process that is at its working-set
     maximum incurs a page fault, it must select a page for replacement using a
     local LRU page-replacement policy.
     When the amount of free memory falls below the threshold, the virtual
     memory manager uses a tactic known as automatic working-set trimming to
     restore the value above the threshold. Automatic working-set trimming works
     by evaluating the number of pages allocated to processes. If a process has
     been allocated more pages than its working-set minimum, the virtual memory
     manager removes pages until the process reaches its working-set minimum.
     A process that is at its working-set minimum may be allocated pages from
     the free-page-frame list once sufficient free memory is available. Windows
     performs working-set trimming on both user mode and system processes.
     Virtual memory is discussed in great detail in the Windows case study in
     Chapter 19.
     9.10.2     Solaris
     In Solaris, when a thread incurs a page fault, the kernel assigns a page to the
     faulting thread from the list of free pages it maintains. Therefore, it is imperative
     that the kernel keep a sufficient amount of free memory available. Associated
     with this list of free pages is a parameter--lotsfree--that represents a
     threshold to begin paging. The lotsfree parameter is typically set to 1/64
     the size of the physical memory. Four times per second, the kernel checks
     whether the amount of free memory is less than lotsfree. If the number of
     free pages falls below lotsfree, a process known as a pageout starts up. The
     pageout process is similar to the second-chance algorithm described in Section
     9.4.5.2, except that it uses two hands while scanning pages, rather than one.
     The pageout process works as follows: The front hand of the clock scans
     all pages in memory, setting the reference bit to 0. Later, the back hand of the
     clock examines the reference bit for the pages in memory, appending each page
     whose reference bit is still set to 0 to the free list and writing to disk its contents
     if modified. Solaris maintains a cache list of pages that have been "freed" but
     have not yet been overwritten. The free list contains frames that have invalid
     contents. Pages can be reclaimed from the cache list if they are accessed before
     being moved to the free list.
     The pageout algorithm uses several parameters to control the rate at which
     pages are scanned (known as the scanrate). The scanrate is expressed in
     pages per second and ranges from slowscan to fastscan. When free memory
     falls below lotsfree, scanning occurs at slowscan pages per second and
     progresses to fastscan, depending on the amount of free memory available.
     The default value of slowscan is 100 pages per second. Fastscan is typically



                                               9.10     Operating-System Examples     447
set to the value (total physical pages)/2 pages per second, with a maximum of
8,192 pages per second. This is shown in Figure 9.29 (with fastscan set to the
maximum).
The distance (in pages) between the hands of the clock is determined
by a system parameter, handspread. The amount of time between the front
hand's clearing a bit and the back hand's investigating its value depends on
the scanrate and the handspread. If scanrate is 100 pages per second and
handspread is 1,024 pages, 10 seconds can pass between the time a bit is set by
the front hand and the time it is checked by the back hand. However, because
of the demands placed on the memory system, a scanrate of several thousand
is not uncommon. This means that the amount of time between clearing and
investigating a bit is often a few seconds.
As mentioned above, the pageout process checks memory four times per
second. However, if free memory falls below the value of desfree (Figure 9.29),
pageout will run a hundred times per second with the intention of keeping at
least desfree free memory available. If the pageout process is unable to keep
the amount of free memory at desfree for a 30-second average, the kernel
begins swapping processes, thereby freeing all pages allocated to swapped
processes. In general, the kernel looks for processes that have been idle for
long periods of time. If the system is unable to maintain the amount of free
memory at minfree, the pageout process is called for every request for a new
page.
Recent      releases    of       the  Solaris  kernel   have  provided  enhancements  of
the paging algorithm. One such enhancement involves recognizing pages
from shared libraries. Pages belonging to libraries that are being shared by
several processes--even if they are eligible to be claimed by the scanner--
are skipped during the page-scanning process. Another enhancement concerns
       8192
       fastscan
             scan rate
       100
       slowscan
                        minfree                desfree                lotsfree
                                         amount of free memory
                            Figure 9.29        Solaris page scanner.



448   Chapter 9  Virtual Memory
      distinguishing pages that have been allocated to processes from pages allocated
      to regular files. This is known as priority paging and is covered in Section 12.6.2.
9.11  Summary
      It is desirable to be able to execute a process whose logical address space is
      larger than the available physical address space. Virtual memory is a technique
      that enables us to map a large logical address space onto a smaller physical
      memory. Virtual memory allows us to run extremely large processes and to
      raise the degree of multiprogramming, increasing CPU utilization. Further, it
      frees application programmers from worrying about memory availability. In
      addition, with virtual memory, several processes can share system libraries
      and memory. With virtual memory, we can also use an efficient type of process
      creation known as copy-on-write, wherein parent and child processes share
      actual pages of memory.
      Virtual    memory         is  commonly     implemented  by  demand   paging.  Pure
      demand paging never brings in a page until that page is referenced. The first
      reference causes a page fault to the operating system. The operating-system
      kernel consults an internal table to determine where the page is located on the
      backing store. It then finds a free frame and reads the page in from the backing
      store. The page table is updated to reflect this change, and the instruction that
      caused the page fault is restarted. This approach allows a process to run even
      though its entire memory image is not in main memory at once. As long as the
      page-fault rate is reasonably low, performance is acceptable.
      We can use demand paging to reduce the number of frames allocated to
      a process. This arrangement can increase the degree of multiprogramming
      (allowing more processes to be available for execution at one time) and --in
      theory, at least--the CPU utilization of the system. It also allows processes
      to be run even though their memory requirements exceed the total available
      physical memory. Such processes run in virtual memory.
      If total memory requirements exceed the capacity of physical memory,
      then it may be necessary to replace pages from memory to free frames for
      new pages. Various page-replacement algorithms are used. FIFO page replace-
      ment is easy to program but suffers from Belady's anomaly. Optimal page
      replacement requires future knowledge. LRU replacement is an approxima-
      tion of optimal page replacement, but even it may be difficult to implement.
      Most page-replacement algorithms, such as the second-chance algorithm, are
      approximations of LRU replacement.
      In addition to a page-replacement algorithm, a frame-allocation policy
      is needed. Allocation can be fixed, suggesting local page replacement, or
      dynamic, suggesting global replacement. The working-set model assumes that
      processes execute in localities. The working set is the set of pages in the current
      locality. Accordingly, each process should be allocated enough frames for its
      current working set. If a process does not have enough memory for its working
      set, it will thrash. Providing enough frames to each process to avoid thrashing
      may require process swapping and scheduling.
      Most operating systems provide features for memory mapping files, thus
      allowing   file  I/O  to  be  treated  as  routine  memory  access.  The  Win32       API
      implements shared memory through memory mapping of files.



                                                                  Practice Exercises   449
          Kernel processes typically require memory to be allocated using pages
that are physically contiguous. The buddy system allocates memory to kernel
processes in units sized according to a power of 2, which often results in
fragmentation. Slab allocators assign kernel data structures to caches associated
with slabs, which are made up of one or more physically contiguous pages.
With slab allocation, no memory is wasted due to fragmentation, and memory
requests can be satisfied quickly.
          In addition to requiring us to solve the major problems of page replacement
and frame allocation, the proper design of a paging system requires that
we consider prepaging, page size, TLB reach, inverted page tables, program
structure, I/O interlock and page locking, and other issues.
Practice  Exercises
9.1       Under what circumstances do page faults occur? Describe the actions
          taken by the operating system when a page fault occurs.
9.2       Assume that you have a page-reference string for a process with m
          frames (initially all empty). The page-reference string has length p, and
          n distinct page numbers occur in it. Answer these questions for any
          page-replacement algorithms:
          a.  What is a lower bound on the number of page faults?
          b.  What is an upper bound on the number of page faults?
9.3       Consider the page table shown in Figure 9.30 for a system with 12-bit
          virtual and physical addresses and with 256-byte pages. The list of free
          page frames is D, E, F (that is, D is at the head of the list, E is second,
          and F is last).
                           Page               Page Frame
                           0                  ­
                           1                  2
                           2                  C
                           3                  A
                           4                  ­
                           5                  4
                           6                  3
                           7                  ­
                           8                  B
                           9                  0
                           Figure 9.30  Page  table for Exercise  9.3.



450  Chapter 9  Virtual Memory
          Convert the following virtual addresses to their equivalent physical
          addresses in hexadecimal. All numbers are given in hexadecimal. (A
          dash for a page frame indicates that the page is not in memory.)
          ·     9EF
          ·     111
          ·     700
          ·     0FF
     9.4  Consider the following page-replacement algorithms. Rank these algo-
          rithms on a five-point scale from "bad" to "perfect" according to their
          page-fault rate. Separate those algorithms that suffer from Belady's
          anomaly from those that do not.
          a.    LRU replacement
          b.    FIFO replacement
          c.    Optimal replacement
          d.    Second-chance replacement
     9.5  Discuss the hardware support required to support demand paging.
     9.6  An operating system supports a paged virtual memory. The central
          processor has a cycle time of 1 microsecond. It costs an additional 1
          microsecond to access a page other than the current one. Pages have 1,000
          words, and the paging device is a drum that rotates at 3,000 revolutions
          per minute and transfers 1 million words per second. The following
          statistical measurements were obtained from the system:
          ·     One percent of all instructions executed accessed a page other than
                the current page.
          ·     Of the instructions that accessed another page, 80 percent accessed
                a page already in memory.
          ·     When a new page was required, the replaced page was modified 50
                percent of the time.
          Calculate the effective instruction time on this system, assuming that the
          system is running one process only and that the processor is idle during
          drum transfers.
     9.7  Consider the two-dimensional array A:
                int A[][] = new int[100][100];
          where A[0][0] is at location 200 in a paged memory system with pages
          of size 200. A small process that manipulates the matrix resides in page
          0 (locations 0 to 199). Thus, every instruction fetch will be from page 0.
          For three page frames, how many page faults are generated by the
          following array-initialization loops? Use LRU replacement, and assume



                                                               Practice Exercises       451
      that page frame 1 contains the process and the other two are initially
      empty.
      a.   for   (int  j   =  0;    j  <  100;      j++)
                for  (int     i  =  0;    i  <  100;    i++)
                     A[i][j]     =  0;
      b.   for   (int  i   =  0;    i  <  100;      i++)
                for  (int     j  =  0;    j  <  100;    j++)
                     A[i][j]     =  0;
9.8   Consider the following page reference string:
                     1, 2, 3, 4, 2, 1, 5, 6, 2, 1, 2, 3, 7, 6, 3, 2, 1, 2, 3, 6.
      How many page faults would occur for the following replacement
      algorithms, assuming one, two, three, four, five, six, and seven frames?
      Remember that all frames are initially empty, so your first unique pages
      will cost one fault each.
      ·    LRU replacement
      ·    FIFO replacement
      ·    Optimal replacement
9.9   Suppose that you want to use a paging algorithm that requires a reference
      bit (such as second-chance replacement or working-set model), but
      the hardware does not provide one. Sketch how you could simulate a
      reference bit even if one were not provided by the hardware, or explain
      why it is not possible to do so. If it is possible, calculate what the cost
      would be.
9.10  You have devised a new page-replacement algorithm that you think may
      be optimal. In some contorted test cases, Belady's anomaly occurs. Is the
      new algorithm optimal? Explain your answer.
9.11  Segmentation is similar to paging but uses variable-sized "pages." Define
      two  segment-replacement          algorithms,       one  based  on  the     FIFO  page-
      replacement scheme and the other on the LRU page-replacement scheme.
      Remember that since segments are not the same size, the segment that
      is chosen for replacement may be too small to leave enough consecutive
      locations for the needed segment. Consider strategies for systems where
      segments cannot be relocated and strategies for systems where they can.
9.12  Consider a demand-paged computer system where the degree of mul-
      tiprogramming    is  currently         fixed  at  four.  The  system        was  recently
      measured to determine utilization of the CPU and the paging disk. Three
      alternative results are shown below. For each case, what is happening?
      Can the degree of multiprogramming be increased to increase the CPU
      utilization? Is the paging helping?
      a.   CPU utilization 13 percent; disk utilization 97 percent
      b.   CPU utilization 87 percent; disk utilization 3 percent
      c.   CPU utilization 13 percent; disk utilization 3 percent



452  Chapter 9  Virtual Memory
     9.13  We have an operating system for a machine that uses base and limit
           registers, but we have modified the machine to provide a page table.
           Can the page tables be set up to simulate base and limit registers? How
           can they be, or why can they not be?
Exercises
     9.14  Assume that a program has just referenced an address in virtual memory.
           Describe a scenario in which each of the following can occur. (If no such
           scenario can occur, explain why.)
           ·    TLB miss with no page fault
           ·    TLB miss and page fault
           ·    TLB hit and no page fault
           ·    TLB hit and page fault
     9.15  A simplified view of thread states is Ready, Running, and Blocked, where
           a thread is either ready and waiting to be scheduled, is running on the
           processor, or is blocked (for example, waiting for I/O). This is illustrated
           in Figure 9.31. Assuming a thread is in the Running state, answer the
           following questions, and explain your answer:
           a.   Will the thread change state if it incurs a page fault? If so, to what
                state will it change?
           b.   Will the thread change state if it generates a TLB miss that is resolved
                in the page table? If so, to what state will it change?
           c.   Will the thread change state if an address reference is resolved in
                the page table? If so, to what state will it change?
     9.16  Consider a system that uses pure demand paging.
           a.   When a process first starts execution, how would you characterize
                the page-fault rate?
           b.   Once the working set for a process is loaded into memory, how
                would you characterize the page-fault rate?
                                           Ready
                Blocked                           Running
                Figure 9.31     Thread state diagram for Exercise 9.15.



                                                             Exercises              453
      c.  Assume that a process changes its locality and the size of the new
          working set is too large to be stored in available free memory.
          Identify some options system designers could choose from to
          handle this situation.
9.17  What is the copy-on-write feature, and under what circumstances is its
      use beneficial? What hardware support is required to implement this
      feature?
9.18  A certain computer provides its users with a virtual memory space of
      232 bytes. The computer has 222 bytes of physical memory. The virtual
      memory is implemented by paging, and the page size is 4,096 bytes.
      A user process generates the virtual address 11123456. Explain how
      the system establishes the corresponding physical location. Distinguish
      between software and hardware operations.
9.19  Assume that we have a demand-paged memory. The page table is held in
      registers. It takes 8 milliseconds to service a page fault if an empty frame
      is available or if the replaced page is not modified and 20 milliseconds if
      the replaced page is modified. Memory-access time is 100 nanoseconds.
          Assume that the page to be replaced is modified 70 percent of the
      time. What is the maximum acceptable page-fault rate for an effective
      access time of no more than 200 nanoseconds?
9.20  When a page fault occurs, the process requesting the page must block
      while waiting for the page to be brought from disk into physical memory.
      Assume that there exists a process with five user-level threads and that
      the mapping of user threads to kernel threads is one to one. If one user
      thread incurs a page fault while accessing its stack, would the other
      user threads belonging to the same process also be affected by the page
      fault--that is, would they also have to wait for the faulting page to be
      brought into memory? Explain.
9.21  Consider the following page reference string:
                7, 2, 3, 1, 2, 5, 3, 4, 6, 7, 7, 1, 0, 5, 4, 6, 2, 3, 0 , 1.
      Assuming demand paging with three frames, how many page faults
      would occur for the following replacement algorithms?
      ·   LRU replacement
      ·   FIFO replacement
      ·   Optimal replacement
9.22  The page table shown in Figure 9.32 is for a system with 16-bit virtual
      and physical addresses and with 4,096-byte pages. The reference bit is
      set to 1 when the page has been referenced. Periodically, a thread zeroes
      out all values of the reference bit. A dash for a page frame indicates
      the page is not in memory. The page-replacement algorithm is localized
      LRU, and all numbers are provided in decimal.
      a.  Convert the following virtual addresses (in hexadecimal) to the
          equivalent physical addresses. You may provide answers in either



454  Chapter 9  Virtual Memory
                    Page            Page Frame           Reference   Bit
                    0                    9                   0
                    1                    1                   0
                    2                    14                  0
                    3                    10                  0
                    4                    ­                   0
                    5                    13                  0
                    6                    8                   0
                    7                    15                  0
                    8                    ­                   0
                    9                    0                   0
                    10                   5                   0
                    11                   4                   0
                    12                   ­                   0
                    13                   ­                   0
                    14                   3                   0
                    15                   2                   0
                        Figure 9.32  Page table for Exercise 9.22.
                hexadecimal or decimal. Also set the reference bit for the appro-
                priate entry in the page table.
                ·   0xE12C
                ·   0x3A9D
                ·   0xA9D9
                ·   0x7001
                ·   0xACA1
           b.   Using the above addresses as a guide, provide an example of a
                logical address (in hexadecimal) that results in a page fault.
           c.   From what set of page frames will the LRU page-replacement
                algorithm choose in resolving a page fault?
     9.23  Assume that you are monitoring the rate at which the pointer in the
           clock algorithm moves. (The pointer indicates the candidate page for
           replacement.) What can you say about the system if you notice the
           following behavior:
           a.   Pointer is moving fast.
           b.   Pointer is moving slow.
     9.24  Discuss  situations  in  which    the  least  frequently  used  (LFU)  page-
           replacement algorithm generates fewer page faults than the least recently
           used (LRU) page-replacement algorithm. Also discuss under what cir-
           cumstances the opposite holds.
     9.25  Discuss  situations  in  which    the  most   frequently  used  (MFU)  page-
           replacement algorithm generates fewer page faults than the least recently
           used (LRU) page-replacement algorithm. Also discuss under what cir-
           cumstances the opposite holds.



                                                        Exercises                    455
9.26  The VAX/VMS system uses a FIFO replacement algorithm for resident
      pages and a free-frame pool of recently used pages. Assume that the
      free-frame pool is managed using the LRU replacement policy. Answer
      the following questions:
      a.  If a page fault occurs and the page does not exist in the free-frame
          pool, how is free space generated for the newly requested page?
      b.  If a page fault occurs and the page exists in the free-frame pool,
          how is the resident page set and the free-frame pool managed to
          make space for the requested page?
      c.  What does the system degenerate to if the number of resident pages
          is set to one?
      d.  What does the system degenerate to if the number of pages in the
          free-frame pool is zero?
9.27  Consider a demand-paging system with the following time-measured
      utilizations:
                          CPU utilization     20%
                          Paging disk         97.7%
                          Other I/O devices         5%
      For each of the following, indicate whether it will (or is likely to) improve
      CPU utilization. Explain your answers.
      a.  Install a faster CPU.
      b.  Install a bigger paging disk.
      c.  Increase the degree of multiprogramming.
      d.  Decrease the degree of multiprogramming.
      e.  Install more main memory.
      f.  Install a faster hard disk or multiple controllers with multiple hard
          disks.
      g.  Add prepaging to the page-fetch algorithms.
      h.  Increase the page size.
9.28  Suppose that a machine provides instructions that can access memory
      locations using the one-level indirect addressing scheme. What sequence
      of page faults is incurred when all of the pages of a program are
      currently nonresident and the first instruction of the program is an
      indirect memory-load operation? What happens when the operating
      system is using a per-process frame allocation technique and only two
      pages are allocated to this process?
9.29  Suppose that your replacement policy (in a paged system) is to examine
      each page regularly and to discard that page if it has not been used since
      the last examination. What would you gain and what would you lose
      by using this policy rather than LRU or second-chance replacement?



456  Chapter 9  Virtual Memory
     9.30  A page-replacement algorithm should minimize the number of page
           faults. We can achieve this minimization by distributing heavily used
           pages evenly over all of memory, rather than having them compete for
           a small number of page frames. We can associate with each page frame
           a counter of the number of pages associated with that frame. Then,
           to replace a page, we can search for the page frame with the smallest
           counter.
           a.   Define a page-replacement algorithm using this basic idea. Specif-
                ically address these problems:
                i.    What is the initial value of the counters?
                ii.   When are counters increased?
                iii.  When are counters decreased?
                iv.   How is the page to be replaced selected?
           b.   How many page faults occur for your algorithm for the following
                reference string with four page frames?
                      1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2.
           c.   What is the minimum number of page faults for an optimal page-
                replacement strategy for the reference string in part b with four
                page frames?
     9.31  Consider a demand-paging system with a paging disk that has an
           average access and transfer time of 20 milliseconds. Addresses are
           translated through a page table in main memory, with an access time of 1
           microsecond per memory access. Thus, each memory reference through
           the page table takes two accesses. To improve this time, we have added
           an associative memory that reduces access time to one memory reference
           if the page-table entry is in the associative memory.
           Assume that 80 percent of the accesses are in the associative memory
           and that, of those remaining, 10 percent (or 2 percent of the total) cause
           page faults. What is the effective memory access time?
     9.32  What is the cause of thrashing? How does the system detect thrashing?
           Once it detects thrashing, what can the system do to eliminate this
           problem?
     9.33  Is it possible for a process to have two working sets, one representing
           data and another representing code? Explain.
     9.34  Consider the parameter        used to define the working-set window in the
           working-set model. When         is set to a small value, what is the effect
           on the page-fault frequency and the number of active (nonsuspended)
           processes currently executing in the system? What is the effect when
           is set to a very high value?
     9.35  In a 1,024-KB segment, memory is allocated using the buddy system.
           Using Figure 9.26 as a guide, draw a tree illustrating how the following
           memory requests are allocated:
           ·    Request 6-KB



                                                  Programming Problems             457
      ·  Request 250 bytes
      ·  Request 900 bytes
      ·  Request 1,500 bytes
      ·  Request 7-KB
      Next, modify the tree for the following releases of memory. Perform
      coalescing whenever possible:
      ·  Release 250 bytes
      ·  Release 900 bytes
      ·  Release 1,500 bytes
9.36  A system provides support for user-level and kernel-level threads. The
      mapping in this system is one to one (there is a corresponding kernel
      thread for each user thread). Does a multithreaded process consist of (a)
      a working set for the entire process or (b) a working set for each thread?
      Explain
9.37  The slab-allocation algorithm uses a separate cache for each different
      object type. Assuming there is one cache per object type, explain why
      this scheme doesn't scale well with multiple CPUs. What could be done
      to address this scalability issue?
9.38  Consider a system that allocates pages of different sizes to its processes.
      What are the advantages of such a paging scheme? What modifications
      to the virtual memory system provide this functionality?
Programming Problems
9.39  Write a program that implements the FIFO, LRU, and optimal page-
      replacement  algorithms  presented      in  this  chapter.  First,  generate  a
      random page-reference string where page numbers range from 0 to 9.
      Apply the random page-reference string to each algorithm, and record
      the number of page faults incurred by each algorithm. Implement the
      replacement algorithms so that the number of page frames can vary from
      1 to 7. Assume that demand paging is used.
9.40  Repeat Exercise 3.22, this time using Windows shared memory. In partic-
      ular, using the producer--consumer strategy, design two programs that
      communicate with shared memory using the Windows API as outlined
      in Section 9.7.2. The producer will generate the numbers specified in
      the Collatz conjecture and write them to a shared memory object. The
      consumer will then read and output the sequence of numbers from
      shared memory.
         In this instance, the producer will be passed an integer parameter
      on the command line specifying how many numbers to produce (for
      example, providing 5 on the command line means the producer process
      will generate the first five numbers).



458  Chapter 9  Virtual Memory
Programming Projects
     Designing a Virtual Memory Manager
     This project consists of writing a program that translates logical to physical
     addresses for a virtual address space of size 216 = 65,536 bytes. Your program
     will read from a file containing logical addresses and, using a TLB as well as
     a page table, will translate each logical address to its corresponding physical
     address and output the value of the byte stored at the translated physical
     address. The goal behind this project is to simulate the steps involved in
     translating logical to physical addresses.
     Specifics
     Your program will read a file containing several 32-bit integer numbers that
     represent logical addresses. However, you need only be concerned with 16-bit
     addresses, so you must mask the rightmost 16 bits of each logical address.
     These 16 bits are divided into (1) an 8-bit page number and (2) 8-bit page offset.
     Hence, the addresses are structured as shown in Figure 9.33.
        Other specifics include the following:
     ·  28 entries in the page table
     ·  Page size of 28 bytes
     ·  16 entries in the TLB
     ·  Frame size of 28 bytes
     ·  256 frames
     ·  Physical memory of 65,536 bytes (256 frames × 256-byte frame size)
        Additionally, your program need only be concerned with reading logical
     addresses and translating them to their corresponding physical addresses. You
     do not need to support writing to the logical address space.
     Address Translation
     Your program will translate logical to physical addresses using a TLB and page
     table as outlined in Section 8.5. First, the page number is extracted from the
     logical address, and the TLB is consulted. In the case of a TLB-hit, the frame
     number is obtained from the TLB. In the case of a TLB-miss, the page table
     must be consulted. In the latter case, either the frame number is obtained
                                                   page          offset
                                                   number
                    31                      16 15          8  7          0
                               Figure 9.33  Address structure.



                                                     Programming Projects                 459
page    offset
number                   page       frame
                         number  number
                     0
                     1
                     2
                     .                                                     0    frame 0
                     .
                     .                     TLB hit                         1    frame 1
                     .                                                     2    frame 2
                     15                             frame                  .
                                                            offset         .
                                                    number                 .
                               TLB
                                                                           .
                         0     page 0                                      255  frame 255
                         1     page 1                                           physical
                         2     page 2                                           memory
                         .
                         .
                         .
        TLB miss         .
                         255  page 255
                               page
                               table
        Figure 9.34      A representation of the address-translation process.
from the page table or a page fault occurs. A visual representation of the
address-translation process appears in Figure 9.34.
Handling Page Faults
Your program will implement demand paging as described in Section 9.2. The
backing store is represented by the file BACKING STORE.bin, a binary file of size
65,536 bytes. When a page fault occurs, you will read in a 256-byte page from the
file BACKING STORE and store it in an available page frame in physical memory.
For example, if a logical address with page number 15 resulted in a page fault,
your program would read in page 15 from BACKING STORE (remember that
pages begin at 0 and are 256 bytes in size) and store it in a page frame in
physical memory. Once this frame is stored (and the page table and TLB are
updated), subsequent accesses to page 15 will be resolved by either the TLB or
the page table.
You will need to treat BACKING STORE.bin as a random-access file so that
you can randomly seek to certain positions of the file for reading. We suggest
using the standard C library functions for performing I/O, including fopen(),
fread(), fseek(), and fclose().
The     size     of  physical    memory    is  the  same    as  the  size  of   the  virtual
address space--65,536 bytes--so you do not need to be concerned about
page replacements during a page fault. Later, we describe a modification
to this project using a smaller amount of physical memory; at that point, a
page-replacement strategy will be required.



460  Chapter 9   Virtual Memory
     Test File
     We provide the file addresses.txt, which contains integer values represent-
     ing logical addresses ranging from 0 - 65535 (the size of the virtual address
     space). Your program will open this file, read each logical address and translate
     it to its corresponding physical address, and output the value of the signed byte
     at the physical address.
     How to Begin
     First, write a simple program that extracts the page number and offset (based
     on Figure 9.33) from the following integer numbers:
         1, 256, 32768, 32769, 128, 65534, 33153
     Perhaps the easiest way to do this is by using the operators for bit-masking
     and bit-shifting. Once you can correctly establish the page number and offset
     from an integer number, you are ready to begin.
         Initially, we suggest that you bypass the TLB and use only a page table. You
     can integrate the TLB once your page table is working properly. Remember,
     address translation can work without a TLB; the TLB just makes it faster. When
     you are ready to implement the TLB, recall that it has only 16 entries, so you
     will need to use a replacement strategy when you update a full TLB. You may
     use either a FIFO or an LRU policy for updating your TLB.
     How to Run Your Program
     Your program should run as follows:
         ./a.out addresses.txt
     Your program will read in the file addresses.txt, which contains 1,000 logical
     addresses ranging from 0 to 65535. Your program is to translate each logical
     address to a physical address and determine the contents of the signed byte
     stored at the correct physical address. (Recall that in the C language, the char
     data type occupies a byte of storage, so we suggest using char values.)
         Your program is to output the following values:
     1.  The logical address being translated (the integer value being read from
         addresses.txt).
     2.  The corresponding physical address (what your program translates the
         logical address to).
     3.  The signed byte value stored at the translated physical address.
         We also provide the file correct.txt, which contains the correct output
     values for the file addresses.txt. You should use this file to determine if your
     program is correctly translating logical to physical addresses.
     Statistics
     After completion, your program is to report the following statistics:



                                                  Bibliographical Notes          461
1.  Page-fault rate --The percentage of address references that resulted in
    page faults.
2.  TLB hit rate --The percentage of address references that were resolved in
    the TLB.
    Since the logical addresses in addresses.txt were generated randomly
and do not reflect any memory access locality, do not expect to have a high TLB
hit rate.
Modifications
This project assumes that physical memory is the same size as the virtual
address space. In practice, physical memory is typically much smaller than a
virtual address space. A suggested modification is to use a smaller physical
address space. We recommend using 128 page frames rather than 256. This
change will require modifying your program so that it keeps track of free page
frames as well as implementing a page-replacement policy using either FIFO
or LRU (Section 9.4).
Bibliographical Notes
Demand paging was first used in the Atlas system, implemented on the
Manchester University MUSE computer around 1960 ([Kilburn et al. (1961)]).
Another early demand-paging system was MULTICS, implemented on the GE
645 system ([Organick (1972)]). Virtual memory was added to Unix in 1979
[Babaoglu and Joy (1981)]
    [Belady et al. (1969)] were the first researchers to observe that the FIFO
replacement strategy may produce the anomaly that bears Belady's name.
[Mattson et al. (1970)] demonstrated that stack algorithms are not subject to
Belady's anomaly.
    The    optimal  replacement  algorithm   was  presented  by     [Belady  (1966)]
and was proved to be optimal by [Mattson et al. (1970)]. Belady's optimal
algorithm is for a fixed allocation; [Prieve and Fabry (1976)] presented an
optimal algorithm for situations in which the allocation can vary.
    The enhanced clock algorithm was discussed by [Carr and Hennessy
(1981)].
    The working-set model was developed by [Denning (1968)]. Discussions
concerning the working-set model were presented by [Denning (1980)].
    The scheme for monitoring the page-fault rate was developed by [Wulf
(1969)],   who  successfully  applied  this  technique  to  the  Burroughs   B5500
computer system.
    Buddy system memory allocators were described in [Knowlton (1965)],
[Peterson and Norman (1977)], and [Purdom, Jr. and Stigler (1970)]. [Bonwick
(1994)] discussed the slab allocator, and [Bonwick and Adams (2001)] extended
the discussion to multiple processors. Other memory-fitting algorithms can be
found in [Stephenson (1983)], [Bays (1977)], and [Brent (1989)]. A survey of
memory-allocation strategies can be found in [Wilson et al. (1995)].
    [Solomon and Russinovich (2000)] and [Russinovich and Solomon (2005)]
described how Windows implements virtual memory. [McDougall and Mauro



462  Chapter 9    Virtual Memory
     (2007)] discussed virtual memory in Solaris. Virtual memory techniques in
     Linux   and    FreeBSD    were  described  by    [Love    (2010)]  and  [McKusick    and
     Neville-Neil    (2005)],  respectively.  [Ganapathy       and  Schimmel     (1998)]  and
     [Navarro et al. (2002)] discussed operating system support for multiple page
     sizes.
Bibliography
     [Babaoglu and Joy (1981)]       O. Babaoglu and W. Joy, "Converting a Swap-Based
     System to Do Paging in an Architecture Lacking Page-Reference Bits", Pro-
     ceedings of the ACM Symposium on Operating Systems Principles (1981), pages
     78­86.
     [Bays (1977)]    C. Bays, "A Comparison of Next-Fit, First-Fit and Best-Fit", Com-
     munications of the ACM, Volume 20, Number 3 (1977), pages 191­192.
     [Belady (1966)]     L. A. Belady, "A Study of Replacement Algorithms for a Virtu-
     al-Storage Computer", IBM Systems Journal, Volume 5, Number 2 (1966), pages
     78­101.
     [Belady et al. (1969)]    L. A. Belady, R. A. Nelson, and G. S. Shedler, "An Anomaly
     in      Space-Time  Characteristics  of    Certain  Programs       Running  in  a  Paging
     Machine", Communications of the ACM, Volume 12, Number 6 (1969), pages
     349­353.
     [Bonwick (1994)]        J. Bonwick, "The Slab Allocator: An Object-Caching Kernel
     Memory Allocator", USENIX Summer (1994), pages 87­98.
     [Bonwick and Adams (2001)]      J. Bonwick and J. Adams, "Magazines and Vmem:
     Extending the Slab Allocator to Many CPUs and Arbitrary Resources", Proceed-
     ings of the 2001 USENIX Annual Technical Conference (2001).
     [Brent (1989)]    R. Brent, "Efficient Implementation of the First-Fit Strategy for
     Dynamic Storage Allocation", ACM Transactions on Programming Languages and
     Systems, Volume 11, Number 3 (1989), pages 388­403.
     [Carr and Hennessy (1981)]      W.   R.    Carr  and  J.  L.  Hennessy,     "WSClock -- A
     Simple and Effective Algorithm for Virtual Memory Management", Proceedings
     of the ACM Symposium on Operating Systems Principles (1981), pages 87­95.
     [Denning (1968)]        P. J. Denning, "The Working Set Model for Program Behavior",
     Communications of the ACM, Volume 11, Number 5 (1968), pages 323­333.
     [Denning (1980)]        P. J. Denning, "Working Sets Past and Present", IEEE Transac-
     tions on Software Engineering, Volume SE-6, Number 1 (1980), pages 64­84.
     [Ganapathy and Schimmel (1998)]            N. Ganapathy and C. Schimmel, "General
     Purpose Operating System Support for Multiple Page Sizes", Proceedings of the
     USENIX Technical Conference (1998).
     [Kilburn et al. (1961)]    T. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner,
     "The Manchester University Atlas Operating System, Part I: Internal Organiza-
     tion", Computer Journal, Volume 4, Number 3 (1961), pages 222­225.



                                                                   Bibliography          463
[Knowlton (1965)]        K. C. Knowlton, "A Fast Storage Allocator", Communications
of the ACM, Volume 8, Number 10 (1965), pages 623­624.
[Love (2010)]   R. Love, Linux Kernel Development, Third Edition, Developer's
Library (2010).
[Mattson et al. (1970)]      R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger,
"Evaluation Techniques for Storage Hierarchies", IBM Systems Journal, Volume
9, Number 2 (1970), pages 78­117.
[McDougall and Mauro (2007)]             R. McDougall and J. Mauro, Solaris Internals,
Second Edition, Prentice Hall (2007).
[McKusick and Neville-Neil (2005)]          M. K. McKusick and G. V. Neville-Neil,
The Design and Implementation of the FreeBSD UNIX Operating System, Addison
Wesley (2005).
[Navarro et al. (2002)]      J. Navarro, S. Lyer, P. Druschel, and A. Cox, "Practical,
Transparent    Operating         System  Support     for  Superpages",  Proceedings  of  the
USENIX Symposium on Operating Systems Design and Implementation (2002).
[Organick (1972)]        E.  I.  Organick,  The  Multics  System:  An   Examination  of  Its
Structure, MIT Press (1972).
[Peterson and Norman (1977)]      J. L. Peterson and T. A. Norman, "Buddy Sys-
tems", Communications of the ACM, Volume 20, Number 6 (1977), pages 421­431.
[Prieve and Fabry (1976)]        B. G. Prieve and R. S. Fabry, "VMIN -- An Optimal
Variable  Space    Page-Replacement         Algorithm",   Communications    of  the  ACM,
Volume 19, Number 5 (1976), pages 295­297.
[Purdom, Jr. and Stigler (1970)]         P. W. Purdom, Jr. and S. M. Stigler, "Statistical
Properties of the Buddy System", J. ACM, Volume 17, Number 4 (1970), pages
683­697.
[Russinovich and Solomon (2005)]            M.   E.  Russinovich   and  D.  A.  Solomon,
Microsoft Windows Internals, Fourth Edition, Microsoft Press (2005).
[Solomon and Russinovich (2000)]            D. A. Solomon and M. E. Russinovich, Inside
Microsoft Windows 2000, Third Edition, Microsoft Press (2000).
[Stephenson (1983)]          C. J. Stephenson, "Fast Fits: A New Method for Dynamic
Storage Allocation", Proceedings of the Ninth Symposium on Operating Systems
Principles (1983), pages 30­32.
[Wilson et al. (1995)]       P. R. Wilson, M. S. Johnstone, M. Neely, and D. Boles,
"Dynamic Storage Allocation: A Survey and Critical Review", Proceedings of the
International Workshop on Memory Management (1995), pages 1­116.
[Wulf (1969)]   W. A. Wulf, "Performance Monitors for Multiprogramming Sys-
tems", Proceedings of the ACM Symposium on Operating Systems Principles (1969),
pages 175­181.






                 Part Four
Storage
Management
Since main memory is usually too small to accommodate all the data and
programs permanently, the computer system must provide secondary
storage to back up main memory. Modern computer systems use disks
as the primary on-line storage medium for information (both programs
and data). The file system provides the mechanism for on-line storage
of and access to both data and programs residing on the disks. A file
is a collection of related information defined by its creator. The files are
mapped by the operating system onto physical devices. Files are normally
organized into directories for ease of use.
     The devices that attach to a computer vary in many aspects. Some
devices transfer a character or a block of characters at a time. Some
can  be   accessed  only  sequentially,  others   randomly.  Some   transfer
data synchronously, others asynchronously. Some are dedicated, some
shared. They can be read-only or read ­ write. They vary greatly in speed.
In  many  ways,  they  are  also  the    slowest  major  component  of        the
computer.
     Because of all this device variation, the operating system needs to
provide a wide range of functionality to applications, to allow them to
control all aspects of the devices. One key goal of an operating system's
I/O subsystem is to provide the simplest interface possible to the rest of
the system. Because devices are a performance bottleneck, another key
is to optimize I/O for maximum concurrency.



