Windows 7


                                      19C H A P T E R
Windows 7
      Updated by Dave Probert
      The Microsoft Windows 7 operating system is a 32-/64-bit preemptive mul-
      titasking client operating system for microprocessors implementing the Intel
      IA-32 and AMD64 instruction set architectures (ISAs). Microsoft's corresponding
      server operating system, Windows Server 2008 R2, is based on the same code
      as Windows 7 but supports only the 64-bit AMD64 and IA64 (Itanium) ISAs.
      Windows 7 is the latest in a series of Microsoft operating systems based on its
      NT code, which replaced the earlier systems based on Windows 95/98. In this
      chapter, we discuss the key goals of Windows 7, the layered architecture of the
      system that has made it so easy to use, the file system, the networking features,
      and the programming interface.
      CHAPTER OBJECTIVES
      ·  To explore the principles underlying Windows 7's design and the specific
         components of the system.
      ·  To provide a detailed discussion of the Windows 7 file system.
      ·  To illustrate the networking protocols supported in Windows 7.
      ·  To describe the interface available in Windows 7 to system and application
         programmers.
      ·  To describe the important algorithms implemented with Windows 7.
19.1  History
      In the mid-1980s, Microsoft and IBM cooperated to develop the OS/2 operating
      system, which was written in assembly language for single-processor Intel
      80286 systems. In 1988, Microsoft decided to end the joint effort with IBM
      and develop its own "new technology" (or NT) portable operating system to
      support both the OS/2 and POSIX application-programming interfaces (APIs). In
                                                                                         829



830  Chapter 19  Windows 7
     October 1988, Dave Cutler, the architect of the DEC VAX/VMS operating system,
     was hired and given the charter of building Microsoft's new operating system.
     Originally, the team planned to use the OS/2 API as NT's native environment,
     but during development, NT was changed to use a new 32-bit Windows API
     (called Win32), based on the popular 16-bit API used in Windows 3.0. The first
     versions of NT were Windows NT 3.1 and Windows NT 3.1 Advanced Server.
     (At that time, 16-bit Windows was at Version 3.1.) Windows NT Version 4.0
     adopted the Windows 95 user interface and incorporated Internet web-server
     and web-browser software. In addition, user-interface routines and all graphics
     code were moved into the kernel to improve performance, with the side effect of
     decreased system reliability. Although previous versions of NT had been ported
     to other microprocessor architectures, the Windows 2000 version, released
     in February 2000, supported only Intel (and compatible) processors due to
     marketplace factors. Windows 2000 incorporated significant changes. It added
     Active Directory (an X.500-based directory service), better networking and
     laptop support, support for plug-and-play devices, a distributed file system,
     and support for more processors and more memory.
     In October 2001, Windows XP was released as both an update to the
     Windows 2000 desktop operating system and a replacement for Windows
     95/98. In 2002, the server edition of Windows XP became available (called
     Windows .Net Server). Windows XP updated the graphical user interface
     (GUI) with a visual design that took advantage of more recent hardware
     advances and many new ease-of-use features. Numerous features were added
     to automatically repair problems in applications and the operating system
     itself. As a result of these changes, Windows XP provided better networking and
     device experience (including zero-configuration wireless, instant messaging,
     streaming   media,  and  digital  photography/video),  dramatic   performance
     improvements for both the desktop and large multiprocessors, and better
     reliability and security than earlier Windows operating systems.
     The long-awaited update to Windows XP, called Windows Vista, was
     released in November 2006, but it was not well received. Although Win-
     dows Vista included many improvements that later showed up in Windows
     7, these improvements were overshadowed by Windows Vista's perceived
     sluggishness and compatibility problems. Microsoft responded to criticisms
     of Windows Vista by improving its engineering processes and working more
     closely with the makers of Windows hardware and applications. The result was
     Windows 7, which was released in October 2009, along with corresponding
     server editions of Windows. Among the significant engineering changes is the
     increased use of execution tracing rather than counters or profiling to analyze
     system behavior. Tracing runs constantly in the system, watching hundreds of
     scenarios execute. When one of these scenarios fails, or when it succeeds but
     does not perform well, the traces can be analyzed to determine the cause.
     Windows 7 uses a client­server architecture (like Mach) to implement two
     operating-system personalities, Win32 and POSIX, with user-level processes
     called subsystems. (At one time, Windows also supported an OS/2 subsystem,
     but it was removed in Windows XP due to the demise of OS/2.) The subsystem
     architecture allows enhancements to be made to one operating-system person-
     ality without affecting the application compatibility of the other. Although the
     POSIX subsystem continues to be available for Windows 7, the Win32 API has
     become very popular, and the POSIX APIs are used by only a few sites. The
     subsystem approach continues to be interesting to study from an operating-



                                                  19.2    Design Principles             831
      system perspective, but machine-virtualization technologies are now becoming
      the dominant way of running multiple operating systems on a single machine.
      Windows 7 is a multiuser operating system, supporting simultaneous
      access through distributed services or through multiple instances of the GUI
      via the Windows terminal services. The server editions of Windows 7 support
      simultaneous terminal server sessions from Windows desktop systems. The
      desktop  editions  of  terminal  server  multiplex  the   keyboard,  mouse,       and
      monitor between virtual terminal sessions for each logged-on user. This feature,
      called fast user switching, allows users to preempt each other at the console of
      a PC without having to log off and log on.
      We noted earlier that some GUI implementation moved into kernel mode
      in Windows NT 4.0. It started to move into user mode again with Windows
      Vista, which included the desktop window manager (DWM) as a user-mode
      process. DWM implements the desktop compositing of Windows, providing
      the Windows Aero interface look on top of the Windows DirectX graphic
      software. DirectX continues to run in the kernel, as does the code implementing
      Windows'  previous     windowing  and    graphics   models  (Win32k  and  GDI).
      Windows 7 made substantial changes to the DWM, significantly reducing its
      memory footprint and improving its performance.
      Windows XP was the first version of Windows to ship a 64-bit version (for
      the IA64 in 2001 and the AMD64 in 2005). Internally, the native NT file system
      (NTFS) and many of the Win32 APIs have always used 64-bit integers where
      appropriate --so the major extension to 64-bit in Windows XP was support
      for large virtual addresses. However, 64-bit editions of Windows also support
      much larger physical memories. By the time Windows 7 shipped, the AMD64 ISA
      had become available on almost all CPUs from both Intel and AMD. In addition,
      by that time, physical memories on client systems frequently exceeded the
      4-GB limit of the IA-32. As a result, the 64-bit version of Windows 7 is now
      commonly installed on larger client systems. Because the AMD64 architecture
      supports high-fidelity IA-32 compatibility at the level of individual processes,
      32- and 64-bit applications can be freely mixed in a single system.
      In the rest of our description of Windows 7, we will not distinguish between
      the client editions of Windows 7 and the corresponding server editions. They
      are based on the same core components and run the same binary files for
      the kernel and most drivers. Similarly, although Microsoft ships a variety of
      different editions of each release to address different market price points, few
      of the differences between editions are reflected in the core of the system. In
      this chapter, we focus primarily on the core components of Windows 7.
19.2  Design Principles
      Microsoft's design goals for Windows included security, reliability, Windows
      and POSIX application compatibility, high performance, extensibility, porta-
      bility, and international support. Some additional goals, energy efficiency and
      dynamic device support, have recently been added to this list. Next, we discuss
      each of these goals and how it is achieved in Windows 7.
      19.2.1   Security
      Windows 7 security goals required more than just adherence to the design
      standards that had enabled Windows NT 4.0 to receive a C2 security classifica-



832  Chapter 19  Windows 7
     tion from the U.S. government (A C2 classification signifies a moderate level of
     protection from defective software and malicious attacks. Classifications were
     defined by the Department of Defense Trusted Computer System Evaluation
     Criteria, also known as the Orange Book, as described in Section 15.8.) Exten-
     sive code review and testing were combined with sophisticated automatic
     analysis tools to identify and investigate potential defects that might represent
     security vulnerabilities.
     Windows bases security on discretionary access controls. System objects,
     including files, registry settings, and kernel objects, are protected by access-
     control lists (ACLs) (see Section 11.6.2). ACLs are vulnerable to user and
     programmer errors, however, as well as to the most common attacks on
     consumer systems, in which the user is tricked into running code, often while
     browsing the Web. Windows 7 includes a mechanism called integrity levels
     that acts as a rudimentary capability system for controlling access. Objects and
     processes are marked as having low, medium, or high integrity. Windows does
     not allow a process to modify an object with a higher integrity level, no matter
     what the setting of the ACL.
     Other       security  measures    include  address-space  layout   randomization
     (ASLR), nonexecutable stacks and heaps, and encryption and digital signature
     facilities. ASLR thwarts many forms of attack by preventing small amounts of
     injected code from jumping easily to code that is already loaded in a process as
     part of normal operation. This safeguard makes it likely that a system under
     attack will fail or crash rather than let the attacking code take control.
     Recent chips from both Intel and AMD are based on the AMD64 architecture,
     which   allows  memory     pages  to  be   marked  so  that  they  cannot   contain
     executable instruction code. Windows tries to mark stacks and memory heaps
     so that they cannot be used to execute code, thus preventing attacks in which
     a program bug allows a buffer to overflow and then is tricked into executing
     the contents of the buffer. This technique cannot be applied to all programs,
     because some rely on modifying data and executing it. A column labeled "data
     execution prevention" in the Windows task manager shows which processes
     are marked to prevent these attacks.
     Windows uses encryption as part of common protocols, such as those used
     to communicate securely with websites. Encryption is also used to protect
     user files stored on disk from prying eyes. Windows 7 allows users to easily
     encrypt virtually a whole disk, as well as removable storage devices such as USB
     flash drives, with a feature called BitLocker. If a computer with an encrypted
     disk is stolen, the thieves will need very sophisticated technology (such as an
     electron microscope) to gain access to any of the computer's files. Windows
     uses digital signatures to sign operating system binaries so it can verify that the
     files were produced by Microsoft or another known company. In some editions
     of Windows, a code integrity module is activated at boot to ensure that all the
     loaded modules in the kernel have valid signatures, assuring that they have
     not been tampered with by an off-line attack.
     19.2.2  Reliability
     Windows matured greatly as an operating system in its first ten years, leading
     to Windows 2000. At the same time, its reliability increased due to such factors
     as maturity in the source code, extensive stress testing of the system, improved
     CPU architectures, and automatic detection of many serious errors in drivers



                                                  19.2  Design Principles           833
from both Microsoft and third parties. Windows has subsequently extended
the tools for achieving reliability to include automatic analysis of source code
for errors, tests that include providing invalid or unexpected input parameters
(known as fuzzing to detect validation failures, and an application version
of the driver verifier that applies dynamic checking for an extensive set of
common user-mode programming errors. Other improvements in reliability
have resulted from moving more code out of the kernel and into user-mode
services. Windows provides extensive support for writing drivers in user mode.
System facilities that were once in the kernel and are now in user mode include
the Desktop Window Manager and much of the software stack for audio.
    One of the most significant improvements in the Windows experience
came  from  adding     memory   diagnostics   as  an    option  at  boot   time.  This
addition is especially valuable because so few consumer PCs have error-
correcting memory. When bad RAM starts to drop bits here and there, the
result is frustratingly erratic behavior in the system. The availability of memory
diagnostics has greatly reduced the stress levels of users with bad RAM.
    Windows 7 introduced a fault-tolerant memory heap. The heap learns from
application crashes and automatically inserts mitigations into future execution
of an application that has crashed. This makes the application more reliable
even if it contains common bugs such as using memory after freeing it or
accessing past the end of the allocation.
    Achieving high reliability in Windows is particularly challenging because
almost one billion computers run Windows. Even reliability problems that
affect only a small percentage of users still impact tremendous numbers of
human beings. The complexity of the Windows ecosystem also adds to the
challenges. Millions of instances of applications, drivers, and other software are
being constantly downloaded and run on Windows systems. Of course, there
is also a constant stream of malware attacks. As Windows itself has become
harder to attack directly, exploits increasingly target popular applications.
    To cope with these challenges, Microsoft is increasingly relying on com-
munications from customer machines to collect large amounts of data from
the ecosystem. Machines can be sampled to see how they are performing,
what software they are running, and what problems they are encountering.
Customers can send data to Microsoft when systems or software crashes or
hangs. This constant stream of data from customer machines is collected very
carefully, with the users' consent and without invading privacy. The result is
that Microsoft is building an ever-improving picture of what is happening in the
Windows ecosystem that allows continuous improvements through software
updates, as well as providing data to guide future releases of Windows.
19.2.3    Windows and POSIX Application Compatibility
As  mentioned,    Windows   XP  was     both  an  update    of  Windows    2000     and
a   replacement   for  Windows  95/98.     Windows    2000  focused  primarily      on
compatibility for business applications. The requirements for Windows XP
included a much higher compatibility with the consumer applications that ran
on Windows 95/98. Application compatibility is difficult to achieve because
many applications check for a particular version of Windows, may depend
to  some  extent  on   the  quirks  of  the  implementation     of  APIs,  may    have
latent application bugs that were masked in the previous system, and so



834  Chapter 19  Windows 7
     forth. Applications may also have been compiled for a different instruction
     set. Windows 7 implements several strategies to run applications despite
     incompatibilities.
     Like Windows XP, Windows 7 has a compatibility layer that sits between
     applications and the Win32 APIs. This layer makes Windows 7 look (almost)
     bug-for-bug compatible with previous versions of Windows. Windows 7, like
     earlier NT releases, maintains support for running many 16-bit applications
     using a thunking, or conversion, layer that translates 16-bit API calls into
     equivalent 32-bit calls. Similarly, the 64-bit version of Windows 7 provides
     a thunking layer that translates 32-bit API calls into native 64-bit calls.
     The Windows subsystem model allows multiple operating-system person-
     alities to be supported. As noted earlier, although the API most commonly
     used with Windows is the Win32 API, some editions of Windows 7 support a
     POSIX subsystem. POSIX is a standard specification for UNIX that allows most
     available UNIX-compatible software to compile and run without modification.
     As a final compatibility measure, several editions of Windows 7 provide
     a virtual machine that runs Windows XP inside Windows 7. This allows
     applications to get bug-for-bug compatibility with Windows XP.
     19.2.4  High Performance
     Windows was designed to provide high performance on desktop systems
     (which are largely constrained by I/O performance), server systems (where
     the CPU is often the bottleneck), and large multithreaded and multiprocessor
     environments (where locking performance and cache-line management are
     keys to scalability). To satisfy performance requirements, NT used a variety
     of techniques, such as asynchronous I/O, optimized protocols for networks,
     kernel-based graphics rendering, and sophisticated caching of file-system data.
     The memory-management and synchronization algorithms were designed
     with an awareness of the performance considerations related to cache lines
     and multiprocessors.
     Windows NT was designed for symmetrical multiprocessing (SMP); on
     a multiprocessor computer, several threads can run at the same time, even
     in the kernel. On each CPU, Windows NT uses priority-based preemptive
     scheduling of threads. Except while executing in the kernel dispatcher or at
     interrupt level, threads in any process running in Windows can be preempted
     by higher-priority threads. Thus, the system responds quickly (see Chapter 6).
     The     subsystems    that  constitute  Windows  NT  communicate             with  one
     another efficiently through a local procedure call (LPC) facility that provides
     high-performance message passing. When a thread requests a synchronous
     service from another process through an LPC, the servicing thread is marked
     ready, and its priority is temporarily boosted to avoid the scheduling delays
     that would occur if it had to wait for threads already in the queue.
     Windows XP further improved performance by reducing the code-path
     length in critical functions, using better algorithms and per-processor data
     structures, using memory coloring for non-uniform memory access (NUMA)
     machines, and implementing more scalable locking protocols, such as queued
     spinlocks. The new locking protocols helped reduce system bus cycles and
     included lock-free lists and queues, atomic read ­modify­write operations
     (like interlocked increment), and other advanced synchronization techniques.



                                                  19.2   Design Principles          835
By the time Windows 7 was developed, several major changes had come
to computing. Client/server computing had increased in importance, so an
advanced  local  procedure     call  (ALPC)  facility   was   introduced  to  provide
higher  performance  and    more     reliability  than  LPC.  The  number     of  CPUs
and the amount of physical memory available in the largest multiprocessors
had increased substantially, so quite a lot of effort was put into improving
operating-system scalability.
The implementation of SMP in Windows NT used bitmasks to represent
collections of processors and to identify, for example, which set of processors a
particular thread could be scheduled on. These bitmasks were defined as fitting
within a single word of memory, limiting the number of processors supported
within a system to 64. Windows 7 added the concept of processor groups to
represent arbitrary numbers of CPUs, thus accommodating more CPU cores.
The number of CPU cores within single systems has continued to increase not
only because of more cores but also because of cores that support more than
one logical thread of execution at a time.
All these additional CPUs created a great deal of contention for the locks
used for scheduling CPUs and memory. Windows 7 broke these locks apart. For
example, before Windows 7, a single lock was used by the Windows scheduler
to synchronize access to the queues containing threads waiting for events. In
Windows 7, each object has its own lock, allowing the queues to be accessed
concurrently. Also, many execution paths in the scheduler were rewritten to be
lock-free. This change resulted in good scalability performance for Windows
even on systems with 256 hardware threads.
Other changes are due to the increasing importance of support for parallel
computing. For years, the computer industry has been dominated by Moore's
Law, leading to higher densities of transistors that manifest themselves as faster
clock rates for each CPU. Moore's Law continues to hold true, but limits have
been reached that prevent CPU clock rates from increasing further. Instead,
transistors are being used to build more and more CPUs into each chip. New
programming models for achieving parallel execution, such as Microsoft's
Concurrency RunTime (ConcRT) and Intel's Threading Building Blocks (TBB),
are being used to express parallelism in C++ programs. Where Moore's Law
has governed computing for forty years, it now seems that Amdahl's Law,
which governs parallel computing, will rule the future.
To support task-based parallelism, Windows 7 provides a new form of
user-mode scheduling (UMS). UMS allows programs to be decomposed into
tasks, and the tasks are then scheduled on the available CPUs by a scheduler
that operates in user mode rather than in the kernel.
The advent of multiple CPUs on the smallest computers is only part of
the shift taking place to parallel computing. Graphics processing units (GPUs)
accelerate the computational algorithms needed for graphics by using SIMD
architectures to execute a single instruction for multiple data at the same
time. This has given rise to the use of GPUs for general computing, not just
graphics. Operating-system support for software like OpenCL and CUDA is
allowing programs to take advantage of the GPUs. Windows supports use of
GPUs through software in its DirectX graphics support. This software, called
DirectCompute, allows programs to specify computational kernels using the
same HLSL (high-level shader language) programming model used to program
the SIMD hardware for graphics shaders. The computational kernels run very



836  Chapter 19   Windows 7
     quickly on the GPU and return their results to the main computation running
     on the CPU.
     19.2.5  Extensibility
     Extensibility refers to the capacity of an operating system to keep up with
     advances in computing technology. To facilitate change over time, the devel-
     opers implemented Windows using a layered architecture. The Windows
     executive runs in kernel mode and provides the basic system services and
     abstractions that support shared use of the system. On top of the executive,
     several server subsystems operate in user mode. Among them are environ-
     mental subsystems that emulate different operating systems. Thus, programs
     written for the Win32 APIs and POSIX all run on Windows in the appropriate
     environment. Because of the modular structure, additional environmental sub-
     systems can be added without affecting the executive. In addition, Windows
     uses loadable drivers in the I/O system, so new file systems, new kinds of
     I/O devices, and new kinds of networking can be added while the system
     is running. Windows uses a client­server model like the Mach operating
     system and supports distributed processing by remote procedure calls (RPCs)
     as defined by the Open Software Foundation.
     19.2.6  Portability
     An operating system is portable if it can be moved from one CPU architecture
     to another with relatively few changes. Windows was designed to be portable.
     Like the UNIX operating system, Windows is written primarily in C and C++.
     The architecture-specific source code is relatively small, and there is very
     little use of assembly code. Porting Windows to a new architecture mostly
     affects the Windows kernel, since the user-mode code in Windows is almost
     exclusively written to be architecture independent. To port Windows, the
     kernel's architecture-specific code must be ported, and sometimes conditional
     compilation is needed in other parts of the kernel because of changes in major
     data structures, such as the page-table format. The entire Windows system
     must then be recompiled for the new CPU instruction set.
     Operating systems are sensitive not only to CPU architecture but also to CPU
     support chips and hardware boot programs. The CPU and support chips are
     collectively known as a chipset. These chipsets and the associated boot code
     determine how interrupts are delivered, describe the physical characteristics of
     each system, and provide interfaces to deeper aspects of the CPU architecture,
     such as error recovery and power management. It would be burdensome to
     have to port Windows to each type of support chip as well as to each CPU
     architecture. Instead, Windows isolates most of the chipset-dependent code in
     a dynamic link library (DLL), called the hardware-abstraction layer (HAL), that
     is loaded with the kernel. The Windows kernel depends on the HAL interfaces
     rather than on the underlying chipset details. This allows the single set of kernel
     and driver binaries for a particular CPU to be used with different chipsets simply
     by loading a different version of the HAL.
     Over the years, Windows has been ported to a number of different CPU
     architectures: Intel IA-32-compatible 32-bit CPUs, AMD64-compatible and IA64
     64-bit CPUs, the DEC Alpha, and the MIPS and PowerPC CPUs. Most of these
     CPU architectures failed in the market. When Windows 7 shipped, only the



                                 19.2                   Design Principles         837
IA-32 and AMD64 architectures were supported on client computers, along
with AMD64 and IA64 on servers.
19.2.7  International Support
Windows was designed for international and multinational use. It provides
support for different locales via the national-language-support (NLS) API.
The NLS API provides specialized routines to format dates, time, and money
in accordance with national customs. String comparisons are specialized to
account for varying character sets. UNICODE is Windows's native character
code. Windows supports ANSI characters by converting them to UNICODE
characters before manipulating them (8-bit to 16-bit conversion). System text
strings are kept in resource files that can be replaced to localize the system
for different languages. Multiple locales can be used concurrently, which is
important to multilingual individuals and businesses.
19.2.8  Energy Efficiency
Increasing energy efficiency for computers causes batteries to last longer for
laptops and netbooks, saves significant operating costs for power and cooling
of data centers, and contributes to green initiatives aimed at lowering energy
consumption by businesses and consumers. For some time, Windows has
implemented several strategies for decreasing energy use. The CPUs are moved
to lower power states--for example, by lowering clock frequency--whenever
possible. In addition, when a computer is not being actively used, Windows
may put the entire computer into a low-power state (sleep) or may even save
all of memory to disk and shut the computer off (hibernation). When the user
returns, the computer powers up and continues from its previous state, so the
user does not need to reboot and restart applications.
Windows 7 added some new strategies for saving energy. The longer a
CPU can stay unused, the more energy can be saved. Because computers are so
much faster than human beings, a lot of energy can be saved just while humans
are thinking. The problem is that too many programs are constantly polling to
see what is happening in the system. A swarm of software timers are firing,
keeping the CPU from staying idle long enough to save much energy. Windows
7 extends CPU idle time by skipping clock ticks, coalescing software timers into
smaller numbers of events, and "parking" entire CPUs when systems are not
heavily loaded.
19.2.9  Dynamic Device Support
Early in the history of the PC industry, computer configurations were fairly
static. Occasionally, new devices might be plugged into the serial, printer, or
game ports on the back of a computer, but that was it. The next steps toward
dynamic configuration of PCs were laptop docks and PCMIA cards. A PC could
suddenly be connected to or disconnected from a whole set of peripherals. In
a contemporary PC, the situation has completely changed. PCs are designed
to enable users to plug and unplug a huge host of peripherals all the time;
external disks, thumb drives, cameras, and the like are constantly coming and
going.



838   Chapter 19        Windows 7
              logon               OS/2            Win16           Win32         MS-DOS               POSIX
              process        applications  applications           applications  applications         applications
              security            OS/2     Win18                                MS-DOS               POSIX
              subsystem      subsystem            VDM                                VDM             subsystem
           authentication
              package
           security account
           manager database                            Win32
                                                      subsystem
                                                                                                     user mode
                                                       executive
           I/O manager                                                                        local
                             object        security    process    plug and      virtual   procedure
           file system       manager       reference   manager     play         memory        call          window
           cache                           monitor                manager       manager   facility   manager
           manager
           device
           drivers                                       kernel                                             graphic
              network                                                                                       device
              drivers                                                                                       drivers
                                           hardware abstraction layer
                                                       hardware
                                  Figure 19.1         Windows block diagram.
           Support for dynamic configuration of devices is continually evolving
      in   Windows.     The  system        can    automatically        recognize         devices     when          they
      are  plugged      in   and  can      find,    install,  and  load         the  appropriate     drivers --
      often without user intervention. When devices are unplugged, the drivers
      automatically unload, and system execution continues without disrupting
      other software.
19.3  System Components
      The architecture of Windows is a layered system of modules, as shown in Figure
      19.1. The main layers are the HAL, the kernel, and the executive, all of which
      run in kernel mode, and a collection of subsystems and services that run in user
      mode. The user-mode subsystems fall into two categories: the environmental
      subsystems, which emulate different operating systems, and the protection
      subsystems, which provide security functions. One of the chief advantages of
      this type of architecture is that interactions between modules are kept simple.
      The remainder of this section describes these layers and subsystems.
      19.3.1  Hardware-Abstraction Layer
      The HAL is the layer of software that hides hardware chipset differences from
      upper levels of the operating system. The HAL exports a virtual hardware



                                        19.3        System Components                839
interface that is used by the kernel dispatcher, the executive, and the device
drivers. Only a single version of each device driver is required for each
CPU architecture, no matter what support chips might be present. Device
drivers map devices and access them directly, but the chipset-specific details
of mapping memory, configuring I/O buses, setting up DMA, and coping with
motherboard-specific facilities are all provided by the HAL interfaces.
19.3.2    Kernel
The kernel layer of Windows has four main responsibilities: thread scheduling,
low-level processor synchronization, interrupt and exception handling, and
switching between user mode and kernel mode. The kernel is implemented in
the C language, using assembly language only where absolutely necessary to
interface with the lowest level of the hardware architecture.
     The kernel is organized according to object-oriented design principles. An
object type in Windows is a system-defined data type that has a set of attributes
(data values) and a set of methods (for example, functions or operations). An
object is an instance of an object type. The kernel performs its job by using a
set of kernel objects whose attributes store the kernel data and whose methods
perform the kernel activities.
19.3.2.1  Kernel Dispatcher
The  kernel  dispatcher  provides  the  foundation  for  the   executive  and        the
subsystems. Most of the dispatcher is never paged out of memory, and its exe-
cution is never preempted. Its main responsibilities are thread scheduling and
context switching, implementation of synchronization primitives, timer man-
agement, software interrupts (asynchronous and deferred procedure calls), and
exception dispatching.
19.3.2.2  Threads and Scheduling
Like many other modern operating systems, Windows uses processes and
threads for executable code. Each process has one or more threads, and each
thread has its own scheduling state, including actual priority, processor affinity,
and CPU usage information.
     There are six possible thread states: ready, standby, running, waiting,
transition, and terminated. Ready indicates that the thread is waiting to
run. The highest-priority ready thread is moved to the standby state, which
means it is the next thread to run. In a multiprocessor system, each processor
keeps one thread in a standby state. A thread is running when it is executing
on a processor. It runs until it is preempted by a higher-priority thread, until
it terminates, until its allotted execution time (quantum) ends, or until it waits
on a dispatcher object, such as an event signaling I/O completion. A thread is
in the waiting state when it is waiting for a dispatcher object to be signaled.
A thread is in the transition state while it waits for resources necessary for
execution; for example, it may be waiting for its kernel stack to be swapped in
from disk. A thread enters the terminated state when it finishes execution.
     The dispatcher uses a 32-level priority scheme to determine the order of
thread execution. Priorities are divided into two classes: variable class and
real-time class. The variable class contains threads having priorities from 1 to



840  Chapter 19  Windows 7
     15, and the real-time class contains threads with priorities ranging from 16
     to 31. The dispatcher uses a queue for each scheduling priority and traverses
     the set of queues from highest to lowest until it finds a thread that is ready
     to run. If a thread has a particular processor affinity but that processor is not
     available, the dispatcher skips past it and continues looking for a ready thread
     that is willing to run on the available processor. If no ready thread is found,
     the dispatcher executes a special thread called the idle thread. Priority class 0
     is reserved for the idle thread.
     When a thread's time quantum runs out, the clock interrupt queues a
     quantum-end deferred procedure call (DPC) to the processor. Queuing the
     DPC results in a software interrupt when the processor returns to normal
     interrupt priority. The software interrupt causes the dispatcher to reschedule
     the processor to execute the next available thread at the preempted thread's
     priority level.
     The priority of the preempted thread may be modified before it is placed
     back on the dispatcher queues. If the preempted thread is in the variable-
     priority class, its priority is lowered. The priority is never lowered below the
     base priority. Lowering the thread's priority tends to limit the CPU consumption
     of compute-bound threads versus I/O-bound threads. When a variable-priority
     thread is released from a wait operation, the dispatcher boosts the priority. The
     amount of the boost depends on the device for which the thread was waiting.
     For example, a thread waiting for keyboard I/O would get a large priority
     increase, whereas a thread waiting for a disk operation would get a moderate
     one. This strategy tends to give good response times to interactive threads
     using a mouse and windows. It also enables I/O-bound threads to keep the I/O
     devices busy while permitting compute-bound threads to use spare CPU cycles
     in the background. In addition, the thread associated with the user's active GUI
     window receives a priority boost to enhance its response time.
     Scheduling occurs when a thread enters the ready or wait state, when
     a thread terminates, or when an application changes a thread's priority or
     processor affinity. If a higher-priority thread becomes ready while a lower-
     priority  thread  is  running,    the  lower-priority  thread  is  preempted.  This
     preemption gives the higher-priority thread preferential access to the CPU.
     Windows is not a hard real-time operating system, however, because it does
     not guarantee that a real-time thread will start to execute within a particular
     time limit; threads are blocked indefinitely while DPCs and interrupt service
     routines (ISRs) are running (as further discussed below).
     Traditionally, operating-system schedulers used sampling to measure CPU
     utilization by threads. The system timer would fire periodically, and the timer
     interrupt handler would take note of what thread was currently scheduled and
     whether it was executing in user or kernel mode when the interrupt occurred.
     This sampling technique was necessary because either the CPU did not have
     a high-resolution clock or the clock was too expensive or unreliable to access
     frequently. Although efficient, sampling was inaccurate and led to anomalies
     such as incorporating interrupt servicing time as thread time and dispatching
     threads that had run for only a fraction of the quantum. Starting with Windows
     Vista, CPU time in Windows has been tracked using the hardware timestamp
     counter (TSC) included in recent processors. Using the TSC results in more
     accurate accounting of CPU usage, and the scheduler will not preempt threads
     before they have run for a full quantum.



                                       19.3  System Components                         841
19.3.2.3  Implementation of Synchronization Primitives
Key operating-system data structures are managed as objects using common
facilities for allocation, reference counting, and security. Dispatcher objects
control dispatching and synchronization in the system. Examples of these
objects include the following:
·  The event object is used to record an event occurrence and to synchronize
   this occurrence with some action. Notification events signal all waiting
   threads, and synchronization events signal a single waiting thread.
·  The mutant provides kernel-mode or user-mode mutual exclusion associ-
   ated with the notion of ownership.
·  The mutex, available only in kernel mode, provides deadlock-free mutual
   exclusion.
·  The semaphore object acts as a counter or gate to control the number of
   threads that access a resource.
·  The thread object is the entity that is scheduled by the kernel dispatcher.
   It is associated with a process object, which encapsulates a virtual address
   space. The thread object is signaled when the thread exits, and the process
   object, when the process exits.
·  The timer object is used to keep track of time and to signal timeouts when
   operations take too long and need to be interrupted or when a periodic
   activity needs to be scheduled.
Many of the dispatcher objects are accessed from user mode via an open
operation that returns a handle. The user-mode code polls or waits on handles
to synchronize with other threads as well as with the operating system (see
Section 19.7.1).
19.3.2.4  Software Interrupts: Asynchronous and Deferred Procedure Calls
The dispatcher implements two types of software interrupts: asynchronous
procedure calls (APCs) and deferred procedure calls (DPCs, mentioned earlier).
An asynchronous procedure call breaks into an executing thread and calls
a procedure. APCs are used to begin execution of new threads, suspend or
resume existing threads, terminate threads or processes, deliver notification
that an asynchronous I/O has completed, and extract the contents of the CPU
registers from a running thread. APCs are queued to specific threads and allow
the system to execute both system and user code within a process's context.
User-mode execution of an APC cannot occur at arbitrary times, but only when
the thread is waiting in the kernel and marked alertable.
   DPCsare used to postpone interrupt processing. After handling all urgent
device-interrupt processing, the ISR schedules the remaining processing by
queuing a DPC. The associated software interrupt will not occur until the CPU
is next at a priority lower than the priority of all I/O device interrupts but higher
than the priority at which threads run. Thus, DPCs do not block other device
ISRs. In addition to deferring device-interrupt processing, the dispatcher uses



842  Chapter 19  Windows 7
     DPCs to process timer expirations and to preempt thread execution at the end
     of the scheduling quantum.
        Execution of DPCs prevents threads from being scheduled on the current
     processor and also keeps APCs from signaling the completion of I/O. This is
     done so that completion of DPC routines does not take an extended amount
     of time. As an alternative, the dispatcher maintains a pool of worker threads.
     ISRs and DPCs may queue work items to the worker threads where they will be
     executed using normal thread scheduling. DPC routines are restricted so that
     they cannot take page faults (be paged out of memory), call system services,
     or take any other action that might result in an attempt to wait for a dispatcher
     object to be signaled. Unlike APCs, DPC routines make no assumptions about
     what process context the processor is executing.
     19.3.2.5  Exceptions and Interrupts
     The kernel dispatcher also provides trap handling for exceptions and interrupts
     generated by hardware or software. Windows defines several architecture-
     independent exceptions, including:
     ·  Memory-access violation
     ·  Integer overflow
     ·  Floating-point overflow or underflow
     ·  Integer divide by zero
     ·  Floating-point divide by zero
     ·  Illegal instruction
     ·  Data misalignment
     ·  Privileged instruction
     ·  Page-read error
     ·  Access violation
     ·  Paging file quota exceeded
     ·  Debugger breakpoint
     ·  Debugger single step
     The trap handlers deal with simple exceptions. Elaborate exception handling
     is performed by the kernel's exception dispatcher. The exception dispatcher
     creates an exception record containing the reason for the exception and finds
     an exception handler to deal with it.
        When an exception occurs in kernel mode, the exception dispatcher simply
     calls a routine to locate the exception handler. If no handler is found, a fatal
     system error occurs, and the user is left with the infamous "blue screen of
     death" that signifies system failure.
        Exception handling is more complex for user-mode processes, because
     an environmental subsystem (such as the POSIX system) sets up a debugger
     port and an exception port for every process it creates. (For details on ports,



                                                  19.3    System Components          843
see Section 19.3.3.4.) If a debugger port is registered, the exception handler
sends the exception to the port. If the debugger port is not found or does not
handle that exception, the dispatcher attempts to find an appropriate exception
handler. If no handler is found, the debugger is called again to catch the error
for debugging. If no debugger is running, a message is sent to the process's
exception port to give the environmental subsystem a chance to translate the
exception. For example, the POSIX environment translates Windows exception
messages into POSIX signals before sending them to the thread that caused
the exception. Finally, if nothing else works, the kernel simply terminates the
process containing the thread that caused the exception.
When Windows fails to handle an exception, it may construct a description
of the error that occurred and request permission from the user to send the
information back to Microsoft for further analysis. In some cases, Microsoft's
automated analysis may be able to recognize the error immediately and suggest
a fix or workaround.
The interrupt dispatcher in the kernel handles interrupts by calling either
an interrupt service routine (ISR) supplied by a device driver or a kernel
trap-handler routine. The interrupt is represented by an interrupt object that
contains all the information needed to handle the interrupt. Using an interrupt
object makes it easy to associate interrupt-service routines with an interrupt
without having to access the interrupt hardware directly.
Different processor architectures have different types and numbers of inter-
rupts. For portability, the interrupt dispatcher maps the hardware interrupts
into a standard set. The interrupts are prioritized and are serviced in priority
order. There are 32 interrupt request levels (IRQLs) in Windows. Eight are
reserved for use by the kernel; the remaining 24 represent hardware interrupts
via the HAL (although most IA-32 systems use only 16). The Windows interrupts
are defined in Figure 19.2.
The kernel uses an interrupt-dispatch table to bind each interrupt level
to a service routine. In a multiprocessor computer, Windows keeps a separate
interrupt-dispatch table (IDT) for each processor, and each processor's IRQL can
be set independently to mask out interrupts. All interrupts that occur at a level
equal to or less than the IRQL of a processor are blocked until the IRQL is lowered
interrupt levels                  types of interrupts
31                    machine check or bus error
30                    power fail
29                    interprocessor notification (request another processor
                      to act; e.g., dispatch a process or update the TLB)
28                    clock (used to keep track of time)
27                    profile
3­26                  traditional PC IRQ hardware interrupts
2                     dispatch and deferred procedure call (DPC) (kernel)
1                     asynchronous procedure call (APC)
0                     passive
                  Figure 19.2     Windows interrupt-request levels.



844  Chapter 19  Windows 7
     by a kernel-level thread or by an ISR returning from interrupt processing.
     Windows takes advantage of this property and uses software interrupts to
     deliver APCs and DPCs, to perform system functions such as synchronizing
     threads with I/O completion, to start thread execution, and to handle timers.
     19.3.2.6  Switching between User-Mode and Kernel-Mode Threads
     What the programmer thinks of as a thread in traditional Windows is actually
     two threads: a user-mode thread (UT) and a kernel-mode thread (KT). Each has
     its own stack, register values, and execution context. A UT requests a system
     service by executing an instruction that causes a trap to kernel mode. The kernel
     layer runs a trap handler that switches between the UT and the corresponding
     KT. When a KT has completed its kernel execution and is ready to switch back
     to the corresponding UT, the kernel layer is called to make the switch to the UT,
     which continues its execution in user mode.
     Windows 7 modifies the behavior of the kernel layer to support user-
     mode scheduling of the UTs. User-mode schedulers in Windows 7 support
     cooperative scheduling. A UT can explicitly yield to another UT by calling
     the user-mode scheduler; it is not necessary to enter the kernel. User-mode
     scheduling is explained in more detail in Section 19.7.3.7.
     19.3.3    Executive
     The Windows executive provides a set of services that all environmental
     subsystems use. The services are grouped as follows: object manager, virtual
     memory manager, process manager, advanced local procedure call facility, I/O
     manager, cache manager, security reference monitor, plug-and-play and power
     managers, registry, and booting.
     19.3.3.1  Object Manager
     For managing kernel-mode entities, Windows uses a generic set of interfaces
     that are manipulated by user-mode programs. Windows calls these entities
     objects, and the executive component that manipulates them is the object
     manager. Examples of objects are semaphores, mutexes, events, processes,
     and threads; all these are dispatcher objects. Threads can block in the kernel
     dispatcher waiting for any of these objects to be signaled. The process, thread,
     and virtual memory APIs use process and thread handles to identify the process
     or thread to be operated on. Other examples of objects include files, sections,
     ports, and various internal I/O objects. File objects are used to maintain the open
     state of files and devices. Sections are used to map files. Local-communication
     endpoints are implemented as port objects.
     User-mode code accesses these objects using an opaque value called a
     handle, which is returned by many APIs. Each process has a handle table
     containing entries that track the objects used by the process. The system
     process, which contains the kernel, has its own handle table, which is protected
     from user code. The handle tables in Windows are represented by a tree
     structure, which can expand from holding 1,024 handles to holding over 16
     million. Kernel-mode code can access an object by using either a handle or a
     referenced pointer.



                                             19.3  System Components                 845
A process gets a handle by creating an object, by opening an existing
object, by receiving a duplicated handle from another process, or by inheriting
a handle from the parent process. When a process exits, all its open handles
are implicitly closed. Since the object manager is the only entity that generates
object handles, it is the natural place to check security. The object manager
checks whether a process has the right to access an object when the process
tries to open the object. The object manager also enforces quotas, such as the
maximum amount of memory a process may use, by charging a process for the
memory occupied by all its referenced objects and refusing to allocate more
memory when the accumulated charges exceed the process's quota.
The object manager keeps track of two counts for each object: the number
of handles for the object and the number of referenced pointers. The handle
count is the number of handles that refer to the object in the handle tables
of all processes, including the system process that contains the kernel. The
referenced pointer count is incremented whenever a new pointer is needed
by the kernel and decremented when the kernel is done with the pointer. The
purpose of these reference counts is to ensure that an object is not freed while
it is still referenced by either a handle or an internal kernel pointer.
The  object  manager  maintains        the   Windows      internal  name     space.  In
contrast to UNIX, which roots the system name space in the file system,
Windows uses an abstract name space and connects the file systems as devices.
Whether a Windows object has a name is up to its creator. Processes and
threads are created without names and referenced either by handle or through
a separate numerical identifier. Synchronization events usually have names,
so that they can be opened by unrelated processes. A name can be either
permanent or temporary. A permanent name represents an entity, such as a
disk drive, that remains even if no process is accessing it. A temporary name
exists only while a process holds a handle to the object. The object manager
supports directories and symbolic links in the name space. As an example,
MS-DOS drive letters are implemented using symbolic links; \Global??\C: is
a symbolic link to the device object \Device\HarddiskVolume2, representing a
mounted file-system volume in the \Device directory.
Each object, as mentioned earlier, is an instance of an object type. The
object type specifies how instances are to be allocated, how the data fields are
to be defined, and how the standard set of virtual functions used for all objects
are to be implemented. The standard functions implement operations such as
mapping names to objects, closing and deleting, and applying security checks.
Functions that are specific to a particular type of object are implemented by
system services designed to operate on that particular object type, not by the
methods specified in the object type.
The  parse()     function  is  the     most  interesting  of  the   standard  object
functions. It allows the implementation of an object. The file systems, the
registry configuration store, and GUI objects are the most notable users of
parse functions to extend the Windows name space.
Returning    to  our  Windows  naming        example,     device    objects  used    to
represent file-system volumes provide a parse function. This allows a name like
\Global??\C:\foo\bar.doc to be interpreted as the file \foo\bar.doc on the
volume represented by the device object HarddiskVolume2. We can illustrate
how naming, parse functions, objects, and handles work together by looking
at the steps to open the file in Windows:



846  Chapter 19  Windows 7
     1.  An application requests that a file named C:\foo\bar.doc be opened.
     2.  The object manager finds the device object HarddiskVolume2, looks up
         the parse procedure IopParseDevice from the object's type, and invokes
         it with the file's name relative to the root of the file system.
     3.  IopParseDevice() allocates a file object and passes it to the file system,
         which fills in the details of how to access C:\foo\bar.doc on the volume.
     4.  When the file system returns, IopParseDevice() allocates an entry for
         the file object in the handle table for the current process and returns the
         handle to the application.
         If the file cannot successfully be opened, IopParseDevice() deletes the
     file object it allocated and returns an error indication to the application.
     19.3.3.2  Virtual Memory Manager
     The executive component that manages the virtual address space, physical
     memory allocation, and paging is the virtual memory (VM) manager. The
     design of the VM manager assumes that the underlying hardware supports
     virtual-to-physical  mapping,   a  paging  mechanism,  and  transparent       cache
     coherence on multiprocessor systems, as well as allowing multiple page-table
     entries to map to the same physical page frame. The VM manager in Windows
     uses a page-based management scheme with page sizes of 4 KB and 2 MB on
     AMD64 and IA-32-compatible processors and 8 KB on the IA64. Pages of data
     allocated to a process that are not in physical memory are either stored in the
     paging files on disk or mapped directly to a regular file on a local or remote
     file system. A page can also be marked zero-fill-on-demand, which initializes
     the page with zeros before it is allocated, thus erasing the previous contents.
         On IA-32 processors, each process has a 4-GB virtual address space. The
     upper 2 GB are mostly identical for all processes and are used by Windows in
     kernel mode to access the operating-system code and data structures. For the
     AMD64 architecture, Windows provides a 8-TB virtual address space for user
     mode out of the 16 EB supported by existing hardware for each process.
         Key areas of the kernel-mode region that are not identical for all processes
     are the self-map, hyperspace, and session space. The hardware references a
     process's page table using physical page-frame numbers, and the page table
     self-map makes the contents of the process's page table accessible using virtual
     addresses. Hyperspace maps the current process's working-set information
     into the kernel-mode address space. Session space is used to share an instance
     of the Win32 and other session-specific drivers among all the processes in
     the same terminal-server (TS) session. Different TS sessions share different
     instances of these drivers, yet they are mapped at the same virtual addresses.
     The lower, user-mode region of virtual address space is specific to each process
     and accessible by both user- and kernel-mode threads.
         The Windows VM manager uses a two-step process to allocate virtual
     memory. The first step reserves one or more pages of virtual addresses in
     the process's virtual address space. The second step commits the allocation by
     assigning virtual memory space (physical memory or space in the paging files).
     Windows limits the amount of virtual memory space a process consumes by
     enforcing a quota on committed memory. A process decommits memory that it



                                       19.3  System Components                       847
is no longer using to free up virtual memory space for use by other processes.
The APIs used to reserve virtual addresses and commit virtual memory take a
handle on a process object as a parameter. This allows one process to control the
virtual memory of another. Environmental subsystems manage the memory of
their client processes in this way.
   Windows implements shared memory by defining a section object. After
getting a handle to a section object, a process maps the memory of the section to
a range of addresses, called a view. A process can establish a view of the entire
section or only the portion it needs. Windows allows sections to be mapped
not just into the current process but into any process for which the caller has a
handle.
   Sections can be used in many ways. A section can be backed by disk space
either in the system-paging file or in a regular file (a memory-mapped file). A
section can be based, meaning that it appears at the same virtual address for all
processes attempting to access it. Sections can also represent physical memory,
allowing a 32-bit process to access more physical memory than can fit in its
virtual address space. Finally, the memory protection of pages in the section
can be set to read-only, read-write, read-write-execute, execute-only, no access,
or copy-on-write.
   Let's look more closely at the last two of these protection settings:
·  A no-access page raises an exception if accessed. The exception can be
   used, for example, to check whether a faulty program iterates beyond
   the end of an array or simply to detect that the program attempted to
   access virtual addresses that are not committed to memory. User- and
   kernel-mode stacks use no-access pages as guard pages to detect stack
   overflows. Another use is to look for heap buffer overruns. Both the user-
   mode memory allocator and the special kernel allocator used by the device
   verifier can be configured to map each allocation onto the end of a page,
   followed by a no-access page to detect programming errors that access
   beyond the end of an allocation.
·  The copy-on-write mechanism enables the VM manager to use physical
   memory more efficiently. When two processes want independent copies of
   data from the same section object, the VM manager places a single shared
   copy into virtual memory and activates the copy-on-write property for
   that region of memory. If one of the processes tries to modify data in a
   copy-on-write page, the VM manager makes a private copy of the page for
   the process.
   The virtual address translation in Windows uses a multilevel page table. For
IA-32 and AMD64 processors, each process has a page directory that contains
512 page-directory entries (PDEs) 8 bytes in size. Each PDE points to a PTE table
that contains 512 page-table entries (PTEs) 8 bytes in size. Each PTE points to
a 4-KB page frame in physical memory. For a variety of reasons, the hardware
requires that the page directories or PTE tables at each level of a multilevel page
table occupy a single page. Thus, the number of PDEs or PTEs that fit in a page
determine how many virtual addresses are translated by that page. See Figure
19.3 for a diagram of this structure.
   The structure described so far can be used to represent only 1 GB of
virtual address translation. For IA-32, a second page-directory level is needed,



848  Chapter 19    Windows 7
                                Page directory pointer table
                    Pointer 0        Pointer 1  Pointer 2         Pointer 3
            Page    Page             Page             Page          Page           Page
        directory   directory   directory             directory     directory      directory
        entry 0     0           entry 511             entry 0       3              entry 511
        Page table  Page        Page table            Page table    Page           Page table
        entry 0     table 0     entry 511             entry 0       table 511      entry 511
            4 KB                     4 KB             4 KB                         4 KB
            page                     page             page                         page
                                Figure 19.3     Page-table layout.
     containing only four entries, as shown in the diagram. On 64-bit processors,
     more levels are needed. For AMD64, Windows uses a total of four full levels.
     The total size of all page-table pages needed to fully represent even a 32-bit
     virtual address space for a process is 8 MB. The VM manager allocates pages of
     PDEs and PTEs as needed and moves page-table pages to disk when not in use.
     The page-table pages are faulted back into memory when referenced.
        We   next   consider    how  virtual    addresses   are     translated     into  physical
     addresses on IA-32-compatible processors. A 2-bit value can represent the
     values 0, 1, 2, 3. A 9-bit value can represent values from 0 to 511; a 12-bit
     value, values from 0 to 4,095. Thus, a 12-bit value can select any byte within a
     4-KB page of memory. A 9-bit value can represent any of the 512 PDEs or PTEs
     in a page directory or PTE-table page. As shown in Figure 19.4, translating a
     virtual address pointer to a byte address in physical memory involves breaking
     the 32-bit pointer into four values, starting from the most significant bits:
     ·  Two bits are used to index into the four PDEs at the top level of the page
        table. The selected PDE will contain the physical page number for each of
        the four page-directory pages that map 1 GB of the address space.
        31                                                                               0
        PTR         PDE index              PTE index                page offset
                   Figure 19.4  Virtual-to-physical address translation on IA-32.



                       19.3  System Components                                        849
·  Nine bits are used to select another PDE, this time from a second-level page
   directory. This PDE will contain the physical page numbers of up to 512
   PTE-table pages.
·  Nine bits are used to select one of 512 PTEs from the selected PTE-table
   page. The selected PTE will contain the physical page number for the byte
   we are accessing.
·  Twelve bits are used as the byte offset into the page. The physical address
   of the byte we are accessing is constructed by appending the lowest 12 bits
   of the virtual address to the end of the physical page number we found in
   the selected PTE.
   The number of bits in a physical address may be different from the number
of bits in a virtual address. In the original IA-32 architecture, the PTE and PDE
were 32-bit structures that had room for only 20 bits of physical page number,
so the physical address size and the virtual address size were the same. Such
systems could address only 4 GB of physical memory. Later, the IA-32 was
extended to the larger 64-bit PTE size used today, and the hardware supported
24-bit physical addresses. These systems could support 64 GB and were used
on server systems. Today, all Windows servers are based on either the AMD64
or the IA64 and support very, very large physical addresses--more than we
can possibly use. (Of course, once upon a time 4 GB seemed optimistically large
for physical memory.)
   To improve performance, the VM manager maps the page-directory and
PTE-table pages into the same contiguous region of virtual addresses in every
process. This self-map allows the VM manager to use the same pointer to access
the current PDE or PTE corresponding to a particular virtual address no matter
what process is running. The self-map for the IA-32 takes a contiguous 8-MB
region of kernel virtual address space; the AMD64 self-map occupies 512 GB.
Although the self-map occupies significant address space, it does not require
any additional virtual memory pages. It also allows the page table's pages to
be automatically paged in and out of physical memory.
   In the creation of a self-map, one of the PDEs in the top-level page directory
refers to the page-directory page itself, forming a "loop" in the page-table
translations. The virtual pages are accessed if the loop is not taken, the PTE-table
pages are accessed if the loop is taken once, the lowest-level page-directory
pages are accessed if the loop is taken twice, and so forth.
   The additional levels of page directories used for 64-bit virtual memory are
translated in the same way except that the virtual address pointer is broken up
into even more values. For the AMD64, Windows uses four full levels, each of
which maps 512 pages, or 9+9+9+9+12 = 48 bits of virtual address.
   To avoid the overhead of translating every virtual address by looking up
the PDE and PTE, processors use translation look-aside buffer (TLB) hardware,
which contains an associative memory cache for mapping virtual pages to
PTEs. The TLB is part of the memory-management unit (MMU) within each
processor. The MMU needs to "walk" (navigate the data structures of) the page
table in memory only when a needed translation is missing from the TLB.
   The PDEs and PTEs contain more than just physical page numbers. They
also have bits reserved for operating-system use and bits that control how the
hardware uses memory, such as whether hardware caching should be used for



850  Chapter 19  Windows 7
     each page. In addition, the entries specify what kinds of access are allowed for
     both user and kernel modes.
        A PDE can also be marked to say that it should function as a PTE rather
     than a PDE. On a IA-32, the first 11 bits of the virtual address pointer select a
     PDE in the first two levels of translation. If the selected PDE is marked to act
     as a PTE, then the remaining 21 bits of the pointer are used as the offset of
     the byte. This results in a 2-MB size for the page. Mixing and matching 4-KB
     and 2-MB page sizes within the page table is easy for the operating system and
     can significantly improve the performance of some programs by reducing how
     often the MMU needs to reload entries in the TLB, since one PDE mapping 2 MB
     replaces 512 PTEs each mapping 4 KB.
        Managing physical memory so that 2-MB pages are available when needed
     is difficult, however, as they may continually be broken up into 4 KB pages,
     causing external fragmentation of memory. Also, the large pages can result
     in very significant internal fragmentation. Because of these problems, it is
     typically only Windows itself, along with large server applications, that use
     large pages to improve the performance of the TLB. They are better suited to do
     so because operating-system and server applications start running when the
     system boots, before memory has become fragmented.
        Windows manages physical memory by associating each physical page
     with one of seven states: free, zeroed, modified, standby, bad, transition, or
     valid.
     ·  A free page is a page that has no particular content.
     ·  A zeroed page is a free page that has been zeroed out and is ready for
        immediate use to satisfy zero-on-demand faults.
     ·  A modified page has been written by a process and must be sent to the
        disk before it is allocated for another process.
     ·  A standby page is a copy of information already stored on disk. Standby
        pages may be pages that were not modified, modified pages that have
        already been written to the disk, or pages that were prefetched because
        they are expected to be used soon.
     ·  A bad page is unusable because a hardware error has been detected.
     ·  A transition page is on its way in from disk to a page frame allocated in
        physical memory.
     ·  A valid page is part of the working set of one or more processes and is
        contained within these processes' page tables.
        While valid pages are contained in processes' page tables, pages in other
     states are kept in separate lists according to state type. The lists are constructed
     by linking the corresponding entries in the page frame number (PFN) database,
     which includes an entry for each physical memory page. The PFN entries also
     include information such as reference counts, locks, and NUMA information.
     Note that the PFN database represents pages of physical memory, whereas the
     PTEs represent pages of virtual memory.
        When the valid bit in a PTE is zero, hardware ignores all the other bits, and
     the VM manager can define them for its own use. Invalid pages can have a
     number of states represented by bits in the PTE. Page-file pages that have never



                           19.3                 System Components                        851
63                                                                                   32
                           Page file offset
31                                                                                   0
                                             T  P            prot              Page  V
                                                                               file
              Figure 19.5  Page-file page-table entry. The valid bit is zero.
been faulted in are marked zero-on-demand. Pages mapped through section
objects encode a pointer to the appropriate section object. PTEs for pages that
have been written to the page file contain enough information to locate the
page on disk, and so forth. The structure of the page-file PTE is shown in Figure
19.5. The T, P, and V bits are all zero for this type of PTE. The PTE includes 5 bits
for page protection, 32 bits for page-file offset, and 4 bits to select the paging
file. There are also 20 bits reserved for additional bookkeeping.
Windows uses a per-working-set, least-recently-used (LRU) replacement
policy to take pages from processes as appropriate. When a process is started,
it is assigned a default minimum working-set size. The working set of each
process is allowed to grow until the amount of remaining physical memory
starts to run low, at which point the VM manager starts to track the age of
the pages in each working set. Eventually, when the available memory runs
critically low, the VM manager trims the working set to remove older pages.
How old a page is depends not on how long it has been in memory but on
when it was last referenced. This is determined by periodically making a pass
through the working set of each process and incrementing the age for pages
that have not been marked in the PTE as referenced since the last pass. When
it becomes necessary to trim the working sets, the VM manager uses heuristics
to decide how much to trim from each process and then removes the oldest
pages first.
A process can have its working set trimmed even when plenty of memory
is available, if it was given a hard limit on how much physical memory it could
use. In Windows 7, the VM manager will also trim processes that are growing
rapidly, even if memory is plentiful. This policy change significantly improves
the responsiveness of the system for other processes.
Windows tracks working sets not only for user-mode processes but also
for the system process, which includes all the pageable data structures and
code that run in kernel mode. Windows 7 created additional working sets for
the system process and associated them with particular categories of kernel
memory; the file cache, kernel heap, and kernel code now have their own
working sets. The distinct working sets allow the VM manager to use different
policies to trim the different categories of kernel memory.



852  Chapter 19  Windows 7
         The VM manager does not fault in only the page immediately needed.
     Research shows that the memory referencing of a thread tends to have a locality
     property. That is, when a page is used, it is likely that adjacent pages will be
     referenced in the near future. (Think of iterating over an array or fetching
     sequential instructions that form the executable code for a thread.) Because of
     locality, when the VM manager faults in a page, it also faults in a few adjacent
     pages. This prefetching tends to reduce the total number of page faults and
     allows reads to be clustered to improve I/O performance.
         In addition to managing committed memory, the VM manager manages
     each process's reserved memory, or virtual address space. Each process has an
     associated tree that describes the ranges of virtual addresses in use and what
     the uses are. This allows the VM manager to fault in page-table pages as needed.
     If the PTE for a faulting address is uninitialized, the VM manager searches for
     the address in the process's tree of virtual address descriptors (VADs) and
     uses this information to fill in the PTE and retrieve the page. In some cases, a
     PTE-table page itself may not exist; such a page must be transparently allocated
     and initialized by the VM manager. In other cases, the page may be shared as
     part of a section object, and the VAD will contain a pointer to that section object.
     The section object contains information on how to find the shared virtual page
     so that the PTE can be initialized to point at it directly.
     19.3.3.3  Process Manager
     The Windows process manager provides services for creating, deleting, and
     using processes, threads, and jobs. It has no knowledge about parent­child
     relationships or process hierarchies; those refinements are left to the particular
     environmental subsystem that owns the process. The process manager is also
     not involved in the scheduling of processes, other than setting the priorities and
     affinities in processes and threads when they are created. Thread scheduling
     takes place in the kernel dispatcher.
         Each process contains one or more threads. Processes themselves can be
     collected into larger units called job objects. The use of job objects allows
     limits to be placed on CPU usage, working-set size, and processor affinities
     that control multiple processes at once. Job objects are used to manage large
     data-center machines.
         An example of process creation in the Win32 environment is as follows:
     1.  A Win32 application calls CreateProcess().
     2.  A message is sent to the Win32 subsystem to notify it that the process is
         being created.
     3.  CreateProcess() in the original process then calls an API in the process
         manager of the NT executive to actually create the process.
     4.  The process manager calls the object manager to create a process object
         and returns the object handle to Win32.
     5.  Win32 calls the process manager again to create a thread for the process
         and returns handles to the new process and thread.
         The Windows APIs for manipulating virtual memory and threads and
     for duplicating handles take a process handle, so subsystems can perform



                                    19.3          System Components                     853
operations on behalf of a new process without having to execute directly in
the new process's context. Once a new process is created, the initial thread
is created, and an asynchronous procedure call is delivered to the thread to
prompt the start of execution at the user-mode image loader. The loader is
in ntdll.dll, which is a link library automatically mapped into every newly
created process. Windows also supports a UNIX fork() style of process creation
in order to support the POSIX environmental subsystem. Although the Win32
environment calls the process manager directly from the client process, POSIX
uses the cross-process nature of the Windows APIs to create the new process
from within the subsystem process.
The process manager relies on the asynchronous procedure calls (APCs)
implemented by the kernel layer. APCs are used to initiate thread execution,
suspend and resume threads, access thread registers, terminate threads and
processes, and support debuggers.
The debugger support in the process manager includes the APIs to suspend
and resume threads and to create threads that begin in suspended mode. There
are also process-manager APIs that get and set a thread's register context and
access another process's virtual memory. Threads can be created in the current
process; they can also be injected into another process. The debugger makes
use of thread injection to execute code within a process being debugged.
While running in the executive, a thread can temporarily attach to a
different process. Thread attach is used by kernel worker threads that need to
execute in the context of the process originating a work request. For example,
the VM manager might use thread attach when it needs access to a process's
working set or page tables, and the I/O manager might use it in updating the
status variable in a process for asynchronous I/O operations.
The process manager also supports impersonation. Each thread has an
associated security token. When the login process authenticates a user, the
security token is attached to the user's process and inherited by its child
processes. The token contains the security identity (SID) of the user, the SIDs of
the groups the user belongs to, the privileges the user has, and the integrity level
of the process. By default, all threads within a process share a common token,
representing the user and the application that started the process. However, a
thread running in a process with a security token belonging to one user can set
a thread-specific token belonging to another user to impersonate that user.
The impersonation facility is fundamental to the client­server RPC model,
where services must act on behalf of a variety of clients with different security
IDs. The right to impersonate a user is most often delivered as part of an RPC
connection from a client process to a server process. Impersonation allows the
server to access system services as if it were the client in order to access or create
objects and files on behalf of the client. The server process must be trustworthy
and must be carefully written to be robust against attacks. Otherwise, one client
could take over a server process and then impersonate any user who made a
subsequent client request.
19.3.3.4  Facilities for Client­Server Computing
The implementation of Windows uses a client­server model throughout. The
environmental subsystems are servers that implement particular operating-
system personalities. Many other services, such as user authentication, network



854  Chapter 19   Windows 7
     facilities,  printer  spooling,  web  services,  network  file   systems,     and  plug-
     and-play, are also implemented using this model. To reduce the memory
     footprint, multiple services are often collected into a few processes running
     the svchost.exe program. Each service is loaded as a dynamic-link library
     (DLL), which implements the service by relying on the user-mode thread-pool
     facilities to share threads and wait for messages (see Section 19.3.3.3).
         The normal implementation paradigm for client­server computing is to
     use RPCs to communicate requests. The Win32 API supports a standard RPC
     protocol, as described in Section 19.6.2.7. RPC uses multiple transports (for
     example, named pipes and TCP/IP) and can be used to implement RPCs between
     systems. When an RPC always occurs between a client and server on the local
     system, the advanced local procedure call facility (ALPC) can be used as the
     transport. At the lowest level of the system, in the implementation of the
     environmental systems, and for services that must be available in the early
     stages of booting, RPC is not available. Instead, native Windows services use
     ALPC directly.
         ALPC is a message-passing mechanism. The server process publishes a
     globally visible connection-port object. When a client wants services from
     a subsystem or service, it opens a handle to the server's connection-port
     object  and  sends    a  connection   request    to  the  port.  The  server  creates  a
     channel and returns a handle to the client. The channel consists of a pair of
     private communication ports: one for client-to-server messages and the other
     for server-to-client messages. Communication channels support a callback
     mechanism, so the client and server can accept requests when they would
     normally be expecting a reply.
         When an ALPC channel is created, one of three message-passing techniques
     is chosen.
     1.  The first technique is suitable for small to medium messages (up to 63
         KB). In this case, the port's message queue is used as intermediate storage,
         and the messages are copied from one process to the other.
     2.  The second technique is for larger messages. In this case, a shared-
         memory section object is created for the channel. Messages sent through
         the port's message queue contain a pointer and size information referring
         to the section object. This avoids the need to copy large messages. The
         sender places data into the shared section, and the receiver views them
         directly.
     3.  The third technique uses APIs that read and write directly into a process's
         address space. ALPC provides functions and synchronization so that a
         server can access the data in a client. This technique is normally used by
         RPC to achieve higher performance for specific scenarios.
         The Win32 window manager uses its own form of message passing, which is
     independent of the executive ALPC facilities. When a client asks for a connection
     that uses window-manager messaging, the server sets up three objects: (1) a
     dedicated server thread to handle requests, (2) a 64-KB shared section object,
     and (3) an event-pair object. An event-pair object is a synchronization object
     used by the Win32 subsystem to provide notification when the client thread



                                          19.3     System Components                  855
has copied a message to the Win32 server, or vice versa. The section object is
used to pass the messages, and the event-pair object provides synchronization.
   Window-manager messaging has several advantages:
·  The section object eliminates message copying, since it represents a region
   of shared memory.
·  The event-pair object eliminates the overhead of using the port object to
   pass messages containing pointers and lengths.
·  The dedicated server thread eliminates the overhead of determining which
   client thread is calling the server, since there is one server thread per client
   thread.
·  The kernel gives scheduling preference to these dedicated server threads
   to improve performance.
19.3.3.5  I/O Manager
The I/O manager is responsible for managing file systems, device drivers, and
network drivers. It keeps track of which device drivers, filter drivers, and file
systems are loaded, and it also manages buffers for I/O requests. It works
with the VM manager to provide memory-mapped file I/O and controls the
Windows cache manager, which handles caching for the entire I/O system. The
I/O manager is fundamentally asynchronous, providing synchronous I/O by
explicitly waiting for an I/O operation to complete. The I/O manager provides
several models of asynchronous I/O completion, including setting of events,
updating of a status variable in the calling process, delivery of APCs to initiating
threads, and use of I/O completion ports, which allow a single thread to process
I/O completions from many other threads.
   Device drivers are arranged in a list for each device (called a driver or
I/O stack). A driver is represented in the system as a driver object. Because a
single driver can operate on multiple devices, the drivers are represented in
the I/O stack by a device object, which contains a link to the driver object.
The I/O manager converts the requests it receives into a standard form called
an I/O request packet (IRP). It then forwards the IRP to the first driver in the
targeted I/O stack for processing. After a driver processes the IRP, it calls the
I/O manager either to forward the IRP to the next driver in the stack or, if all
processing is finished, to complete the operation represented by the IRP.
   The I/O request may be completed in a context different from the one in
which it was made. For example, if a driver is performing its part of an I/O
operation and is forced to block for an extended time, it may queue the IRP to
a worker thread to continue processing in the system context. In the original
thread, the driver returns a status indicating that the I/O request is pending so
that the thread can continue executing in parallel with the I/O operation. An
IRP may also be processed in interrupt-service routines and completed in an
arbitrary process context. Because some final processing may need to take place
in the context that initiated the I/O, the I/O manager uses an APC to do final
I/O-completion processing in the process context of the originating thread.
   The I/O stack model is very flexible. As a driver stack is built, various
drivers have the opportunity to insert themselves into the stack as filter drivers.
Filter drivers can examine and potentially modify each I/O operation. Mount



856  Chapter 19  Windows 7
     management, partition management, and disk striping and mirroring are all
     examples of functionality implemented using filter drivers that execute beneath
     the file system in the stack. File-system filter drivers execute above the file
     system and have been used to implement functionalities such as hierarchical
     storage management, single instancing of files for remote boot, and dynamic
     format conversion. Third parties also use file-system filter drivers to implement
     virus detection.
         Device drivers for Windows are written to the Windows Driver Model
     (WDM) specification. This model lays out all the requirements for device drivers,
     including how to layer filter drivers, share common code for handling power
     and plug-and-play requests, build correct cancellation logic, and so forth.
         Because of the richness of the WDM, writing a full WDM device driver
     for each new hardware device can involve a great deal of work. Fortunately,
     the port/miniport model makes it unnecessary to do this. Within a class of
     similar devices, such as audio drivers, SATA devices, or Ethernet controllers,
     each instance of a device shares a common driver for that class, called a port
     driver. The port driver implements the standard operations for the class and
     then calls device-specific routines in the device's miniport driver to implement
     device-specific functionality. The TCP/IP network stack is implemented in
     this way, with the ndis.sys class driver implementing much of the network
     driver functionality and calling out to the network miniport drivers for specific
     hardware.
         Recent versions of Windows, including Windows 7, provide additional
     simplifications for writing device drivers for hardware devices. Kernel-mode
     drivers can now be written using the Kernel-Mode Driver Framework (KMDF),
     which provides a simplified programming model for drivers on top of WDM.
     Another option is the User-Mode Driver Framework (UMDF). Many drivers
     do not need to operate in kernel mode, and it is easier to develop and deploy
     drivers in user mode. It also makes the system more reliable, because a failure
     in a user-mode driver does not cause a kernel-mode crash.
     19.3.3.6  Cache Manager
     In  many    operating  systems,  caching  is  done  by  the  file  system.   Instead,
     Windows provides a centralized caching facility. The cache manager works
     closely with the VM manager to provide cache services for all components
     under the control of the I/O manager. Caching in Windows is based on files
     rather than raw blocks. The size of the cache changes dynamically according
     to how much free memory is available in the system. The cache manager
     maintains a private working set rather than sharing the system process's
     working set. The cache manager memory-maps files into kernel memory and
     then uses special interfaces to the VM manager to fault pages into or trim them
     from this private working set.
         The cache is divided into blocks of 256 KB. Each cache block can hold a
     view (that is, a memory-mapped region) of a file. Each cache block is described
     by a virtual address control block (VACB) that stores the virtual address and
     file offset for the view, as well as the number of processes using the view. The
     VACBs reside in a single array maintained by the cache manager.
         When the I/O manager receives a file's user-level read request, the I/O
     manager sends an IRP to the I/O stack for the volume on which the file resides.



                                            19.3       System Components            857
For files that are marked as cacheable, the file system calls the cache manager to
look up the requested data in its cached file views. The cache manager calculates
which entry of that file's VACB index array corresponds to the byte offset of
the request. The entry either points to the view in the cache or is invalid. If it
is invalid, the cache manager allocates a cache block (and the corresponding
entry in the VACB array) and maps the view into the cache block. The cache
manager then attempts to copy data from the mapped file to the caller's buffer.
If the copy succeeds, the operation is completed.
If the copy fails, it does so because of a page fault, which causes the VM
manager to send a noncached read request to the I/O manager. The I/O manager
sends another request down the driver stack, this time requesting a paging
operation, which bypasses the cache manager and reads the data from the file
directly into the page allocated for the cache manager. Upon completion, the
VACB is set to point at the page. The data, now in the cache, are copied to the
caller's buffer, and the original I/O request is completed. Figure 19.6 shows an
overview of these operations.
A kernel-level read operation is similar, except that the data can be accessed
directly from the cache rather than being copied to a buffer in user space. To
use file-system metadata (data structures that describe the file system), the
kernel uses the cache manager's mapping interface to read the metadata.
To modify the metadata, the file system uses the cache manager's pinning
interface. Pinning a page locks the page into a physical-memory page frame
so that the VM manager cannot move the page or page it out. After updating
the metadata, the file system asks the cache manager to unpin the page. A
modified page is marked dirty, and so the VM manager flushes the page to
disk.
To improve performance, the cache manager keeps a small history of read
requests and from this history attempts to predict future requests. If the cache
manager finds a pattern in the previous three requests, such as sequential
access forward or backward, it prefetches data into the cache before the next
                                                       process
                                                       I/O      I/O manager
                      cached I/O
       cache manager                                   file system
       data copy                                            noncached I/O
                      page fault
       VM manager                                      disk driver
                               Figure 19.6  File I/O.



858  Chapter 19  Windows 7
     request is submitted by the application. In this way, the application may find
     its data already cached and not need to wait for disk I/O.
     The cache manager is also responsible for telling the VM manager to flush
     the contents of the cache. The cache manager's default behavior is write-back
     caching: it accumulates writes for 4 to 5 seconds and then wakes up the cache-
     writer thread. When write-through caching is needed, a process can set a flag
     when opening the file, or the process can call an explicit cache-flush function.
     A fast-writing process could potentially fill all the free cache pages before
     the cache-writer thread had a chance to wake up and flush the pages to disk.
     The cache writer prevents a process from flooding the system in the following
     way. When the amount of free cache memory becomes low, the cache manager
     temporarily blocks processes attempting to write data and wakes the cache-
     writer thread to flush pages to disk. If the fast-writing process is actually a
     network redirector for a network file system, blocking it for too long could
     cause network transfers to time out and be retransmitted. This retransmission
     would waste network bandwidth. To prevent such waste, network redirectors
     can instruct the cache manager to limit the backlog of writes in the cache.
     Because a network file system needs to move data between a disk and the
     network interface, the cache manager also provides a DMA interface to move
     the data directly. Moving data directly avoids the need to copy data through
     an intermediate buffer.
     19.3.3.7  Security Reference Monitor
     Centralizing management of system entities in the object manager enables
     Windows to use a uniform mechanism to perform run-time access validation
     and audit checks for every user-accessible entity in the system. Whenever a
     process opens a handle to an object, the security reference monitor (SRM)
     checks the process's security token and the object's access-control list to see
     whether the process has the necessary access rights.
     The SRM is also responsible for manipulating the privileges in security
     tokens. Special privileges are required for users to perform backup or restore
     operations on file systems, debug processes, and so forth. Tokens can also be
     marked as being restricted in their privileges so that they cannot access objects
     that are available to most users. Restricted tokens are used primarily to limit
     the damage that can be done by execution of untrusted code.
     The integrity level of the code executing in a process is also represented
     by a token. Integrity levels are a type of capability mechanism, as mentioned
     earlier. A process cannot modify an object with an integrity level higher than
     that of the code executing in the process, whatever other permissions have
     been granted. Integrity levels were introduced to make it harder for code that
     successfully attacks outward-facing software, like Internet Explorer, to take
     over a system.
     Another responsibility of the SRM is logging security audit events. The
     Department of Defense's Common Criteria (the 2005 successor to the Orange
     Book) requires that a secure system have the ability to detect and log all
     attempts to access system resources so that it can more easily trace attempts at
     unauthorized access. Because the SRM is responsible for making access checks,
     it generates most of the audit records in the security-event log.



                                         19.3   System Components                859
19.3.3.8  Plug-and-Play Manager
The operating system uses the plug-and-play (PnP) manager to recognize
and adapt to changes in the hardware configuration. PnP devices use standard
protocols to identify themselves to the system. The PnP manager automatically
recognizes installed devices and detects changes in devices as the system
operates. The manager also keeps track of hardware resources used by a
device, as well as potential resources that could be used, and takes care of
loading the appropriate drivers. This management of hardware resources--
primarily interrupts and I/O memory ranges--has the goal of determining a
hardware configuration in which all devices are able to operate successfully.
The PnP manager handles dynamic reconfiguration as follows. First, it
gets a list of devices from each bus driver (for example, PCI or USB). It loads
the installed driver (after finding one, if necessary) and sends an add-device
request to the appropriate driver for each device. The PnP manager then figures
out the optimal resource assignments and sends a start-device request to
each driver specifying the resource assignments for the device. If a device
needs to be reconfigured, the PnP manager sends a query-stop request, which
asks the driver whether the device can be temporarily disabled. If the driver
can disable the device, then all pending operations are completed, and new
operations are prevented from starting. Finally, the PnP manager sends a stop
request and can then reconfigure the device with a new start-device request.
The       PnP   manager  also  supports  other  requests.  For  example,    query-
remove, which operates similarly to query-stop, is employed when a user
is getting ready to eject a removable device, such as a USB storage device. The
surprise-remove request is used when a device fails or, more likely, when a
user removes a device without telling the system to stop it first. Finally, the
remove request tells the driver to stop using a device permanently.
Many programs in the system are interested in the addition or removal
of devices, so the PnP manager supports notifications. Such a notification, for
example, gives GUI file menus the information they need to update their list
of disk volumes when a new storage device is attached or removed. Installing
devices often results in adding new services to the svchost.exe processes in
the system. These services frequently set themselves up to run whenever the
system boots and continue to run even if the original device is never plugged
into the system. Windows 7 introduced a service-trigger mechanism in the
service control manager (SCM), which manages the system services. With this
mechanism, services can register themselves to start only when SCM receives a
notification from the PnP manager that the device of interest has been added
to the system.
19.3.3.9  Power Manager
Windows works with the hardware to implement sophisticated strategies
for energy efficiency, as described in Section 19.2.8. The policies that drive
these strategies are implemented by the power manager. The power manager
detects current system conditions, such as the load on CPUs or I/O devices, and
improves energy efficiency by reducing the performance and responsiveness of
the system when need is low. The power manager can also put the entire system
into a very efficient sleep mode and can even write all the contents of memory
to disk and turn off the power to allow the system to go into hibernation.



860  Chapter 19  Windows 7
     The primary advantage of sleep is that the system can enter fairly quickly,
     perhaps just a few seconds after the lid closes on a laptop. The return from
     sleep is also fairly quick. The power is turned down low on the CPUs and I/O
     devices, but the memory continues to be powered enough that its contents are
     not lost.
     Hibernation      takes  considerably  longer  because    the  entire  contents     of
     memory must be transferred to disk before the system is turned off. However,
     the fact that the system is, in fact, turned off is a significant advantage. If
     there is a loss of power to the system, as when the battery is swapped on a
     laptop or a desktop system is unplugged, the saved system data will not be
     lost. Unlike shutdown, hibernation saves the currently running system so a
     user can resume where she left off, and because hibernation does not require
     power, a system can remain in hibernation indefinitely.
     Like the PnP manager, the power manager provides notifications to the
     rest of the system about changes in the power state. Some applications want to
     know when the system is about to be shut down so they can start saving their
     states to disk.
     19.3.3.10   Registry
     Windows keeps much of its configuration information in internal databases,
     called hives, that are managed by the Windows configuration manager, which
     is commonly known as the registry. There are separate hives for system
     information, default user preferences, software installation, security, and boot
     options. Because the information in the system hive is required to boot the
     system, the registry manager is implemented as a component of the executive.
     The registry represents the configuration state in each hive as a hierarchical
     namespace of keys (directories), each of which can contain a set of typed values,
     such as UNICODE string, ANSI string, integer, or untyped binary data. In theory,
     new keys and values are created and initialized as new software is installed;
     then they are modified to reflect changes in the configuration of that software.
     In practice, the registry is often used as a general-purpose database, as an
     interprocess-communication mechanism, and for many other such inventive
     purposes.
     Restarting applications, or even the system, every time a configuration
     change was made would be a nuisance. Instead, programs rely on various
     kinds of notifications, such as those provided by the PnP and power managers,
     to learn about changes in the system configuration. The registry also supplies
     notifications; it allows threads to register to be notified when changes are
     made to some part of the registry. The threads can thus detect and adapt to
     configuration changes recorded in the registry itself.
     Whenever significant changes are made to the system, such as when
     updates to the operating system or drivers are installed, there is a danger that
     the configuration data may be corrupted (for example, if a working driver is
     replaced by a nonworking driver or an application fails to install correctly and
     leaves partial information in the registry). Windows creates a system restore
     point before making such changes. The restore point contains a copy of the
     hives before the change and can be used to return to this version of the hives
     and thereby get a corrupted system working again.



                                         19.3      System Components              861
   To improve the stability of the registry configuration, Windows added a
transaction mechanism beginning with Windows Vista that can be used to
prevent the registry from being partially updated with a collection of related
configuration    changes.  Registry  transactions  can   be  part  of  more  general
transactions administered by the kernel transaction manager (KTM), which
can also include file-system transactions. KTM transactions do not have the
full semantics found in normal database transactions, and they have not
supplanted the system restore facility for recovering from damage to the
registry configuration caused by software installation.
19.3.3.11  Booting
The booting of a Windows PC begins when the hardware powers on and
firmware begins executing from ROM. In older machines, this firmware was
known as the BIOS, but more modern systems use UEFI (the Unified Extensible
Firmware Interface), which is faster and more general and makes better use of
the facilities in contemporary processors. The firmware runs power-on self-test
(POST) diagnostics; identifies many of the devices attached to the system and
initializes them to a clean, power-up state; and then builds the description
used by the advanced configuration and power interface (ACPI). Next, the
firmware finds the system disk, loads the Windows bootmgr program, and
begins executing it.
   In a machine that has been hibernating, the winresume program is loaded
next. It restores the running system from disk, and the system continues
execution at the point it had reached right before hibernating. In a machine
that has been shut down, the bootmgr performs further initialization of the
system and then loads winload. This program loads hal.dll, the kernel
(ntoskrnl.exe), any drivers needed in booting, and the system hive. winload
then transfers execution to the kernel.
   The kernel initializes itself and creates two processes. The system process
contains all the internal kernel worker threads and never executes in user mode.
The first user-mode process created is SMSS, for session manager subsystem,
which is similar to the INIT (initialization) process in UNIX. SMSS performs
further initialization of the system, including establishing the paging files,
loading    more  device  drivers,  and   managing  the   Windows       sessions.  Each
session is used to represent a logged-on user, except for session 0, which is
used to run system-wide background services, such as LSASS and SERVICES.
A session is anchored by an instance of the CSRSS process. Each session other
than 0 initially runs the WINLOGON process. This process logs on a user and
then launches the EXPLORER process, which implements the Windows GUI
experience. The following list itemizes some of these aspects of booting:
·  SMSS completes system initialization and then starts up session 0 and the
   first login session.
·  WININIT runs in session 0 to initialize user mode and start LSASS, SERVICES,
   and the local session manager, LSM.
·  LSASS, the security subsystem, implements facilities such as authentication
   of users.



862   Chapter 19  Windows 7
      ·  SERVICES contains the service control manager, or SCM, which supervises
         all background activities in the system, including user-mode services. A
         number of services will have registered to start when the system boots.
         Others will be started only on demand or when triggered by an event such
         as the arrival of a device.
      ·  CSRSS is the Win32 environmental subsystem process. It is started in every
         session--unlike the POSIX subsystem, which is started only on demand
         when a POSIX process is created.
      ·  WINLOGON is run in each Windows session other than session 0 to log on
         a user.
         The system optimizes the boot process by prepaging from files on disk
      based on previous boots of the system. Disk access patterns at boot are also
      used to lay out system files on disk to reduce the number of I/O operations
      required. The processes necessary to start the system are reduced by grouping
      services into fewer processes. All of these approaches contribute to a dramatic
      reduction in system boot time. Of course, system boot time is less important
      than it once was because of the sleep and hibernation capabilities of Windows.
19.4  Terminal Services and Fast User Switching
      Windows supports a GUI-based console that interfaces with the user via
      keyboard, mouse, and display. Most systems also support audio and video.
      Audio input is used by Windows voice-recognition software; voice recognition
      makes the system more convenient and increases its accessibility for users with
      disabilities. Windows 7 added support for multi-touch hardware, allowing
      users to input data by touching the screen and making gestures with one or
      more fingers. Eventually, the video-input capability, which is currently used
      for communication applications, is likely to be used for visually interpreting
      gestures, as Microsoft has demonstrated for its Xbox 360 Kinect product. Other
      future input experiences may evolve from Microsoft's surface computer. Most
      often installed at public venues, such as hotels and conference centers, the
      surface computer is a table surface with special cameras underneath. It can
      track the actions of multiple users at once and recognize objects that are placed
      on top.
         The PC was, of course, envisioned as a personal computer --an inherently
      single-user machine. Modern Windows, however, supports the sharing of a PC
      among multiple users. Each user that is logged on using the GUI has a session
      created to represent the GUI environment he will be using and to contain all the
      processes created to run his applications. Windows allows multiple sessions to
      exist at the same time on a single machine. However, Windows only supports
      a single console, consisting of all the monitors, keyboards, and mice connected
      to the PC. Only one session can be connected to the console at a time. From the
      logon screen displayed on the console, users can create new sessions or attach
      to an existing session that was previously created. This allows multiple users
      to share a single PC without having to log off and on between users. Microsoft
      calls this use of sessions fast user switching.



                                             19.5                 File System             863
      Users can also create new sessions, or connect to existing sessions, on one
      PC from a session running on another Windows PC. The terminal server (TS)
      connects one of the GUI windows in a user's local session to the new or existing
      session, called a remote desktop, on the remote computer. The most common
      use of remote desktops is for users to connect to a session on their work PC
      from their home PC.
      Many corporations use corporate terminal-server systems maintained in
      data centers to run all user sessions that access corporate resources, rather than
      allowing users to access those resources from the PCs in each user's office. Each
      server computer may handle many dozens of remote-desktop sessions. This
      is a form of thin-client computing, in which individual computers rely on a
      server for many functions. Relying on data-center terminal servers improves
      reliability, manageability, and security of the corporate computing resources.
      The TS is also used by Windows to implement remote assistance. A remote
      user can be invited to share a session with the user logged on to the session on
      the console. The remote user can watch the user's actions and even be given
      control of the desktop to help resolve computing problems.
19.5  File System
      The native file system in Windows is NTFS. It is used for all local volumes.
      However, associated USB thumb drives, flash memory on cameras, and external
      disks may be formatted with the 32-bit FAT file system for portability. FAT is
      a much older file-system format that is understood by many systems besides
      Windows, such as the software running on cameras. A disadvantage is that
      the FAT file system does not restrict file access to authorized users. The only
      solution for securing data with FAT is to run an application to encrypt the data
      before storing it on the file system.
      In contrast, NTFS uses ACLs to control access to individual files and supports
      implicit encryption of individual files or entire volumes (using Windows
      BitLocker feature). NTFS implements many other features as well, including
      data recovery, fault tolerance, very large files and file systems, multiple data
      streams, UNICODE names, sparse files, journaling, volume shadow copies, and
      file compression.
      19.5.1  NTFS Internal Layout
      The fundamental entity in NTFS is a volume. A volume is created by the
      Windows logical disk management utility and is based on a logical disk
      partition. A volume may occupy a portion of a disk or an entire disk, or may
      span several disks.
      NTFS does not deal with individual sectors of a disk but instead uses clusters
      as the units of disk allocation. A cluster is a number of disk sectors that is a
      power of 2. The cluster size is configured when an NTFS file system is formatted.
      The default cluster size is based on the volume size --4 KB for volumes larger
      than 2 GB. Given the size of today's disks, it may make sense to use cluster sizes
      larger than the Windows defaults to achieve better performance, although these
      performance gains will come at the expense of more internal fragmentation.
      NTFS uses logical cluster numbers (LCNs) as disk addresses. It assigns them
      by numbering clusters from the beginning of the disk to the end. Using this



864  Chapter 19  Windows 7
     scheme, the system can calculate a physical disk offset (in bytes) by multiplying
     the LCN by the cluster size.
     A file in NTFS is not a simple byte stream as it is in UNIX; rather, it is a
     structured object consisting of typed attributes. Each attribute of a file is an
     independent byte stream that can be created, deleted, read, and written. Some
     attribute types are standard for all files, including the file name (or names, if
     the file has aliases, such as an MS-DOS short name), the creation time, and the
     security descriptor that specifies the access control list. User data are stored in
     data attributes.
     Most traditional data files have an unnamed data attribute that contains
     all the file's data. However, additional data streams can be created with
     explicit names. For instance, in Macintosh files stored on a Windows server, the
     resource fork is a named data stream. The IProp interfaces of the Component
     Object Model (COM) use a named data stream to store properties on ordinary
     files, including thumbnails of images. In general, attributes may be added as
     necessary and are accessed using a file-name:attribute syntax. NTFS returns
     only the size of the unnamed attribute in response to file-query operations,
     such as when running the dir command.
     Every file in NTFS is described by one or more records in an array stored in a
     special file called the master file table (MFT). The size of a record is determined
     when the file system is created; it ranges from 1 to 4 KB. Small attributes
     are stored in the MFT record itself and are called resident attributes. Large
     attributes, such as the unnamed bulk data, are called nonresident attributes
     and are stored in one or more contiguous extents on the disk. A pointer to
     each extent is stored in the MFT record. For a small file, even the data attribute
     may fit inside the MFT record. If a file has many attributes--or if it is highly
     fragmented, so that many pointers are needed to point to all the fragments
     --one record in the MFT might not be large enough. In this case, the file is
     described by a record called the base file record, which contains pointers to
     overflow records that hold the additional pointers and attributes.
     Each file in an NTFS volume has a unique ID called a file reference. The file
     reference is a 64-bit quantity that consists of a 48-bit file number and a 16-bit
     sequence number. The file number is the record number (that is, the array slot)
     in the MFT that describes the file. The sequence number is incremented every
     time an MFT entry is reused. The sequence number enables NTFS to perform
     internal consistency checks, such as catching a stale reference to a deleted file
     after the MFT entry has been reused for a new file.
     19.5.1.1  NTFS B+ Tree
     As in UNIX, the NTFS namespace is organized as a hierarchy of directories. Each
     directory uses a data structure called a B+ tree to store an index of the file names
     in that directory. In a B+ tree, the length of every path from the root of the tree to
     a leaf is the same, and the cost of reorganizing the tree is eliminated. The index
     root of a directory contains the top level of the B+ tree. For a large directory,
     this top level contains pointers to disk extents that hold the remainder of the
     tree. Each entry in the directory contains the name and file reference of the
     file, as well as a copy of the update timestamp and file size taken from the
     file's resident attributes in the MFT. Copies of this information are stored in the
     directory so that a directory listing can be efficiently generated. Because all the
     file names, sizes, and update times are available from the directory itself, there
     is no need to gather these attributes from the MFT entries for each of the files.



                                                  19.5  File System                 865
19.5.1.2  NTFS Metadata
The NTFS volume's metadata are all stored in files. The first file is the MFT. The
second file, which is used during recovery if the MFT is damaged, contains a
copy of the first 16 entries of the MFT. The next few files are also special in
purpose. They include the files described below.
·  The log file records all metadata updates to the file system.
·  The volume file contains the name of the volume, the version of NTFS that
   formatted the volume, and a bit that tells whether the volume may have
   been corrupted and needs to be checked for consistency using the chkdsk
   program.
·  The attribute-definition table indicates which attribute types are used in
   the volume and what operations can be performed on each of them.
·  The root directory is the top-level directory in the file-system hierarchy.
·  The bitmap file indicates which clusters on a volume are allocated to files
   and which are free.
·  The boot file contains the startup code for Windows and must be located
   at a particular disk address so that it can be found easily by a simple ROM
   bootstrap loader. The boot file also contains the physical address of the
   MFT.
·  The bad-cluster file keeps track of any bad areas on the volume; NTFS uses
   this record for error recovery.
   Keeping all the NTFS metadata in actual files has a useful property. As
discussed in Section 19.3.3.6, the cache manager caches file data. Since all
the NTFS metadata reside in files, these data can be cached using the same
mechanisms used for ordinary data.
19.5.2    Recovery
In many simple file systems, a power failure at the wrong time can damage
the file-system data structures so severely that the entire volume is scrambled.
Many UNIX file systems, including UFS but not ZFS, store redundant metadata
on the disk, and they recover from crashes by using the fsck program to check
all the file-system data structures and restore them forcibly to a consistent
state. Restoring them often involves deleting damaged files and freeing data
clusters that had been written with user data but not properly recorded in the
file system's metadata structures. This checking can be a slow process and can
cause the loss of significant amounts of data.
   NTFS takes a different approach to file-system robustness. In NTFS, all file-
system data-structure updates are performed inside transactions. Before a data
structure is altered, the transaction writes a log record that contains redo and
undo information. After the data structure has been changed, the transaction
writes a commit record to the log to signify that the transaction succeeded.
   After a crash, the system can restore the file-system data structures to
a consistent state by processing the log records, first redoing the operations
for committed transactions and then undoing the operations for transactions



866  Chapter 19     Windows 7
     that did not commit successfully before the crash. Periodically (usually every
     5 seconds), a checkpoint record is written to the log. The system does not
     need log records prior to the checkpoint to recover from a crash. They can be
     discarded, so the log file does not grow without bounds. The first time after
     system startup that an NTFS volume is accessed, NTFS automatically performs
     file-system recovery.
     This scheme does not guarantee that all the user-file contents are correct
     after a crash. It ensures only that the file-system data structures (the metadata
     files) are undamaged and reflect some consistent state that existed prior to the
     crash. It would be possible to extend the transaction scheme to cover user files,
     and Microsoft took some steps to do this in Windows Vista.
     The log is stored in the third metadata file at the beginning of the volume.
     It is created with a fixed maximum size when the file system is formatted. It
     has two sections: the logging area, which is a circular queue of log records, and
     the restart area, which holds context information, such as the position in the
     logging area where NTFS should start reading during a recovery. In fact, the
     restart area holds two copies of its information, so recovery is still possible if
     one copy is damaged during the crash.
     The logging functionality is provided by the log-file service. In addition
     to writing the log records and performing recovery actions, the log-file service
     keeps track of the free space in the log file. If the free space gets too low,
     the log-file service queues pending transactions, and NTFS halts all new I/O
     operations. After the in-progress operations complete, NTFS calls the cache
     manager to flush all data and then resets the log file and performs the queued
     transactions.
     19.5.3  Security
     The security of an NTFS volume is derived from the Windows object model.
     Each NTFS file references a security descriptor, which specifies the owner of the
     file, and an access-control list, which contains the access permissions granted
     or denied to each user or group listed. Early versions of NTFS used a separate
     security descriptor as an attribute of each file. Beginning with Windows 2000,
     the security-descriptors attribute points to a shared copy, with a significant
     savings in disk and caching space; many, many files have identical security
     descriptors.
     In normal operation, NTFS does not enforce permissions on traversal of
     directories in file path names. However, for compatibility with POSIX, these
     checks can be enabled. Traversal checks are inherently more expensive, since
     modern parsing of file path names uses prefix matching rather than directory-
     by-directory parsing of path names. Prefix matching is an algorithm that looks
     up strings in a cache and finds the entry with the longest match--for example,
     an entry for \foo\bar\dir would be a match for \foo\bar\dir2\dir3\myfile.
     The prefix-matching cache allows path-name traversal to begin much deeper
     in the tree, saving many steps. Enforcing traversal checks means that the user's
     access must be checked at each directory level. For instance, a user might lack
     permission to traverse \foo\bar, so starting at the access for \foo\bar\dir
     would be an error.



                                                            19.5    File System             867
19.5.4    Volume Management and Fault Tolerance
FtDisk    is  the  fault-tolerant   disk  driver     for  Windows.  When        installed,  it
provides several ways to combine multiple disk drives into one logical volume
so as to improve performance, capacity, or reliability.
19.5.4.1  Volume Sets and RAID Sets
One way to combine multiple disks is to concatenate them logically to form a
large logical volume, as shown in Figure 19.7. In Windows, this logical volume,
called a volume set, can consist of up to 32 physical partitions. A volume set
that contains an NTFS volume can be extended without disturbance of the data
already stored in the file system. The bitmap metadata on the NTFS volume are
simply extended to cover the newly added space. NTFS continues to use the
same LCN mechanism that it uses for a single physical disk, and the FtDisk
driver supplies the mapping from a logical-volume offset to the offset on one
particular disk.
Another       way  to      combine      multiple  physical  partitions  is  to  interleave
their blocks in round-robin fashion to form a stripe set. This scheme is also
called RAID level 0, or disk striping. (For more on RAID (redundant arrays of
inexpensive disks), see Section 10.7.) FtDisk uses a stripe size of 64 KB. The
first 64 KB of the logical volume are stored in the first physical partition, the
second 64 KB in the second physical partition, and so on, until each partition
has contributed 64 KB of space. Then, the allocation wraps around to the first
disk, allocating the second 64-KB block. A stripe set forms one large logical
volume, but the physical layout can improve the I/O bandwidth, because for
a large I/O, all the disks can transfer data in parallel. Windows also supports
RAID level 5, stripe set with parity, and RAID level 1, mirroring.
          disk 1 (2.5 GB)           disk 2 (2.5 GB)
                                                            disk C: (FAT) 2 GB
                           LCNs 128001­783361
                                                            logical drive D: (NTFS) 3 GB
          LCNs 0­128000
                           Figure 19.7  Volume set on two drives.



868  Chapter 19  Windows 7
     19.5.4.2    Sector Sparing and Cluster Remapping
     To deal with disk sectors that go bad, FtDisk uses a hardware technique called
     sector sparing, and NTFS uses a software technique called cluster remapping.
     Sector sparing is a hardware capability provided by many disk drives. When
     a disk drive is formatted, it creates a map from logical block numbers to good
     sectors on the disk. It also leaves extra sectors unmapped, as spares. If a sector
     fails, FtDisk instructs the disk drive to substitute a spare. Cluster remapping
     is a software technique performed by the file system. If a disk block goes
     bad, NTFS substitutes a different, unallocated block by changing any affected
     pointers in the MFT. NTFS also makes a note that the bad block should never be
     allocated to any file.
     When a disk block goes bad, the usual outcome is a data loss. But sector
     sparing or cluster remapping can be combined with fault-tolerant volumes to
     mask the failure of a disk block. If a read fails, the system reconstructs the
     missing data by reading the mirror or by calculating the exclusive    or parity
     in a stripe set with parity. The reconstructed data are stored in a new location
     that is obtained by sector sparing or cluster remapping.
     19.5.5      Compression
     NTFS can perform data compression on individual files or on all data files in
     a directory. To compress a file, NTFS divides the file's data into compression
     units, which are blocks of 16 contiguous clusters. When a compression unit
     is written, a data-compression algorithm is applied. If the result fits into
     fewer than 16 clusters, the compressed version is stored. When reading, NTFS
     can determine whether data have been compressed: if they have been, the
     length of the stored compression unit is less than 16 clusters. To improve
     performance when reading contiguous compression units, NTFS prefetches
     and decompresses ahead of the application requests.
     For sparse files or files that contain mostly zeros, NTFS uses another
     technique to save space. Clusters that contain only zeros because they have
     never been written are not actually allocated or stored on disk. Instead, gaps
     are left in the sequence of virtual-cluster numbers stored in the MFT entry for
     the file. When reading a file, if NTFS finds a gap in the virtual-cluster numbers,
     it just zero-fills that portion of the caller's buffer. This technique is also used
     by UNIX.
     19.5.6      Mount Points, Symbolic Links, and Hard Links
     Mount points are a form of symbolic link specific to directories on NTFS that
     were introduced in Windows 2000. They provide a mechanism for organizing
     disk volumes that is more flexible than the use of global names (like drive
     letters). A mount point is implemented as a symbolic link with associated
     data  that  contains    the  true  volume  name.  Ultimately,  mount  points         will
     supplant drive letters completely, but there will be a long transition due to
     the dependence of many applications on the drive-letter scheme.
     Windows Vista introduced support for a more general form of symbolic
     links, similar to those found in UNIX. The links can be absolute or relative, can
     point to objects that do not exist, and can point to both files and directories



                                                            19.6  Networking               869
      even across volumes. NTFS also supports hard links, where a single file has an
      entry in more than one directory of the same volume.
      19.5.7   Change Journal
      NTFS keeps a journal describing all changes that have been made to the file
      system. User-mode services can receive notifications of changes to the journal
      and then identify what files have changed by reading from the journal. The
      search indexer service uses the change journal to identify files that need to be
      re-indexed. The file-replication service uses it to identify files that need to be
      replicated across the network.
      19.5.8   Volume Shadow Copies
      Windows implements the capability of bringing a volume to a known state
      and then creating a shadow copy that can be used to back up a consistent
      view of the volume. This technique is known as snapshots in some other file
      systems. Making a shadow copy of a volume is a form of copy-on-write, where
      blocks modified after the shadow copy is created are stored in their original
      form in the copy. To achieve a consistent state for the volume requires the
      cooperation of applications, since the system cannot know when the data used
      by the application are in a stable state from which the application could be
      safely restarted.
      The server version of Windows uses shadow copies to efficiently maintain
      old versions of files stored on file servers. This allows users to see documents
      stored on file servers as they existed at earlier points in time. The user can use
      this feature to recover files that were accidentally deleted or simply to look at
      a previous version of the file, all without pulling out backup media.
19.6  Networking
      Windows supports both peer-to-peer and client­server networking. It also has
      facilities for network management. The networking components in Windows
      provide  data  transport,  interprocess  communication,     file  sharing  across    a
      network, and the ability to send print jobs to remote printers.
      19.6.1   Network Interfaces
      To describe networking in Windows, we must first mention two of the internal
      networking interfaces: the network device interface specification (NDIS) and
      the transport driver interface (TDI). The NDIS interface was developed in 1989
      by Microsoft and 3Com to separate network adapters from transport protocols
      so that either could be changed without affecting the other. NDIS resides at
      the interface between the data-link and network layers in the ISO model and
      enables many protocols to operate over many different network adapters. In
      terms of the ISO model, the TDI is the interface between the transport layer
      (layer 4) and the session layer (layer 5). This interface enables any session-layer
      component to use any available transport mechanism. (Similar reasoning led
      to the streams mechanism in UNIX.) The TDI supports both connection-based
      and connectionless transport and has functions to send any type of data.



870  Chapter 19  Windows 7
     19.6.2    Protocols
     Windows implements transport protocols as drivers. These drivers can be
     loaded and unloaded from the system dynamically, although in practice the
     system typically has to be rebooted after a change. Windows comes with several
     networking protocols. Next, we discuss a number of these protocols.
     19.6.2.1  Server-Message Block
     The server-message-block (SMB) protocol was first introduced in MS-DOS 3.1.
     The system uses the protocol to send I/O requests over the network. The SMB
     protocol has four message types. Session     control messages are commands
     that start and end a redirector connection to a shared resource at the server. A
     redirector uses File messages to access files at the server. Printer messages
     are used to send data to a remote print queue and to receive status information
     from the queue, and Message messages are used to communicate with another
     workstation. A version of the SMB protocol was published as the common
     Internet file system (CIFS) and is supported on a number of operating systems.
     19.6.2.2  Transmission Control Protocol/Internet Protocol
     The transmission control protocol/Internet protocol (TCP/IP) suite that is used
     on the Internet has become the de facto standard networking infrastructure.
     Windows uses TCP/IP to connect to a wide variety of operating systems
     and hardware platforms. The Windows TCP/IP package includes the simple
     network-management protocol (SNM), the dynamic host-configuration proto-
     col (DHCP), and the older Windows Internet name service (WINS). Windows
     Vista introduced a new implementation of TCP/IP that supports both IPv4
     and IPv6 in the same network stack. This new implementation also supports
     offloading of the network stack onto advanced hardware, to achieve very high
     performance for servers.
          Windows provides a software firewall that limits the TCP ports that can
     be used by programs for network communication. Network firewalls are
     commonly implemented in routers and are a very important security measure.
     Having a firewall built into the operating system makes a hardware router
     unnecessary, and it also provides more integrated management and easier use.
     19.6.2.3  Point-to-Point Tunneling Protocol
     The  point-to-point  tunneling  protocol  (PPTP)  is  a  protocol  provided       by
     Windows to communicate between remote-access server modules running
     on Windows server machines and other client systems that are connected
     over the Internet. The remote-access servers can encrypt data sent over the
     connection, and they support multiprotocol virtual private networks (VPNs)
     over the Internet.
     19.6.2.4  HTTP Protocol
     The HTTP protocol is used to get/put information using the World Wide Web.
     Windows implements HTTP using a kernel-mode driver, so web servers can
     operate with a low-overhead connection to the networking stack. HTTP is a



                                                       19.6  Networking             871
fairly general protocol, which Windows makes available as a transport option
for implementing RPC.
19.6.2.5  Web-Distributed Authoring and Versioning Protocol
Web-distributed authoring and versioning (WebDAV) is an HTTP-based protocol
for collaborative authoring across a network. Windows builds a WebDAV
redirector into the file system. Being built directly into the file system enables
WebDAV to work with other file-system features, such as encryption. Personal
files can then be stored securely in a public place. Because WebDAV uses HTTP,
which is a get/put protocol, Windows has to cache the files locally so programs
can use read and write operations on parts of the files.
19.6.2.6  Named Pipes
Named pipes are a connection-oriented messaging mechanism. A process can
use named pipes to communicate with other processes on the same machine.
Since named pipes are accessed through the file-system interface, the security
mechanisms used for file objects also apply to named pipes. The SMB protocol
supports named pipes, so named pipes can also be used for communication
between processes on different systems.
The       format  of  pipe  names   follows  the  uniform    naming  convention
(UNC). A UNC name looks like a typical remote file name. The format is
\\server name\share name\x\y\z, where server name identifies a server
on the network; share name identifies any resource that is made available
to network users, such as directories, files, named pipes, and printers; and
\x\y\z is a normal file path name.
19.6.2.7  Remote Procedure Calls
A remote procedure call (RPC) is a client­server mechanism that enables an
application on one machine to make a procedure call to code on another
machine. The client calls a local procedure --a stub routine --that packs its
arguments into a message and sends them across the network to a particular
server process. The client-side stub routine then blocks. Meanwhile, the server
unpacks the message, calls the procedure, packs the return results into a
message, and sends them back to the client stub. The client stub unblocks,
receives the message, unpacks the results of the RPC, and returns them to the
caller. This packing of arguments is sometimes called marshaling. The client
stub code and the descriptors necessary to pack and unpack the arguments for
an RPC are compiled from a specification written in the Microsoft Interface
Definition Language.
The       Windows     RPC   mechanism    follows  the  widely  used  distributed-
computing-environment standard for RPC messages, so programs written to
use Windows RPCs are highly portable. The RPC standard is detailed. It hides
many of the architectural differences among computers, such as the sizes
of binary numbers and the order of bytes and bits in computer words, by
specifying standard data formats for RPC messages.



872  Chapter 19  Windows 7
     19.6.2.8  Component Object Model
     The component object model (COM) is a mechanism for interprocess commu-
     nication that was developed for Windows. COM objects provide a well-defined
     interface to manipulate the data in the object. For instance, COM is the infras-
     tructure used by Microsoft's object linking and embedding (OLE) technology
     for inserting spreadsheets into Microsoft Word documents. Many Windows
     services provide COM interfaces. Windows has a distributed extension called
     DCOM that can be used over a network utilizing RPC to provide a transparent
     method of developing distributed applications.
     19.6.3    Redirectors and Servers
     In Windows, an application can use the Windows I/O API to access files from a
     remote computer as though they were local, provided that the remote computer
     is running a CIFS server such as those provided by Windows. A redirector is the
     client-side object that forwards I/O requests to a remote system, where they are
     satisfied by a server. For performance and security, the redirectors and servers
     run in kernel mode.
          In more detail, access to a remote file occurs as follows:
     1.   The application calls the I/O manager to request that a file be opened with
          a file name in the standard UNC format.
     2.   The I/O manager builds an I/O request packet, as described in Section
          19.3.3.5.
     3.   The I/O manager recognizes that the access is for a remote file and calls a
          driver called a multiple universal-naming-convention provider (MUP).
     4.   The MUP sends the I/O request packet asynchronously to all registered
          redirectors.
     5.   A redirector that can satisfy the request responds to the MUP. To avoid
          asking all the redirectors the same question in the future, the MUP uses a
          cache to remember which redirector can handle this file.
     6.   The redirector sends the network request to the remote system.
     7.   The remote-system network drivers receive the request and pass it to the
          server driver.
     8.   The server driver hands the request to the proper local file-system driver.
     9.   The proper device driver is called to access the data.
     10.  The results are returned to the server driver, which sends the data back
          to the requesting redirector. The redirector then returns the data to the
          calling application via the I/O manager.
     A similar process occurs for applications that use the Win32 network API, rather
     than the UNC services, except that a module called a multi-provider router is
     used instead of a MUP.
          For  portability,  redirectors  and  servers  use  the  TDI  API  for  network
     transport. The requests themselves are expressed in a higher-level protocol,



                                         19.6                  Networking          873
which by default is the SMB protocol described in Section 19.6.2. The list of
redirectors is maintained in the system hive of the registry.
19.6.3.1  Distributed File System
UNC names are not always convenient, because multiple file servers may be
available to serve the same content and UNC names explicitly include the name
of the server. Windows supports a distributed file-system (DFS) protocol that
allows a network administrator to serve up files from multiple servers using a
single distributed name space.
19.6.3.2  Folder Redirection and Client-Side Caching
To improve the PC experience for users who frequently switch among com-
puters, Windows allows administrators to give users roaming profiles, which
keep users' preferences and other settings on servers. Folder redirection is
then used to automatically store a user's documents and other files on a server.
This works well until one of the computers is no longer attached to the
network, as when a user takes a laptop onto an airplane. To give users off-line
access to their redirected files, Windows uses client-side caching (CSC). CSC
is also used when the computer is on-line to keep copies of the server files
on the local machine for better performance. The files are pushed up to the
server as they are changed. If the computer becomes disconnected, the files are
still available, and the update of the server is deferred until the next time the
computer is online.
19.6.4    Domains
Many networked environments have natural groups of users, such as students
in a computer laboratory at school or employees in one department in a
business. Frequently, we want all the members of the group to be able to
access shared resources on their various computers in the group. To manage
the global access rights within such groups, Windows uses the concept of
a domain. Previously, these domains had no relationship whatsoever to the
domain-name system (DNS) that maps Internet host names to IP addresses.
Now, however, they are closely related.
Specifically, a Windows domain is a group of Windows workstations
and servers that share a common security policy and user database. Since
Windows uses the Kerberos protocol for trust and authentication, a Windows
domain is the same thing as a Kerberos realm. Windows uses a hierarchical
approach for establishing trust relationships between related domains. The
trust relationships are based on DNS and allow transitive trusts that can flow up
and down the hierarchy. This approach reduces the number of trusts required
for n domains from n  (n - 1) to O(n). The workstations in the domain trust
the domain controller to give correct information about the access rights of
each user (loaded into the user's access token by LSASS). All users retain the
ability to restrict access to their own workstations, however, no matter what
any domain controller may say.



874   Chapter 19  Windows 7
      19.6.5    Active Directory
      Active Directory is the Windows implementation of lightweight directory-
      access protocol (LDAP) services. Active Directory stores the topology infor-
      mation about the domain, keeps the domain-based user and group accounts
      and passwords, and provides a domain-based store for Windows features that
      need it, such as Windows group policy. Administrators use group policies to
      establish uniform standards for desktop preferences and software. For many
      corporate information-technology groups, uniformity drastically reduces the
      cost of computing.
19.7  Programmer Interface
      The Win32 API is the fundamental interface to the capabilities of Windows. This
      section describes five main aspects of the Win32 API: access to kernel objects,
      sharing of objects between processes, process management, interprocess com-
      munication, and memory management.
      19.7.1    Access to Kernel Objects
      The Windows kernel provides many services that application programs can
      use.  Application    programs  obtain  these  services  by  manipulating  kernel
      objects. A process gains access to a kernel object named XXX by calling the
      CreateXXX function to open a handle to an instance of XXX. This handle is
      unique to the process. Depending on which object is being opened, if the
      Create() function fails, it may return 0, or it may return a special constant
      named INVALID HANDLE VALUE. A process can close any handle by calling the
      CloseHandle() function, and the system may delete the object if the count of
      handles referencing the object in all processes drops to zero.
      19.7.2    Sharing Objects between Processes
      Windows provides three ways to share objects between processes. The first
      way is for a child process to inherit a handle to the object. When the parent
      calls the CreateXXX function, the parent supplies a SECURITIES ATTRIBUTES
      structure with the bInheritHandle field set to TRUE. This field creates an
      inheritable handle. Next, the child process is created, passing a value of TRUE
      to the CreateProcess() function's bInheritHandle argument. Figure 19.8
      shows a code sample that creates a semaphore handle inherited by a child
      process.
            Assuming the child process knows which handles are shared, the parent
      and child can achieve interprocess communication through the shared objects.
      In the example in Figure 19.8, the child process gets the value of the handle
      from the first command-line argument and then shares the semaphore with
      the parent process.
            The second way to share objects is for one process to give the object a
      name when the object is created and for the second process to open the name.
      This method has two drawbacks: Windows does not provide a way to check
      whether an object with the chosen name already exists, and the object name
      space is global, without regard to the object type. For instance, two applications



                                             19.7  Programmer Interface              875
   SECURITY ATTRIBUTES sa;
   sa.nlength = sizeof(sa);
   sa.lpSecurityDescriptor          =   NULL;
   sa.bInheritHandle         =  TRUE;
   Handle   a semaphore      =   CreateSemaphore(&sa,  1,     1,      NULL);
   char comand line[132];
   ostrstream ostring(command line, sizeof(command line));
   ostring  <<     a semaphore      <<  ends;
   CreateProcess("another process.exe", command line,
         NULL,     NULL,  TRUE,  .  .   .);
      Figure 19.8  Code enabling a child to share an object by inheriting a handle.
may create and share a single object named "foo" when two distinct objects--
possibly of different types--were desired.
   Named objects have the advantage that unrelated processes can readily
share them. The first process calls one of the CreateXXX functions and supplies
a name as a parameter. The second process gets a handle to share the object
by calling OpenXXX() (or CreateXXX) with the same name, as shown in the
example in Figure 19.9.
   The third way to share objects is via the DuplicateHandle() function.
This method requires some other method of interprocess communication to
pass the duplicated handle. Given a handle to a process and the value of a
handle within that process, a second process can get a handle to the same
object and thus share it. An example of this method is shown in Figure 19.10.
19.7.3   Process Management
In Windows, a process is a loaded instance of an application and a thread is an
executable unit of code that can be scheduled by the kernel dispatcher. Thus,
a process contains one or more threads. A process is created when a thread
in some other process calls the CreateProcess() API. This routine loads
any dynamic link libraries used by the process and creates an initial thread
in the process. Additional threads can be created by the CreateThread()
function. Each thread is created with its own stack, which defaults to 1 MB
unless otherwise specified in an argument to CreateThread().
// Process A
.  .  .
HANDLE a semaphore        =  CreateSemaphore(NULL,     1,  1,         "MySEM1");
.  .  .
// Process B
.  .  .
HANDLE b semaphore =         OpenSemaphore(SEMAPHORE   ALL     ACCESS,
   FALSE, "MySEM1");
.  .  .
            Figure 19.9   Code for sharing an object by name lookup.



876  Chapter 19      Windows 7
     //  Process         A  wants    to  give  Process    B   access  to     a  semaphore
     //  Process         A
     HANDLE a semaphore = CreateSemaphore(NULL, 1,                        1,    NULL);
     //  send       the     value    of  the  semaphore   to  Process     B
     //  using       a   message     or  shared   memory     object
     ...
     //  Process         B
     HANDLE      process a        =  OpenProcess(PROCESS ALL ACCESS,                  FALSE,
         process id of A);
     HANDLE b semaphore;
     DuplicateHandle(process a,                a semaphore,
         GetCurrentProcess(), &b semaphore,
         0,    FALSE,       DUPLICATE    SAME ACCESS);
     //  use     b   semaphore       to  access   the  semaphore
     ...
                    Figure 19.10     Code for sharing an object by passing a handle.
     19.7.3.1    Scheduling Rule
     Priorities  in  the    Win32    environment  are    based  on   the  native      kernel  (NT)
     scheduling model, but not all priority values may be chosen. The Win32 API
     uses four priority classes:
     1.  IDLE PRIORITY CLASS (NT priority level 4)
     2.  NORMAL PRIORITY CLASS (NT priority level 8)
     3.  HIGH PRIORITY CLASS (NT priority level 13)
     4.  REALTIME PRIORITY CLASS (NT priority level 24)
     Processes are typically members of the NORMAL PRIORITY CLASS unless the
     parent of the process was of the IDLE PRIORITY CLASS or another class was
     specified when CreateProcess was called. The priority class of a process is
     the default for all threads that execute in the process. It can be changed with
     the SetPriorityClass() function or by passing an argument to the START
     command. Only users with the increase scheduling priority privilege can move
     a process into the REALTIME PRIORITY CLASS. Administrators and power users
     have this privilege by default.
         When a user is running an interactive process, the system needs to schedule
     the process's threads to provide good responsiveness. For this reason, Windows
     has a special scheduling rule for processes in the NORMAL PRIORITY CLASS.
     Windows distinguishes between the process associated with the foreground
     window on the screen and the other (background) processes. When a process
     moves into the foreground, Windows increases the scheduling quantum for all
     its threads by a factor of 3; CPU-bound threads in the foreground process will
     run three times longer than similar threads in background processes.



                                          19.7  Programmer Interface                 877
19.7.3.2  Thread Priorities
A thread starts with an initial priority determined by its class. The priority
can be altered by the SetThreadPriority() function. This function takes an
argument that specifies a priority relative to the base priority of its class:
·  THREAD PRIORITY LOWEST: base - 2
·  THREAD PRIORITY BELOW NORMAL: base - 1
·  THREAD PRIORITY NORMAL: base + 0
·  THREAD PRIORITY ABOVE NORMAL: base + 1
·  THREAD PRIORITY HIGHEST: base + 2
   Two other designations are also used to adjust the priority. Recall from
Section 19.3.2.2 that the kernel has two priority classes: 16­31 for the real-
time class and 1­15 for the variable class. THREAD PRIORITY IDLE sets the
priority to 16 for real-time threads and to 1 for variable-priority threads.
THREAD PRIORITY TIME CRITICAL sets the priority to 31 for real-time threads
and to 15 for variable-priority threads.
   As discussed in Section 19.3.2.2, the kernel adjusts the priority of a variable
class thread dynamically depending on whether the thread is I/O bound or
CPU bound. The Win32 API provides a method to disable this adjustment via
SetProcessPriorityBoost() and SetThreadPriorityBoost() functions.
19.7.3.3  Thread Suspend and Resume
A thread can be created in a suspended state or can be placed in a suspended
state later by use of the SuspendThread() function. Before a suspended
thread can be scheduled by the kernel dispatcher, it must be moved out of
the suspended state by use of the ResumeThread() function. Both functions
set a counter so that if a thread is suspended twice, it must be resumed twice
before it can run.
19.7.3.4  Thread Synchronization
To synchronize concurrent access to shared objects by threads, the kernel pro-
vides synchronization objects, such as semaphores and mutexes. These are dis-
patcher objects, as discussed in Section 19.3.2.2. Threads can also synchronize
with kernel services operating on kernel objects--such as threads, processes,
and files--because these are also dispatcher objects. Synchronization with ker-
nel dispatcher objects can be achieved by use of the WaitForSingleObject()
and WaitForMultipleObjects() functions; these functions wait for one or
more dispatcher objects to be signaled.
   Another method of synchronization is available to threads within the same
process that want to execute code exclusively. The Win32 critical section object
is a user-mode mutex object that can often be acquired and released without
entering the kernel. On a multiprocessor, a Win32 critical section will attempt
to spin while waiting for a critical section held by another thread to be released.
If the spinning takes too long, the acquiring thread will allocate a kernel mutex
and yield its CPU. Critical sections are particularly efficient because the kernel
mutex is allocated only when there is contention and then used only after



878  Chapter 19  Windows 7
     attempting to spin. Most mutexes in programs are never actually contended,
     so the savings are significant.
     Before using a critical section, some thread in the process must call Ini-
     tializeCriticalSection(). Each thread that wants to acquire the mutex
     calls  EnterCriticalSection()       and  then  later  calls  LeaveCriticalSec-
     tion() to release the mutex. There is also a TryEnterCriticalSection()
     function, which attempts to acquire the mutex without blocking.
     For programs that want user-mode reader­writer locks rather than a
     mutex, Win32 supports slim reader ­writer (SRW) locks. SRW locks have APIs
     similar to those for critical sections, such as InitializeSRWLock, AcquireS-
     RWLockXXX,  and   ReleaseSRWLockXXX,     where  XXX   is     either  Exclusive     or
     Shared, depending on whether the thread wants write access or just read
     access to the object protected by the lock. The Win32 API also supports condition
     variables, which can be used with either critical sections or SRW locks.
     19.7.3.5  Thread Pool
     Repeatedly creating and deleting threads can be expensive for applications and
     services that perform small amounts of work in each instantiation. The Win32
     thread pool provides user-mode programs with three services: a queue to
     which work requests may be submitted (via the SubmitThreadpoolWork()
     function), an API that can be used to bind callbacks to waitable handles
     (RegisterWaitForSingleObject()), and APIs to work with timers (Cre-
     ateThreadpoolTimer() and WaitForThreadpoolTimerCallbacks()) and
     to bind callbacks to I/O completion queues (BindIoCompletionCallback()).
     The goal of using a thread pool is to increase performance and reduce
     memory footprint. Threads are relatively expensive, and each processor can
     only be executing one thread at a time no matter how many threads are
     available. The thread pool attempts to reduce the number of runnable threads
     by slightly delaying work requests (reusing each thread for many requests)
     while providing enough threads to effectively utilize the machine's CPUs. The
     wait and I/O- and timer-callback APIs allow the thread pool to further reduce
     the number of threads in a process, using far fewer threads than would be
     necessary if a process were to devote separate threads to servicing each waitable
     handle, timer, or completion port.
     19.7.3.6  Fibers
     A fiber is user-mode code that is scheduled according to a user-defined
     scheduling algorithm. Fibers are completely a user-mode facility; the kernel
     is not aware that they exist. The fiber mechanism uses Windows threads as
     if they were CPUs to execute the fibers. Fibers are cooperatively scheduled,
     meaning that they are never preempted but must explicitly yield the thread
     on which they are running. When a fiber yields a thread, another fiber can be
     scheduled on it by the run-time system (the programming language run-time
     code).
     The system creates a fiber by calling either ConvertThreadToFiber()
     or CreateFiber(). The primary difference between these functions is that
     CreateFiber() does not begin executing the fiber that was created. To begin
     execution, the application must call SwitchToFiber(). The application can
     terminate a fiber by calling DeleteFiber().



                                             19.7  Programmer Interface                879
Fibers are not recommended for threads that use Win32 APIs rather than
standard C-library functions because of potential incompatibilities. Win32 user-
mode threads have a thread-environment block (TEB) that contains numerous
per-thread fields used by the Win32 APIs. Fibers must share the TEB of the thread
on which they are running. This can lead to problems when a Win32 interface
puts state information into the TEB for one fiber and then the information is
overwritten by a different fiber. Fibers are included in the Win32 API to facilitate
the porting of legacy UNIX applications that were written for a user-mode
thread model such as Pthreads.
19.7.3.7  User-Mode Scheduling (UMS) and ConcRT
A new mechanism in Windows 7, user-mode scheduling (UMS), addresses
several limitations of fibers. First, recall that fibers are unreliable for executing
Win32 APIs because they do not have their own TEBs. When a thread running
a fiber blocks in the kernel, the user scheduler loses control of the CPU for a
time as the kernel dispatcher takes over scheduling. Problems may result when
fibers change the kernel state of a thread, such as the priority or impersonation
token, or when they start asynchronous I/O.
UMS provides an alternative model by recognizing that each Windows
thread is actually two threads: a kernel thread (KT) and a user thread (UT).
Each type of thread has its own stack and its own set of saved registers. The
KT and UT appear as a single thread to the programmer because UTs can
never block but must always enter the kernel, where an implicit switch to the
corresponding KT takes place. UMS uses each UT's TEB to uniquely identify
the UT. When a UT enters the kernel, an explicit switch is made to the KT that
corresponds to the UT identified by the current TEB. The reason the kernel does
not know which UT is running is that UTs can invoke a user-mode scheduler,
as fibers do. But in UMS, the scheduler switches UTs, including switching the
TEBs.
When a UT enters the kernel, its KT may block. When this happens, the
kernel switches to a scheduling thread, which UMS calls a primary, and uses
this thread to reenter the user-mode scheduler so that it can pick another UT
to run. Eventually, a blocked KT will complete its operation and be ready to
return to user mode. Since UMS has already reentered the user-mode scheduler
to run a different UT, UMS queues the UT corresponding to the completed KT
to a completion list in user mode. When the user-mode scheduler is choosing
a new UT to switch to, it can examine the completion list and treat any UT on
the list as a candidate for scheduling.
Unlike fibers, UMS is not intended to be used directly by the program-
mer. The details of writing user-mode schedulers can be very challenging,
and UMS does not include such a scheduler. Rather, the schedulers come
from programming language libraries that build on top of UMS. Microsoft
Visual Studio 2010 shipped with Concurrency Runtime (ConcRT), a concurrent
programming framework for C++. ConcRT provides a user-mode scheduler
together with facilities for decomposing programs into tasks, which can then
be scheduled on the available CPUs. ConcRT provides support for par for
styles of constructs, as well as rudimentary resource management and task
synchronization primitives. The key features of UMS are depicted in Figure
19.11.



880  Chapter 19        Windows 7
                                           NTOS executive           KT0 blocks
                                      KT0     KT1    KT2                        Primary
                       trap code           Thread parking                       thread
               kernel
               user
                                                     UT completion list
                       UT0                                 User-mode
                                                           scheduler
     Only primary thread runs in user-mode
     Trap code switches to parked KT
     KT blocks  primary returns to user-mode               UT1  UT0
     KT unblocks & parks  queue UT completion
                            Figure 19.11      User-mode scheduling.
     19.7.3.8  Winsock
     Winsock is the Windows sockets API. Winsock is a session-layer interface that is
     largely compatible with UNIX sockets but has some added Windows extensions.
     It provides a standardized interface to many transport protocols that may have
     different addressing schemes, so that any Winsock application can run on
     any Winsock-compliant protocol stack. Winsock underwent a major update in
     Windows Vista to add tracing, IPv6 support, impersonation, new security APIs
     and many other features.
     Winsock follows the Windows Open System Architecture (WOSA) model,
     which provides a standard service provider interface (SPI) between applications
     and networking protocols. Applications can load and unload layered protocols
     that build additional functionality, such as additional security, on top of the
     transport protocol layers. Winsock supports asynchronous operations and
     notifications, reliable multicasting, secure sockets, and kernel mode sockets.
     There is also support for simpler usage models, like the WSAConnectByName()
     function, which accepts the target as strings specifying the name or IP address
     of the server and the service or port number of the destination port.
     19.7.4    Interprocess Communication Using Windows Messaging
     Win32 applications handle interprocess communication in several ways. One
     way is by using shared kernel objects. Another is by using the Windows
     messaging facility, an approach that is particularly popular for Win32 GUI
     applications.     One  thread    can   send  a  message    to  another     thread   or  to  a
     window by calling PostMessage(), PostThreadMessage(), SendMessage(),
     SendThreadMessage(), or SendMessageCallback(). Posting a message and
     sending a message differ in this way: the post routines are asynchronous; they
     return immediately, and the calling thread does not know when the message
     is actually delivered. The send routines are synchronous: they block the caller
     until the message has been delivered and processed.



                                                        19.7  Programmer Interface     881
//     allocate     16   MB  at     the  top  of   our  address      space
void       *buf  =  VirtualAlloc(0,           0x1000000,      MEM RESERVE   |   MEM TOP DOWN,
    PAGE READWRITE);
//     commit    the     upper   8  MB   of   the  allocated   space
VirtualAlloc(buf + 0x800000, 0x800000, MEM COMMIT, PAGE READWRITE);
//     do   something    with       the  memory
.   .  .
//     now  decommit     the     memory
VirtualFree(buf          +   0x800000,        0x800000,   MEM  DECOMMIT);
//     release      all  of  the    allocated      address    space
VirtualFree(buf,             0,  MEM  RELEASE);
                 Figure 19.12    Code fragments for allocating virtual memory.
    In addition to sending a message, a thread can send data with the message.
Since processes have separate address spaces, the data must be copied. The
system copies data by calling SendMessage() to send a message of type
WM COPYDATA with a COPYDATASTRUCT data structure that contains the length
and address of the data to be transferred. When the message is sent, Windows
copies the data to a new block of memory and gives the virtual address of the
new block to the receiving process.
    Every   Win32        thread     has  its  own  input      queue  from   which  it  receives
messages. If a Win32 application does not call GetMessage() to handle events
on its input queue, the queue fills up; and after about five seconds, the system
marks the application as "Not Responding".
19.7.5      Memory Management
The Win32 API provides several ways for an application to use memory: virtual
memory, memory-mapped files, heaps, and thread-local storage.
19.7.5.1    Virtual Memory
An application calls VirtualAlloc() to reserve or commit virtual memory
and VirtualFree() to decommit or release the memory. These functions
enable the application to specify the virtual address at which the memory
is allocated. They operate on multiples of the memory page size. Examples of
these functions appear in Figure 19.12.
    A process may lock some of its committed pages into physical memory
by calling VirtualLock(). The maximum number of pages a process can lock
is 30, unless the process first calls SetProcessWorkingSetSize() to increase
the maximum working-set size.
19.7.5.2    Memory-Mapping Files
Another way for an application to use memory is by memory-mapping a file
into its address space. Memory mapping is also a convenient way for two
processes to share memory: both processes map the same file into their virtual
memory. Memory mapping is a multistage process, as you can see in the
example in Figure 19.13.



882  Chapter 19        Windows 7
     //     open    the      file     or  create   it  if  it    does   not  exist
     HANDLE hfile = CreateFile("somefile", GENERIC READ | GENERIC                              WRITE,
         FILE SHARE READ | FILE SHARE WRITE, NULL,
         OPEN ALWAYS, FILE ATTRIBUTE NORMAL, NULL);
     //     create     the      file      mapping  8   MB  in  size
     HANDLE       hmap       =  CreateFileMapping(hfile,             PAGE    READWRITE,
         SEC     COMMIT,        0,    0x800000,    "SHM    1");
     //     now   get     a     view  of  the  space       mapped
     void       *buf   =     MapViewOfFile(hmap,           FILE    MAP  ALL  ACCESS,
         0,     0,  0,    0x800000);
     //     do   something         with   the  mapped      file
     .   .  .
     // now unmap the file
     UnMapViewOfFile(buf);
     CloseHandle(hmap);
     CloseHandle(hfile);
                      Figure 19.13        Code fragments for memory mapping of a file.
         If a process wants to map some address space just to share a memory region
     with another process, no file is needed. The process calls CreateFileMap-
     ping() with a file handle of 0xffffffff and a particular size. The resulting
     file-mapping object can be shared by inheritance, by name lookup, or by handle
     duplication.
     19.7.5.3     Heaps
     Heaps provide a third way for applications to use memory, just as with
     malloc() and free() in standard C. A heap in the Win32 environment is
     a region of reserved address space. When a Win32 process is initialized, it is
     created with a default heap. Since most Win32 applications are multithreaded,
     access to the heap is synchronized to protect the heap's space-allocation data
     structures from being damaged by concurrent updates by multiple threads.
         Win32 provides several heap-management functions so that a process can
     allocate and manage a private heap. These functions are HeapCreate(), Hea-
     pAlloc(), HeapRealloc(), HeapSize(), HeapFree(), and HeapDestroy().
     The Win32 API also provides the HeapLock() and HeapUnlock() functions to
     enable a thread to gain exclusive access to a heap. Unlike VirtualLock(), these
     functions perform only synchronization; they do not lock pages into physical
     memory.
         The original Win32 heap was optimized for efficient use of space. This led to
     significant problems with fragmentation of the address space for larger server
     programs that ran for long periods of time. A new low-fragmentation heap
     (LFH) design introduced in Windows XP greatly reduced the fragmentation
     problem.       The      Windows      7  heap     manager      automatically    turns  on  LFH  as
     appropriate.
     19.7.5.4     Thread-Local Storage
     A fourth way for applications to use memory is through a thread-local storage
     (TLS) mechanism. Functions that rely on global or static data typically fail



                                                              Practice Exercises         883
                       //   reserve   a   slot  for    a  variable
                       DWORD     var index   =  T1sAlloc();
                       //   set  it   to  the   value     10
                       T1sSetValue(var index,          10);
                       //   get  the  value
                       int  var  T1sGetValue(var index);
                       // release the index
                       T1sFree(var index);
               Figure 19.14          Code for dynamic thread-local storage.
      to work properly in a multithreaded environment. For instance, the C run-
      time function strtok() uses a static variable to keep track of its current
      position while parsing a string. For two concurrent threads to execute strtok()
      correctly, they need separate current position variables. TLS provides a way
      to maintain instances of variables that are global to the function being executed
      but not shared with any other thread.
      TLS provides both dynamic and static methods of creating thread-local
      storage. The dynamic method is illustrated in Figure 19.14. The TLS mechanism
      allocates global heap storage and attaches it to the thread environment block
      that Windows allocates to every user-mode thread. The TEB is readily accessible
      by each thread and is used not just for TLS but for all the per-thread state
      information in user mode.
      To use a thread-local static variable, the application declares the variable
      as follows to ensure that every thread has its own private copy:
                           declspec(thread) DWORD cur         pos  =  0;
19.8  Summary
      Microsoft designed Windows to be an extensible, portable operating system
      --one able to take advantage of new techniques and hardware. Windows
      supports multiple operating environments and symmetric multiprocessing,
      including both 32-bit and 64-bit processors and NUMA computers. The use
      of kernel objects to provide basic services, along with support for client­
      server computing, enables Windows to support a wide variety of applica-
      tion environments. Windows provides virtual memory, integrated caching,
      and preemptive scheduling. It supports elaborate security mechanisms and
      includes internationalization features. Windows runs on a wide variety of
      computers, so users can choose and upgrade hardware to match their budgets
      and performance requirements without needing to alter the applications they
      run.
Practice Exercises
      19.1  What type of operating system is Windows? Describe two of its major
            features.
      19.2  List the design goals of Windows. Describe two in detail.



884  Chapter 19  Windows 7
     19.3   Describe the booting process for a Windows system.
     19.4   Describe the three main architectural layers of the Windows kernel.
     19.5   What is the job of the object manager?
     19.6   What types of services does the process manager provide?
     19.7   What is a local procedure call?
     19.8   What are the responsibilities of the I/O manager?
     19.9   What types of networking does Windows support? How does Windows
            implement transport protocols? Describe two networking protocols.
     19.10  How is the NTFS namespace organized?
     19.11  How does NTFS handle data structures? How does NTFS recover from
            a system crash? What is guaranteed after a recovery takes place? `
     19.12  How does Windows allocate user memory?
     19.13  Describe some of the ways in which an application can use memory
            via the Win32 API.
Exercises
     19.14  Under what circumstances would one use the deferred procedure calls
            facility in Windows?
     19.15  What is a handle, and how does a process obtain a handle?
     19.16  Describe the management scheme of the virtual memory manager. How
            does the VM manager improve performance?
     19.17  Describe a useful application of the no-access page facility provided in
            Windows.
     19.18  Describe the three techniques used for communicating data in a local
            procedure call. What settings are most conducive to the application of
            the different message-passing techniques?
     19.19  What manages caching in Windows? How is the cache managed?
     19.20  How  does  the  NTFS      directory  structure  differ  from  the  directory
            structure used in UNIX operating systems?
     19.21  What is a process, and how is it managed in Windows?
     19.22  What is the fiber abstraction provided by Windows? How does it differ
            from the thread abstraction?
     19.23  How does user-mode scheduling (UMS) in Windows 7 differ from
            fibers? What are some trade-offs between fibers and UMS?
     19.24  UMS considers a thread to have two parts, a UT and a KT. How might it
            be useful to allow UTs to continue executing in parallel with their KTs?
     19.25  What is the performance trade-off of allowing KTs and UTs to execute
            on different processors?



                                                              Bibliography           885
19.26  Why does the self-map occupy large amounts of virtual address space
       but no additional virtual memory?
19.27  How does the self-map make it easy for the VM manager to move the
       page-table pages to and from disk? Where are the page-table pages
       kept on disk?
19.28  When       a  Windows     system   hibernates,   the  system     is  powered  off.
       Suppose you changed the CPU or the amount of RAM on a hibernating
       system. Do you think that would work? Why or why not?
19.29  Give an example showing how the use of a suspend count is helpful in
       suspending and resuming threads in Windows.
Bibliographical Notes
[Russinovich      and  Solomon   (2009)]  give  an  overview        of  Windows  7   and
considerable technical detail about system internals and components.
[Brown (2000)] presents details of the security architecture of Windows.
The Microsoft Developer Network Library (http://msdn.microsoft.com)
supplies a wealth of information on Windows and other Microsoft products,
including documentation of all the published APIs.
[Iseminger (2000)] provides a good reference on the Windows Active
Directory. Detailed discussions of writing programs that use the Win32 API
appear in [Richter (1997)]. [Silberschatz et al. (2010)] supply a good discussion
of B+ trees.
The source code for a 2005 WRK version of the Windows kernel, together
with a collection of slides and other CRK curriculum materials, is available from
www.microsoft.com/WindowsAcademic for use by universities.
Bibliography
[Brown (2000)]       K.  Brown,  Programming    Windows      Security,  Addison-Wesley
(2000).
[Iseminger (2000)]       D. Iseminger, Active Directory Services for Microsoft Windows
2000. Technical Reference, Microsoft Press (2000).
[Richter (1997)]       J. Richter, Advanced Windows, Microsoft Press (1997).
[Russinovich and Solomon (2009)]          M. E. Russinovich and D. A. Solomon, Win-
dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,
Microsoft Press (2009).
[Silberschatz et al. (2010)]     A.  Silberschatz,  H.  F.  Korth,  and     S.  Sudarshan,
Database System Concepts, Sixth Edition, McGraw-Hill (2010).



