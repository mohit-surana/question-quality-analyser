Process Synchronization


Process                                                      5C H A P T E R
Synchronization
     A cooperating process is one that can affect or be affected by other processes
     executing in the system. Cooperating processes can either directly share a
     logical address space (that is, both code and data) or be allowed to share data
     only through files or messages. The former case is achieved through the use of
     threads, discussed in Chapter 4. Concurrent access to shared data may result in
     data inconsistency, however. In this chapter, we discuss various mechanisms
     to ensure the orderly execution of cooperating processes that share a logical
     address space, so that data consistency is maintained.
     CHAPTER OBJECTIVES
     ·  To introduce the critical-section problem, whose solutions can be used to
        ensure the consistency of shared data.
     ·  To present both software and hardware solutions of the critical-section
        problem.
     ·  To examine several classical process-synchronization problems.
     ·  To explore several tools that are used to solve process synchronization
        problems.
5.1  Background
     We've already seen that processes can execute concurrently or in parallel.
     Section 3.2.2 introduced the role of process scheduling and described how
     the CPU scheduler switches rapidly between processes to provide concurrent
     execution. This means that one process may only partially complete execution
     before another process is scheduled. In fact, a process may be interrupted at
     any point in its instruction stream, and the processing core may be assigned
     to execute instructions of another process. Additionally, Section 4.2 introduced
     parallel execution, in which two instruction streams (representing different
     processes) execute simultaneously on separate processing cores. In this chapter,
                                                                                       203



204  Chapter 5     Process Synchronization
     we explain how concurrent or parallel execution can contribute to issues
     involving the integrity of data shared by several processes.
     Let's consider an example of how this can happen. In Chapter 3, we devel-
     oped a model of a system consisting of cooperating sequential processes or
     threads, all running asynchronously and possibly sharing data. We illustrated
     this model with the producer­consumer problem, which is representative of
     operating systems. Specifically, in Section 3.4.1, we described how a bounded
     buffer could be used to enable processes to share memory.
     We now return to our consideration of the bounded buffer. As we pointed
     out, our original solution allowed at most BUFFER SIZE - 1 items in the buffer
     at the same time. Suppose we want to modify the algorithm to remedy this
     deficiency. One possibility is to add an integer variable counter, initialized to
     0. counter is incremented every time we add a new item to the buffer and is
     decremented every time we remove one item from the buffer. The code for the
     producer process can be modified as follows:
                while (true) {
                   /* produce      an   item     in    next  produced  */
                   while     (counter      ==  BUFFER SIZE)
                       ;  /*  do   nothing     */
                   buffer[in]      = next produced;
                   in = (in +      1)   %   BUFFER SIZE;
                   counter++;
                }
     The code for the consumer process can be modified as follows:
                while (true) {
                   while     (counter      ==  0)
                       ;  /*  do   nothing     */
                   next consumed       =    buffer[out];
                   out    =  (out  +   1)   %  BUFFER SIZE;
                   counter--;
                   /*   consume    the     item    in  next  consumed  */
                }
     Although the producer and consumer routines shown above are correct
     separately, they may not function correctly when executed concurrently. As
     an illustration, suppose that the value of the variable counter is currently
     5 and that the producer and consumer processes concurrently execute the
     statements "counter++" and "counter--". Following the execution of these
     two statements, the value of the variable counter may be 4, 5, or 6! The only
     correct result, though, is counter == 5, which is generated correctly if the
     producer and consumer execute separately.



                                                             5.1  Background          205
    We can show that the value of counter may be incorrect as follows. Note
that the statement "counter++" may be implemented in machine language (on
a typical machine) as follows:
                                register1 = counter
                                register1 = register1 + 1
                                counter = register1
where  register1   is  one  of  the  local  CPU  registers.  Similarly,  the  statement
"counter--" is implemented as follows:
                                register2 = counter
                                register2 = r egister2 - 1
                                counter = register2
where again register2 is one of the local CPU registers. Even though register1 and
register2 may be the same physical register (an accumulator, say), remember
that the contents of this register will be saved and restored by the interrupt
handler (Section 1.2.3).
    The concurrent execution of "counter++" and "counter--" is equivalent
to  a  sequential  execution    in   which  the  lower-level      statements  presented
previously are interleaved in some arbitrary order (but the order within each
high-level statement is preserved). One such interleaving is the following:
       T0:  producer   execute       r egister1 = counter         {r egister1 = 5}
       T1:  producer   execute       register1 = register1 + 1    {r egister1 = 6}
       T2:  consumer   execute       r egister2 = counter         {r egister2 = 5}
       T3:  consumer   execute       register2 = register2 - 1    {r egister2 = 4}
       T4:  producer   execute       counter = r egister1         {counter = 6}
       T5:  consumer   execute       counter = r egister2         {counter = 4}
Notice that we have arrived at the incorrect state "counter == 4", indicating
that four buffers are full, when, in fact, five buffers are full. If we reversed the
order of the statements at T4 and T5, we would arrive at the incorrect state
"counter == 6".
    We would arrive at this incorrect state because we allowed both processes
to manipulate the variable counter concurrently. A situation like this, where
several processes access and manipulate the same data concurrently and the
outcome of the execution depends on the particular order in which the access
takes place, is called a race condition. To guard against the race condition
above, we need to ensure that only one process at a time can be manipulating
the variable counter. To make such a guarantee, we require that the processes
be synchronized in some way.
    Situations such as the one just described occur frequently in operating
systems as different parts of the system manipulate resources. Furthermore, as
we have emphasized in earlier chapters, the growing importance of multicore
systems has brought an increased emphasis on developing multithreaded
applications. In such applications, several threads--which are quite possibly
sharing data--are running in parallel on different processing cores. Clearly,



206  Chapter 5   Process Synchronization
                                do {
                                         entry section
                                          critical section
                                         exit section
                                          remainder section
                                } while (true);
                    Figure 5.1  General structure of a typical process    Pi  .
     we want any changes that result from such activities not to interfere with one
     another. Because of the importance of this issue, we devote a major portion of
     this chapter to process synchronization and coordination among cooperating
     processes.
5.2  The Critical-Section Problem
     We begin our consideration of process synchronization by discussing the so-
     called critical-section problem. Consider a system consisting of n processes
     {P0, P1, ..., Pn-1}. Each process has a segment of code, called a critical section,
     in which the process may be changing common variables, updating a table,
     writing a file, and so on. The important feature of the system is that, when
     one process is executing in its critical section, no other process is allowed to
     execute in its critical section. That is, no two processes are executing in their
     critical sections at the same time. The critical-section problem is to design a
     protocol that the processes can use to cooperate. Each process must request
     permission to enter its critical section. The section of code implementing this
     request is the entry section. The critical section may be followed by an exit
     section. The remaining code is the remainder section. The general structure of
     a typical process Pi is shown in Figure 5.1. The entry section and exit section
     are enclosed in boxes to highlight these important segments of code.
         A solution to the critical-section problem must satisfy the following three
     requirements:
     1.  Mutual exclusion. If process Pi is executing in its critical section, then no
         other processes can be executing in their critical sections.
     2.  Progress.  If  no  process  is  executing     in  its  critical  section  and  some
         processes wish to enter their critical sections, then only those processes
         that are not executing in their remainder sections can participate in
         deciding which will enter its critical section next, and this selection cannot
         be postponed indefinitely.
     3.  Bounded waiting. There exists a bound, or limit, on the number of times
         that other processes are allowed to enter their critical sections after a



                                                5.3       Peterson's Solution                 207
     process has made a request to enter its critical section and before that
     request is granted.
     We assume that each process is executing at a nonzero speed. However, we can
     make no assumption concerning the relative speed of the n processes.
     At a given point in time, many kernel-mode processes may be active in
     the operating system. As a result, the code implementing an operating system
     (kernel code) is subject to several possible race conditions. Consider as an
     example a kernel data structure that maintains a list of all open files in the
     system. This list must be modified when a new file is opened or closed (adding
     the file to the list or removing it from the list). If two processes were to open files
     simultaneously, the separate updates to this list could result in a race condition.
     Other kernel data structures that are prone to possible race conditions include
     structures for maintaining memory allocation, for maintaining process lists,
     and for interrupt handling. It is up to kernel developers to ensure that the
     operating system is free from such race conditions.
     Two general approaches are used to handle critical sections in operating
     systems:  preemptive  kernels  and  nonpreemptive    kernels.  A      preemptive
     kernel allows a process to be preempted while it is running in kernel mode. A
     nonpreemptive kernel does not allow a process running in kernel mode to be
     preempted; a kernel-mode process will run until it exits kernel mode, blocks,
     or voluntarily yields control of the CPU.
     Obviously, a nonpreemptive kernel is essentially free from race conditions
     on kernel data structures, as only one process is active in the kernel at a time.
     We cannot say the same about preemptive kernels, so they must be carefully
     designed to ensure that shared kernel data are free from race conditions.
     Preemptive kernels are especially difficult to design for SMP architectures,
     since in these environments it is possible for two kernel-mode processes to run
     simultaneously on different processors.
     Why, then, would anyone favor a preemptive kernel over a nonpreemptive
     one? A preemptive kernel may be more responsive, since there is less risk that a
     kernel-mode process will run for an arbitrarily long period before relinquishing
     the processor to waiting processes. (Of course, this risk can also be minimized
     by designing kernel code that does not behave in this way.) Furthermore, a
     preemptive kernel is more suitable for real-time programming, as it will allow
     a real-time process to preempt a process currently running in the kernel. Later
     in this chapter, we explore how various operating systems manage preemption
     within the kernel.
5.3  Peterson's Solution
     Next, we illustrate a classic software-based solution to the critical-section
     problem known as Peterson's solution. Because of the way modern computer
     architectures perform basic machine-language instructions, such as load and
     store, there are no guarantees that Peterson's solution will work correctly on
     such architectures. However, we present the solution because it provides a good
     algorithmic description of solving the critical-section problem and illustrates
     some of the complexities involved in designing software that addresses the
     requirements of mutual exclusion, progress, and bounded waiting.



208  Chapter 5    Process Synchronization
                            do   {
                                    flag[i] = true;
                                    turn = j;
                                    while  (flag[j]     &&  turn  ==  j);
                                      critical section
                                    flag[i] = false;
                                      remainder section
                            } while (true);
                  Figure 5.2     The structure of process Pi in Peterson's solution.
         Peterson's solution is restricted to two processes that alternate execution
     between    their  critical  sections    and  remainder   sections.   The         processes    are
     numbered     P0   and  P1.  For  convenience,      when  presenting   Pi ,       we  use  Pj  to
     denote the other process; that is, j equals 1 - i.
         Peterson's solution requires the two processes to share two data items:
                       int    turn;
                       boolean      flag[2];
     The variable turn indicates whose turn it is to enter its critical section. That is,
     if turn  ==  i, then process     Pi   is allowed to execute in its critical section. The
     flag array is used to indicate if a process is ready to enter its critical section.
     For example, if flag[i] is true, this value indicates that Pi is ready to enter
     its critical section. With an explanation of these data structures complete, we
     are now ready to describe the algorithm shown in Figure 5.2.
         To enter the critical section, process Pi first sets flag[i] to be true and
     then sets turn to the value j, thereby asserting that if the other process wishes
     to enter the critical section, it can do so. If both processes try to enter at the same
     time, turn will be set to both i and j at roughly the same time. Only one of these
     assignments will last; the other will occur but will be overwritten immediately.
     The eventual value of turn determines which of the two processes is allowed
     to enter its critical section first.
         We now prove that this solution is correct. We need to show that:
     1.  Mutual exclusion is preserved.
     2.  The progress requirement is satisfied.
     3.  The bounded-waiting requirement is met.
         To prove property 1, we note that each Pi enters its critical section only
     if either flag[j] == false or turn == i. Also note that, if both processes
     can be executing in their critical sections at the same time, then flag[0] ==
     flag[1] == true. These two observations imply that P0 and P1 could not have
     successfully executed their while statements at about the same time, since the



                                             5.4   Synchronization Hardware                  209
     value of turn can be either 0 or 1 but cannot be both. Hence, one of the processes
     --say, Pj --must have successfully executed the while statement, whereas Pi
     had to execute at least one additional statement ("turn == j"). However, at
     that time, flag[j] == true and turn == j, and this condition will persist as
     long as Pj is in its critical section; as a result, mutual exclusion is preserved.
     To prove properties 2 and 3, we note that a process Pi can be prevented from
     entering the critical section only if it is stuck in the while loop with the condition
     flag[j] == true and turn == j; this loop is the only one possible. If Pj is not
     ready to enter the critical section, then flag[j] == false, and Pi can enter its
     critical section. If Pj has set flag[j] to true and is also executing in its while
     statement, then either turn == i or turn == j. If turn == i, then Pi will enter
     the critical section. If turn == j, then Pj will enter the critical section. However,
     once Pj exits its critical section, it will reset flag[j] to false, allowing Pi to
     enter its critical section. If Pj resets flag[j] to true, it must also set turn to i.
     Thus, since Pi does not change the value of the variable turn while executing
     the while statement, Pi will enter the critical section (progress) after at most
     one entry by Pj (bounded waiting).
5.4  Synchronization Hardware
     We have just described one software-based solution to the critical-section
     problem. However, as mentioned, software-based solutions such as Peterson's
     are not guaranteed to work on modern computer architectures. In the following
     discussions, we explore several more solutions to the critical-section problem
     using techniques ranging from hardware to software-based APIs available to
     both kernel developers and application programmers. All these solutions are
     based on the premise of locking --that is, protecting critical regions through
     the use of locks. As we shall see, the designs of such locks can be quite
     sophisticated.
     We start by presenting some simple hardware instructions that are available
     on many systems and showing how they can be used effectively in solving the
     critical-section problem. Hardware features can make any programming task
     easier and improve system efficiency.
     The critical-section problem could be solved simply in a single-processor
     environment if we could prevent interrupts from occurring while a shared
     variable was being modified. In this way, we could be sure that the current
     sequence of instructions would be allowed to execute in order without pre-
     emption. No other instructions would be run, so no unexpected modifications
     could be made to the shared variable. This is often the approach taken by
     nonpreemptive kernels.
                     boolean test and set(boolean *target) {
                        boolean  rv   =  *target;
                        *target  =    true;
                        return   rv;
                     }
     Figure 5.3         The definition of the test and set() instruction.



210  Chapter 5   Process Synchronization
                          do   {
                               while (test and set(&lock))
                                  ;   /*  do   nothing  */
                                  /*  critical     section       */
                               lock   =   false;
                                  /*  remainder         section    */
                          }   while   (true);
             Figure 5.4      Mutual-exclusion implementation with test        and  set().
        Unfortunately, this solution is not as feasible in a multiprocessor environ-
     ment. Disabling interrupts on a multiprocessor can be time consuming, since
     the message is passed to all the processors. This message passing delays entry
     into each critical section, and system efficiency decreases. Also consider the
     effect on a system's clock if the clock is kept updated by interrupts.
        Many     modern        computer   systems       therefore    provide  special   hardware
     instructions that allow us either to test and modify the content of a word or
     to swap the contents of two words atomically--that is, as one uninterruptible
     unit. We can use these special instructions to solve the critical-section problem
     in a relatively simple manner. Rather than discussing one specific instruction
     for one specific machine, we abstract the main concepts behind these types
     of instructions by describing the test and set() and compare and swap()
     instructions.
        The test and set() instruction can be defined as shown in Figure 5.3.
     The important characteristic of this instruction is that it is executed atomically.
     Thus, if two test and set() instructions are executed simultaneously (each
     on a different CPU), they will be executed sequentially in some arbitrary order. If
     the machine supports the test and set() instruction, then we can implement
     mutual exclusion by declaring a boolean variable lock, initialized to false.
     The structure of process Pi is shown in Figure 5.4.
        The compare and swap() instruction, in contrast to the test and set()
     instruction, operates on three operands; it is defined in Figure 5.5. The operand
     value   is  set  to  new  value     only  if  the  expression   (*value  ==   exected)     is
     true. Regardless, compare and swap() always returns the original value of the
     variable value. Like the test and set() instruction, compare and swap() is
     int compare and swap(int             *value,       int  expected,  int        new  value)  {
        int  temp     =   *value;
        if   (*value      ==   expected)
            *value = new value;
        return temp;
     }
             Figure 5.5       The definition of the compare and swap() instruction.



                                                    5.4  Synchronization Hardware     211
do  {
    while (compare and              swap(&lock,              0,  1)  !=  0)
       ;    /*    do    nothing     */
       /*   critical           section        */
    lock    =    0;
       /*   remainder          section           */
}   while   (true);
Figure 5.6  Mutual-exclusion implementation with the compare and swap()
                                           instruction.
executed atomically. Mutual exclusion can be provided as follows: a global
variable (lock) is declared and is initialized to 0. The first process that invokes
compare and swap() will set lock to 1. It will then enter its critical section,
because the original value of lock was equal to the expected value of 0.
Subsequent calls to compare and swap() will not succeed, because lock now
is not equal to the expected value of 0. When a process exits its critical section,
it sets lock back to 0, which allows another process to enter its critical section.
The structure of process Pi is shown in Figure 5.6.
Although these algorithms satisfy the mutual-exclusion requirement, they
do not satisfy the bounded-waiting requirement. In Figure 5.7, we present
another algorithm using the test and set() instruction that satisfies all the
critical-section requirements. The common data structures are
               do    {
                   waiting[i]           =     true;
                   key = true;
                   while       (waiting[i]               &&  key)
                        key = test and set(&lock);
                   waiting[i]           =     false;
                        /*     critical          section     */
                   j    =  (i  +    1)     %  n;
                   while       ((j  !=        i)    &&   !waiting[j])
                        j   =  (j   +   1)       %  n;
                   if      (j  ==   i)
                        lock   =    false;
                   else
                        waiting[j]            =     false;
                        /*     remainder            section  */
               }   while       (true);
    Figure  5.7    Bounded-waiting mutual exclusion with test            and  set().



212  Chapter 5  Process Synchronization
                             boolean       waiting[n];
                             boolean       lock;
     These data structures are initialized to false. To prove that the mutual-
     exclusion  requirement  is  met,  we  note  that  process   Pi  can  enter  its  critical
     section only if either waiting[i] == false or key == false. The value
     of key can become false only if the test and set() is executed. The first
     process to execute the test and set() will find key == false; all others must
     wait. The variable waiting[i] can become false only if another process
     leaves its critical section; only one waiting[i] is set to false, maintaining the
     mutual-exclusion requirement.
     To prove that the progress requirement is met, we note that the arguments
     presented for mutual exclusion also apply here, since a process exiting the
     critical section either sets lock to false or sets waiting[j] to false. Both
     allow a process that is waiting to enter its critical section to proceed.
     To prove that the bounded-waiting requirement is met, we note that, when
     a process leaves its critical section, it scans the array waiting in the cyclic
     ordering (i + 1, i + 2, ..., n - 1, 0, ..., i - 1). It designates the first process in this
     ordering that is in the entry section (waiting[j] == true) as the next one to
     enter the critical section. Any process waiting to enter its critical section will
     thus do so within n - 1 turns.
     Details    describing   the  implementation       of  the   atomic   test   and  set()
     and compare and swap() instructions are discussed more fully in books on
     computer architecture.
5.5  Mutex Locks
     The hardware-based solutions to the critical-section problem presented in
     Section 5.4 are complicated as well as generally inaccessible to application
     programmers. Instead, operating-systems designers build software tools to
     solve the critical-section problem. The simplest of these tools is the mutex
     lock. (In fact, the term mutex is short for mutual exclusion.) We use the mutex
     lock to protect critical regions and thus prevent race conditions. That is, a
     process must acquire the lock before entering a critical section; it releases the
     lock when it exits the critical section. The acquire()function acquires the lock,
     and the release() function releases the lock, as illustrated in Figure 5.8.
     A mutex lock has a boolean variable available whose value indicates if
     the lock is available or not. If the lock is available, a call to acquire() succeeds,
     and the lock is then considered unavailable. A process that attempts to acquire
     an unavailable lock is blocked until the lock is released.
     The definition of acquire() is as follows:
                             acquire() {
                                     while (!available)
                                       ;   /*  busy    wait  */
                                     available = false;;
                             }



                                                                   5.6  Semaphores         213
                                   do  {
                                          acquire lock
                                                 critical section
                                          release lock
                                                 remainder section
                                   } while (true);
          Figure      5.8  Solution to the critical-section problem using mutex locks.
     The  definition  of release() is as follows:
                                release() {
                                   available       =  true;
                                }
          Calls to either acquire() or release() must be performed atomically.
     Thus, mutex locks are often implemented using one of the hardware mecha-
     nisms described in Section 5.4, and we leave the description of this technique
     as an exercise.
          The main disadvantage of the implementation given here is that it requires
     busy waiting. While a process is in its critical section, any other process that
     tries to enter its critical section must loop continuously in the call to acquire().
     In fact, this type of mutex lock is also called a spinlock because the process
     "spins" while waiting for the lock to become available. (We see the same issue
     with the code examples illustrating the test and set() instruction and the
     compare and swap() instruction.) This continual looping is clearly a problem
     in a real multiprogramming system, where a single CPU is shared among many
     processes. Busy waiting wastes CPU cycles that some other process might be
     able to use productively.
          Spinlocks do have an advantage, however, in that no context switch is
     required when a process must wait on a lock, and a context switch may
     take considerable time. Thus, when locks are expected to be held for short
     times, spinlocks are useful. They are often employed on multiprocessor systems
     where one thread can "spin" on one processor while another thread performs
     its critical section on another processor.
          Later in this chapter (Section 5.7), we examine how mutex locks can be
     used to solve classical synchronization problems. We also discuss how these
     locks are used in several operating systems, as well as in Pthreads.
5.6  Semaphores
     Mutex locks, as we mentioned earlier, are generally considered the simplest of
     synchronization tools. In this section, we examine a more robust tool that can



214  Chapter 5  Process Synchronization
     behave similarly to a mutex lock but can also provide more sophisticated ways
     for processes to synchronize their activities.
     A semaphore S is an integer variable that, apart from initialization, is
     accessed only through two standard atomic operations: wait() and signal().
     The wait() operation was originally termed P (from the Dutch proberen, "to
     test"); signal() was originally called V (from verhogen, "to increment"). The
     definition of wait() is as follows:
                            wait(S) {
                               while         (S     <= 0)
                                          ;  //     busy  wait
                               S--;
                            }
     The definition of signal() is as follows:
                            signal(S)            {
                               S++;
                            }
     All modifications to the integer value of the semaphore in the wait() and
     signal() operations must be executed indivisibly. That is, when one process
     modifies the semaphore value, no other process can simultaneously modify
     that same semaphore value. In addition, in the case of wait(S), the testing of
     the integer value of S (S  0), as well as its possible modification (S--), must
     be executed without interruption. We shall see how these operations can be
     implemented in Section 5.6.2. First, let's see how semaphores can be used.
     5.6.1  Semaphore Usage
     Operating systems often distinguish between counting and binary semaphores.
     The value of a counting semaphore can range over an unrestricted domain.
     The value of a binary semaphore can range only between 0 and 1. Thus, binary
     semaphores behave similarly to mutex locks. In fact, on systems that do not
     provide mutex locks, binary semaphores can be used instead for providing
     mutual exclusion.
     Counting semaphores can be used to control access to a given resource
     consisting of a finite number of instances. The semaphore is initialized to the
     number of resources available. Each process that wishes to use a resource
     performs a wait() operation on the semaphore (thereby decrementing the
     count). When a process releases a resource, it performs a signal() operation
     (incrementing the count). When the count for the semaphore goes to 0, all
     resources are being used. After that, processes that wish to use a resource will
     block until the count becomes greater than 0.
     We can also use semaphores to solve various synchronization problems.
     For example, consider two concurrently running processes: P1 with a statement
     S1 and P2 with a statement S2. Suppose we require that S2 be executed only
     after S1 has completed. We can implement this scheme readily by letting P1
     and P2 share a common semaphore synch, initialized to 0. In process P1, we
     insert the statements



                                                      5.6  Semaphores               215
                           S1;
             signal(synch);
In process P2, we insert the statements
             wait(synch);
                           S2;
Because synch is initialized to 0, P2 will execute S2 only after P1 has invoked
signal(synch), which is after statement S1 has been executed.
5.6.2     Semaphore Implementation
Recall that the implementation of mutex locks discussed in Section 5.5 suffers
from busy waiting. The definitions of the wait() and signal() semaphore
operations just described present the same problem. To overcome the need
for busy waiting, we can modify the definition of the wait() and signal()
operations as follows: When a process executes the wait() operation and finds
that the semaphore value is not positive, it must wait. However, rather than
engaging in busy waiting, the process can block itself. The block operation
places a process into a waiting queue associated with the semaphore, and the
state of the process is switched to the waiting state. Then control is transferred
to the CPU scheduler, which selects another process to execute.
A process that is blocked, waiting on a semaphore S, should be restarted
when some other process executes a signal() operation. The process is
restarted by a wakeup() operation, which changes the process from the waiting
state to the ready state. The process is then placed in the ready queue. (The
CPU may or may not be switched from the running process to the newly ready
process, depending on the CPU-scheduling algorithm.)
To implement semaphores under this definition, we define a semaphore as
follows:
             typedef struct {
                                int  value;
                                struct process  *list;
             } semaphore;
Each semaphore has an integer value and a list of processes list. When
a process must wait on a semaphore, it is added to the list of processes. A
signal() operation removes one process from the list of waiting processes
and awakens that process.
Now, the wait() semaphore operation can be defined as
          wait(semaphore *S) {
                           S->value--;
                           if (S->value < 0) {
                                     add this process to S->list;
                                     block();
                           }
          }



216  Chapter 5  Process Synchronization
     and the signal() semaphore operation can be defined as
                signal(semaphore *S) {
                        S->value++;
                        if (S->value <= 0) {
                                    remove a process P from S->list;
                                    wakeup(P);
                        }
                }
     The block() operation suspends the process that invokes it. The wakeup(P)
     operation resumes the execution of a blocked process P. These two operations
     are provided by the operating system as basic system calls.
     Note that in this implementation, semaphore values may be negative,
     whereas semaphore values are never negative under the classical definition of
     semaphores with busy waiting. If a semaphore value is negative, its magnitude
     is the number of processes waiting on that semaphore. This fact results from
     switching the order of the decrement and the test in the implementation of the
     wait() operation.
     The list of waiting processes can be easily implemented by a link field in
     each process control block (PCB). Each semaphore contains an integer value and
     a pointer to a list of PCBs. One way to add and remove processes from the list
     so as to ensure bounded waiting is to use a FIFO queue, where the semaphore
     contains both head and tail pointers to the queue. In general, however, the list
     can use any queueing strategy. Correct usage of semaphores does not depend
     on a particular queueing strategy for the semaphore lists.
     It is critical that semaphore operations be executed atomically. We must
     guarantee that no two processes can execute wait() and signal() operations
     on the same semaphore at the same time. This is a critical-section problem;
     and in a single-processor environment, we can solve it by simply inhibiting
     interrupts during the time the wait() and signal() operations are executing.
     This scheme works in a single-processor environment because, once interrupts
     are inhibited, instructions from different processes cannot be interleaved. Only
     the currently running process executes until interrupts are reenabled and the
     scheduler can regain control.
     In a multiprocessor environment, interrupts must be disabled on every pro-
     cessor. Otherwise, instructions from different processes (running on different
     processors) may be interleaved in some arbitrary way. Disabling interrupts on
     every processor can be a difficult task and furthermore can seriously diminish
     performance. Therefore, SMP systems must provide alternative locking tech-
     niques--such as compare and swap() or spinlocks--to ensure that wait()
     and signal() are performed atomically.
     It is important to admit that we have not completely eliminated busy
     waiting with this definition of the wait() and signal() operations. Rather,
     we have moved busy waiting from the entry section to the critical sections
     of application programs. Furthermore, we have limited busy waiting to the
     critical sections of the wait() and signal() operations, and these sections are
     short (if properly coded, they should be no more than about ten instructions).
     Thus, the critical section is almost never occupied, and busy waiting occurs



                                                         5.6  Semaphores          217
rarely, and then for only a short time. An entirely different situation exists
with application programs whose critical sections may be long (minutes or
even hours) or may almost always be occupied. In such cases, busy waiting is
extremely inefficient.
5.6.3  Deadlocks and Starvation
The implementation of a semaphore with a waiting queue may result in a
situation where two or more processes are waiting indefinitely for an event
that can be caused only by one of the waiting processes. The event in question
is the execution of a signal() operation. When such a state is reached, these
processes are said to be deadlocked.
To illustrate this, consider a system consisting of two processes, P0 and P1,
each accessing two semaphores, S and Q, set to the value 1:
                                 P0            P1
                        wait(S);               wait(Q);
                        wait(Q);               wait(S);
                                 .             .
                                 .             .
                                 .             .
                        signal(S);             signal(Q);
                        signal(Q);             signal(S);
Suppose that P0 executes wait(S) and then P1 executes wait(Q). When P0
executes wait(Q), it must wait until P1 executes signal(Q). Similarly, when
P1 executes wait(S), it must wait until P0 executes signal(S). Since these
signal() operations cannot be executed, P0 and P1 are deadlocked.
We say that a set of processes is in a deadlocked state when every process
in the set is waiting for an event that can be caused only by another process
in the set. The events with which we are mainly concerned here are resource
acquisition and release. Other types of events may result in deadlocks, as we
show in Chapter 7. In that chapter, we describe various mechanisms for dealing
with the deadlock problem.
Another problem related to deadlocks is indefinite blocking or starvation,
a situation in which processes wait indefinitely within the semaphore. Indefi-
nite blocking may occur if we remove processes from the list associated with a
semaphore in LIFO (last-in, first-out) order.
5.6.4  Priority Inversion
A scheduling challenge arises when a higher-priority process needs to read
or modify kernel data that are currently being accessed by a lower-priority
process--or a chain of lower-priority processes. Since kernel data are typically
protected with a lock, the higher-priority process will have to wait for a
lower-priority one to finish with the resource. The situation becomes more
complicated if the lower-priority process is preempted in favor of another
process with a higher priority.
As an example, assume we have three processes-- L, M, and H --whose
priorities follow the order L        <  M  <   H. Assume that process  H  requires



218  Chapter 5  Process Synchronization
                PRIORITY INVERSION AND THE MARS PATHFINDER
     Priority inversion can be more than a scheduling inconvenience. On systems
     with tight time constraints -- such as real-time systems -- priority inversion
     can cause a process to take longer than it should to accomplish a task. When
     that happens, other failures can cascade, resulting in system failure.
            Consider the Mars Pathfinder, a NASA space probe that landed a robot, the
     Sojourner rover, on Mars in 1997 to conduct experiments. Shortly after the
     Sojourner began operating, it started to experience frequent computer resets.
     Each reset reinitialized all hardware and software, including communica-
     tions. If the problem had not been solved, the Sojourner would have failed in
     its mission.
            The problem was caused by the fact that one high-priority task, "bc dist,"
     was taking longer than expected to complete its work. This task was being
     forced to wait for a shared resource that was held by the lower-priority
     "ASI/MET" task, which in turn was preempted by multiple medium-priority
     tasks. The "bc dist" task would stall waiting for the shared resource, and
     ultimately the "bc sched" task would discover the problem and perform the
     reset. The Sojourner was suffering from a typical case of priority inversion.
            The operating system on the Sojourner was the VxWorks real-time operat-
     ing system, which had a global variable to enable priority inheritance on all
     semaphores. After testing, the variable was set on the Sojourner (on Mars!),
     and the problem was solved.
            A  full  description   of   the  problem,  its  detection,  and  its    solu-
     tion      was   written  by   the  software  team      lead  and   is  available   at
     http://research.microsoft.com/en-us/um/people/mbj/mars pathfinder/
     authoritative account.html.
     resource R, which is currently being accessed by process L. Ordinarily, process
     H would wait for L to finish using resource R. However, now suppose that
     process M becomes runnable, thereby preempting process L. Indirectly, a
     process with a lower priority--process M--has affected how long process
     H must wait for L to relinquish resource R.
     This problem is known as priority inversion. It occurs only in systems with
     more than two priorities, so one solution is to have only two priorities. That is
     insufficient for most general-purpose operating systems, however. Typically
     these  systems  solve    the  problem   by   implementing    a  priority-inheritance
     protocol. According to this protocol, all processes that are accessing resources
     needed by a higher-priority process inherit the higher priority until they are
     finished with the resources in question. When they are finished, their priorities
     revert to their original values. In the example above, a priority-inheritance
     protocol would allow process L to temporarily inherit the priority of process
     H, thereby preventing process M from preempting its execution. When process
     L had finished using resource R, it would relinquish its inherited priority from
     H and assume its original priority. Because resource R would now be available,
     process H --not M--would run next.



                                       5.7     Classic Problems of Synchronization     219
                       do       {
                                    .  .    .
                                /*  produce       an  item  in  next produced   */
                                    .  .    .
                                wait(empty);
                                wait(mutex);
                                    .  .    .
                                /*  add     next  produced     to  the  buffer  */
                                    .  .    .
                                signal(mutex);
                                signal(full);
                       } while (true);
                       Figure 5.9   The structure of the producer process.
5.7  Classic Problems of Synchronization
     In this section, we present a number of synchronization problems as examples
     of a large class of concurrency-control problems. These problems are used for
     testing nearly every newly proposed synchronization scheme. In our solutions
     to the problems, we use semaphores for synchronization, since that is the
     traditional way to present such solutions. However, actual implementations of
     these solutions could use mutex locks in place of binary semaphores.
     5.7.1  The Bounded-Buffer Problem
     The bounded-buffer problem was introduced in Section 5.1; it is commonly
     used to illustrate the power of synchronization primitives. Here, we present a
     general structure of this scheme without committing ourselves to any particular
     implementation. We provide a related programming project in the exercises at
     the end of the chapter.
     In our problem, the producer and consumer processes share the following
     data structures:
                                    int     n;
                                    semaphore     mutex     =  1;
                                    semaphore     empty     =  n;
                                    semaphore     full = 0
     We assume that the pool consists of n buffers, each capable of holding one item.
     The mutex semaphore provides mutual exclusion for accesses to the buffer pool
     and is initialized to the value 1. The empty and full semaphores count the
     number of empty and full buffers. The semaphore empty is initialized to the
     value n; the semaphore full is initialized to the value 0.
     The code for the producer process is shown in Figure 5.9, and the code
     for the consumer process is shown in Figure 5.10. Note the symmetry between
     the producer and the consumer. We can interpret this code as the producer
     producing full buffers for the consumer or as the consumer producing empty
     buffers for the producer.



220  Chapter 5  Process Synchronization
                do     {
                       wait(full);
                       wait(mutex);
                           ...
                       /*  remove   an  item    from  buffer     to  next  consumed    */
                           ...
                       signal(mutex);
                       signal(empty);
                           ...
                       /*  consume  the    item  in   next consumed      */
                           ...
                } while (true);
                Figure 5.10     The structure of the  consumer process.
     5.7.2  The Readers ­ Writers Problem
     Suppose that a database is to be shared among several concurrent processes.
     Some of these processes may want only to read the database, whereas others
     may want to update (that is, to read and write) the database. We distinguish
     between these two types of processes by referring to the former as readers
     and to the latter as writers. Obviously, if two readers access the shared data
     simultaneously, no adverse effects will result. However, if a writer and some
     other process (either a reader or a writer) access the database simultaneously,
     chaos may ensue.
     To ensure that these difficulties do not arise, we require that the writers
     have exclusive access to the shared database while writing to the database. This
     synchronization problem is referred to as the readers­writers problem. Since it
     was originally stated, it has been used to test nearly every new synchronization
     primitive. The readers­writers problem has several variations, all involving
     priorities. The simplest one, referred to as the first readers­writers problem,
     requires that no reader be kept waiting unless a writer has already obtained
     permission to use the shared object. In other words, no reader should wait for
     other readers to finish simply because a writer is waiting. The second readers
     ­writers problem requires that, once a writer is ready, that writer perform its
     write as soon as possible. In other words, if a writer is waiting to access the
     object, no new readers may start reading.
     A solution to either problem may result in starvation. In the first case,
     writers may starve; in the second case, readers may starve. For this reason,
     other variants of the problem have been proposed. Next, we present a solution
     to the first readers­writers problem. See the bibliographical notes at the end
     of the chapter for references describing starvation-free solutions to the second
     readers­writers problem.
     In the solution to the first readers­writers problem, the reader processes
     share the following data structures:
                               semaphore   rw mutex       =  1;
                               semaphore   mutex      =  1;
                               int  read count   =    0;
     The semaphores mutex and rw mutex are initialized to 1; read count is
     initialized to 0. The semaphore rw mutex is common to both reader and writer



             5.7   Classic Problems of Synchronization                                 221
do  {
    wait(rw mutex);
             .  .  .
    /*       writing  is  performed                        */
             .  .  .
    signal(rw mutex);
} while (true);
Figure 5.11     The structure of a writer process.
processes. The mutex semaphore is used to ensure mutual exclusion when the
variable read count is updated. The read count variable keeps track of how
many processes are currently reading the object. The semaphore rw mutex
functions as a mutual exclusion semaphore for the writers. It is also used by
the first or last reader that enters or exits the critical section. It is not used by
readers who enter or exit while other readers are in their critical sections.
The code for a writer process is shown in Figure 5.11; the code for a
reader process is shown in Figure 5.12. Note that, if a writer is in the critical
section and n readers are waiting, then one reader is queued on rw mutex, and
n - 1 readers are queued on mutex. Also observe that, when a writer executes
signal(rw mutex), we may resume the execution of either the waiting readers
or a single waiting writer. The selection is made by the scheduler.
The readers­writers problem and its solutions have been generalized to
provide reader­writer locks on some systems. Acquiring a reader­writer lock
requires specifying the mode of the lock: either read or write access. When a
process wishes only to read shared data, it requests the reader­writer lock
in read mode. A process wishing to modify the shared data must request the
lock in write mode. Multiple processes are permitted to concurrently acquire
a reader­writer lock in read mode, but only one process may acquire the lock
for writing, as exclusive access is required for writers.
Reader­writer locks are most useful in the following situations:
do  {
    wait(mutex);
    read count++;
    if       (read count  ==  1)
             wait(rw mutex);
    signal(mutex);
             .  .  .
    /*       reading  is  performed                        */
             .  .  .
    wait(mutex);
    read count--;
    if       (read count  ==  0)
             signal(rw mutex);
    signal(mutex);
} while (true);
Figure 5.12     The structure of a reader process.



222  Chapter 5  Process Synchronization
                                         RICE
                      Figure 5.13  The situation of the dining philosophers.
     ·  In applications where it is easy to identify which processes only read shared
        data and which processes only write shared data.
     ·  In applications that have more readers than writers. This is because reader­
        writer locks generally require more overhead to establish than semaphores
        or mutual-exclusion locks. The increased concurrency of allowing multiple
        readers compensates for the overhead involved in setting up the reader­
        writer lock.
     5.7.3  The Dining-Philosophers Problem
     Consider five philosophers who spend their lives thinking and eating. The
     philosophers share a circular table surrounded by five chairs, each belonging
     to one philosopher. In the center of the table is a bowl of rice, and the table is laid
     with five single chopsticks (Figure 5.13). When a philosopher thinks, she does
     not interact with her colleagues. From time to time, a philosopher gets hungry
     and tries to pick up the two chopsticks that are closest to her (the chopsticks
     that are between her and her left and right neighbors). A philosopher may pick
     up only one chopstick at a time. Obviously, she cannot pick up a chopstick that
     is already in the hand of a neighbor. When a hungry philosopher has both her
     chopsticks at the same time, she eats without releasing the chopsticks. When
     she is finished eating, she puts down both chopsticks and starts thinking again.
        The dining-philosophers problem is considered a classic synchronization
     problem neither because of its practical importance nor because computer
     scientists dislike philosophers but because it is an example of a large class
     of concurrency-control problems. It is a simple representation of the need
     to allocate several resources among several processes in a deadlock-free and
     starvation-free manner.
        One simple solution is to represent each chopstick with a semaphore. A
     philosopher tries to grab a chopstick by executing a wait() operation on that
     semaphore. She releases her chopsticks by executing the signal() operation
     on the appropriate semaphores. Thus, the shared data are
                              semaphore chopstick[5];



                                                                    5.8     Monitors       223
                          do  {
                              wait(chopstick[i]);
                              wait(chopstick[(i+1) %          5]);
                                  .  .  .
                              /*  eat   for   awhile  */
                                  .  .  .
                              signal(chopstick[i]);
                              signal(chopstick[(i+1)          %  5]);
                                  .  .  .
                              /*  think    for  awhile    */
                                  .  .  .
                          } while (true);
                          Figure 5.14      The structure of philosopher i.
     where all the elements of chopstick are initialized to 1. The structure of
     philosopher i is shown in Figure 5.14.
        Although    this  solution      guarantees  that  no   two  neighbors  are    eating
     simultaneously, it nevertheless must be rejected because it could create a
     deadlock. Suppose that all five philosophers become hungry at the same time
     and each grabs her left chopstick. All the elements of chopstick will now be
     equal to 0. When each philosopher tries to grab her right chopstick, she will be
     delayed forever.
        Several possible remedies to the deadlock problem are replaced by:
     ·  Allow at most four philosophers to be sitting simultaneously at the table.
     ·  Allow a philosopher to pick up her chopsticks only if both chopsticks are
        available (to do this, she must pick them up in a critical section).
     ·  Use an asymmetric solution--that is, an odd-numbered philosopher picks
        up first her left chopstick and then her right chopstick, whereas an even-
        numbered philosopher picks up her right chopstick and then her left
        chopstick.
        In Section 5.8, we present a solution to the dining-philosophers problem
     that ensures freedom from deadlocks. Note, however, that any satisfactory
     solution to the dining-philosophers problem must guard against the possibility
     that one of the philosophers will starve to death. A deadlock-free solution does
     not necessarily eliminate the possibility of starvation.
5.8  Monitors
     Although  semaphores        provide   a  convenient  and    effective  mechanism      for
     process synchronization, using them incorrectly can result in timing errors
     that are difficult to detect, since these errors happen only if particular execution
     sequences take place and these sequences do not always occur.
        We have seen an example of such errors in the use of counters in our
     solution to the producer­consumer problem (Section 5.1). In that example,
     the timing problem happened only rarely, and even then the counter value



224  Chapter 5  Process Synchronization
     appeared   to  be   reasonable --off  by  only  1.    Nevertheless,   the  solution  is
     obviously not an acceptable one. It is for this reason that semaphores were
     introduced in the first place.
        Unfortunately, such timing errors can still occur when semaphores are
     used. To illustrate how, we review the semaphore solution to the critical-section
     problem. All processes share a semaphore variable mutex, which is initialized
     to 1. Each process must execute wait(mutex) before entering the critical section
     and signal(mutex) afterward. If this sequence is not observed, two processes
     may be in their critical sections simultaneously. Next, we examine the various
     difficulties that may result. Note that these difficulties will arise even if a
     single process is not well behaved. This situation may be caused by an honest
     programming error or an uncooperative programmer.
     ·  Suppose that a process interchanges the order in which the wait() and
        signal() operations on the semaphore mutex are executed, resulting in
        the following execution:
                                         signal(mutex);
                                               ...
                                           critical section
                                               ...
                                         wait(mutex);
        In this situation, several processes may be executing in their critical sections
        simultaneously, violating the mutual-exclusion requirement. This error
        may be discovered only if several processes are simultaneously active
        in  their   critical  sections.  Note  that  this  situation  may  not  always    be
        reproducible.
     ·  Suppose that a process replaces signal(mutex) with wait(mutex). That
        is, it executes
                                           wait(mutex);
                                               ...
                                           critical section
                                               ...
                                           wait(mutex);
        In this case, a deadlock will occur.
     ·  Suppose that a process omits the wait(mutex), or the signal(mutex), or
        both. In this case, either mutual exclusion is violated or a deadlock will
        occur.
     These examples illustrate that various types of errors can be generated easily
     when programmers use semaphores incorrectly to solve the critical-section
     problem. Similar problems may arise in the other synchronization models
     discussed in Section 5.7.
        To deal with such errors, researchers have developed high-level language
     constructs. In this section, we describe one fundamental high-level synchro-
     nization construct--the monitor type.



                                                                   5.8  Monitors  225
            monitor       monitor name
            {
                      /*  shared  variable         declarations         */
                      function    P1  (   .  .  .  )   {
                          ...
                      }
                      function    P2  (   .  .  .  )   {
                          ...
                      }
                          .
                          .
                          .
                      function    Pn  (   .  .  .  )   {
                          ...
                      }
                      initialization      code     (   .  .  .  )  {
                          ...
                      }
            }
                          Figure 5.15     Syntax of a monitor.
5.8.1  Monitor Usage
An abstract data type--or ADT --encapsulates data with a set of functions
to operate on that data that are independent of any specific implementation
of the ADT. A monitor type is an ADT that includes a set of programmer-
defined operations that are provided with mutual exclusion within the monitor.
The monitor type also declares the variables whose values define the state
of an instance of that type, along with the bodies of functions that operate
on those variables. The syntax of a monitor type is shown in Figure 5.15.
The representation of a monitor type cannot be used directly by the various
processes. Thus, a function defined within a monitor can access only those
variables   declared     locally  within  the   monitor    and  its   formal  parameters.
Similarly, the local variables of a monitor can be accessed by only the local
functions.
The monitor construct ensures that only one process at a time is active
within the monitor. Consequently, the programmer does not need to code
this synchronization constraint explicitly (Figure 5.16). However, the monitor
construct, as defined so far, is not sufficiently powerful for modeling some
synchronization schemes. For this purpose, we need to define additional syn-
chronization mechanisms. These mechanisms are provided by the condition
construct. A programmer who needs to write a tailor-made synchronization
scheme can define one or more variables of type condition:
                                  condition        x,  y;



226  Chapter 5  Process Synchronization
                                             entry queue
                             shared data
                                   ...
                             operations
                             initialization
                             code
                      Figure 5.16            Schematic view of a monitor.
         The only operations that can be invoked on a condition variable are wait()
     and signal(). The operation
                                             x.wait();
     means that the process invoking this operation is suspended until another
     process invokes
                                             x.signal();
         The x.signal() operation resumes exactly one suspended process. If no
     process is suspended, then the signal() operation has no effect; that is, the
     state of x is the same as if the operation had never been executed (Figure
     5.17). Contrast this operation with the signal() operation associated with
     semaphores, which always affects the state of the semaphore.
         Now suppose that, when the x.signal() operation is invoked by a process
     P, there exists a suspended process Q associated with condition x. Clearly, if the
     suspended process Q is allowed to resume its execution, the signaling process
     P must wait. Otherwise, both P and Q would be active simultaneously within
     the monitor. Note, however, that conceptually both processes can continue
     with their execution. Two possibilities exist:
     1.  Signal and wait. P either waits until Q leaves the monitor or waits for
         another condition.
     2.  Signal and continue. Q either waits until P leaves the monitor or waits
         for another condition.



                                                             5.8          Monitors       227
                                                             entry queue
                                  shared data
   queues associated with   x
           x, y conditions  y
                                            ···
                                  operations
                                  initialization
                                  code
                     Figure 5.17  Monitor with condition variables.
   There are reasonable arguments in favor of adopting either option. On
the one hand, since P was already executing in the monitor, the signal-and-
continue method seems more reasonable. On the other, if we allow thread P
to continue, then by the time Q is resumed, the logical condition for which Q
was waiting may no longer hold. A compromise between these two choices
was adopted in the language Concurrent Pascal. When thread P executes the
signal operation, it immediately leaves the monitor. Hence, Q is immediately
resumed.
   Many programming languages have incorporated the idea of the monitor
as described in this section, including Java and C# (pronounced "C-sharp").
Other languages--such as Erlang--provide some type of concurrency support
using a similar mechanism.
5.8.2  Dining-Philosophers Solution Using Monitors
Next, we illustrate monitor concepts by presenting a deadlock-free solution to
the dining-philosophers problem. This solution imposes the restriction that a
philosopher may pick up her chopsticks only if both of them are available. To
code this solution, we need to distinguish among three states in which we may
find a philosopher. For this purpose, we introduce the following data structure:
                enum {THINKING, HUNGRY, EATING} state[5];
Philosopher  i  can  set    the   variable  state[i]      =  EATING       only  if  her  two
neighbors are not eating: (state[(i+4)      %     5]  !=  EATING) and (state[(i+1)
%  5]  !=  EATING).



228  Chapter  5  Process Synchronization
                 monitor DiningPhilosophers
                 {
                    enum {THINKING, HUNGRY, EATING}                       state[5];
                    condition      self[5];
                    void pickup(int i) {
                          state[i] = HUNGRY;
                          test(i);
                          if   (state[i]       !=    EATING)
                              self[i].wait();
                    }
                    void putdown(int i) {
                          state[i] = THINKING;
                          test((i    +     4)  %     5);
                          test((i    +     1)  %     5);
                    }
                    void test(int i) {
                          if   ((state[(i         +  4)      %  5]  !=    EATING)  &&
                             (state[i]     ==     HUNGRY)           &&
                             (state[(i + 1) % 5] != EATING)) {
                               state[i]        =  EATING;
                               self[i].signal();
                          }
                    }
                    initialization code() {
                          for  (int     i  =   0;    i    <     5;  i++)
                              state[i]     =   THINKING;
                       }
                 }
                 Figure 5.18   A monitor solution to the dining-philosopher problem.
     We also need to declare
                                     condition self[5];
     This allows philosopher i to delay herself when she is hungry but is unable to
     obtain the chopsticks she needs.
     We are now in a position to describe our solution to the dining-philosophers
     problem. The distribution of the chopsticks is controlled by the monitor Din-
     ingPhilosophers, whose definition is shown in Figure 5.18. Each philosopher,
     before starting to eat, must invoke the operation pickup(). This act may result
     in the suspension of the philosopher process. After the successful completion of
     the operation, the philosopher may eat. Following this, the philosopher invokes
     the putdown() operation. Thus, philosopher i must invoke the operations
     pickup() and putdown() in the following sequence:



                                                             5.8  Monitors           229
                      DiningPhilosophers.pickup(i);
                                          ...
                                          eat
                                          ...
                      DiningPhilosophers.putdown(i);
     It is easy to show that this solution ensures that no two neighbors are eating
simultaneously and that no deadlocks will occur. We note, however, that it is
possible for a philosopher to starve to death. We do not present a solution to
this problem but rather leave it as an exercise for you.
5.8.3  Implementing a Monitor Using Semaphores
We now consider a possible implementation of the monitor mechanism using
semaphores. For each monitor, a semaphore mutex (initialized to 1) is provided.
A process must execute wait(mutex) before entering the monitor and must
execute signal(mutex) after leaving the monitor.
     Since a signaling process must wait until the resumed process either leaves
or waits, an additional semaphore, next, is introduced, initialized to 0. The
signaling processes can use next to suspend themselves. An integer variable
next count is also provided to count the number of processes suspended on
next. Thus, each external function F is replaced by
                              wait(mutex);
                                    ...
                              body of F
                                    ...
                              if (next count > 0)
                              signal(next);
                              else
                              signal(mutex);
Mutual exclusion within a monitor is ensured.
     We can now describe how condition variables are implemented as well.
For  each  condition  x,  we  introduce   a    semaphore  x  sem  and  an   integer
variable x count, both initialized to 0. The operation x.wait() can now be
implemented as
                              x count++;
                              if (next count > 0)
                              signal(next);
                              else
                              signal(mutex);
                              wait(x sem);
                              x count--;
     The operation x.signal() can be implemented as



230  Chapter 5     Process Synchronization
                              if (x count > 0) {
                                        next count++;
                                        signal(x sem);
                                        wait(next);
                                        next count--;
                              }
         This implementation is applicable to the definitions of monitors given by
     both Hoare and Brinch-Hansen (see the bibliographical notes at the end of
     the chapter). In some cases, however, the generality of the implementation is
     unnecessary, and a significant improvement in efficiency is possible. We leave
     this problem to you in Exercise 5.30.
     5.8.4   Resuming Processes within a Monitor
     We turn now to the subject of process-resumption order within a monitor. If
     several processes are suspended on condition x, and an x.signal() operation
     is  executed  by  some   process,  then  how  do    we  determine  which  of         the
     suspended processes should be resumed next? One simple solution is to use a
     first-come, first-served (FCFS) ordering, so that the process that has been waiting
     the longest is resumed first. In many circumstances, however, such a simple
     scheduling scheme is not adequate. For this purpose, the conditional-wait
     construct can be used. This construct has the form
                                        x.wait(c);
     where c is an integer expression that is evaluated when the wait() operation
     is executed. The value of c, which is called a priority number, is then stored
     with the name of the process that is suspended. When x.signal() is executed,
     the process with the smallest priority number is resumed next.
         To illustrate this new mechanism, consider the ResourceAllocator mon-
     itor shown in Figure 5.19, which controls the allocation of a single resource
     among competing processes. Each process, when requesting an allocation of
     this resource, specifies the maximum time it plans to use the resource. The mon-
     itor allocates the resource to the process that has the shortest time-allocation
     request. A process that needs to access the resource in question must observe
     the following sequence:
                              R.acquire(t);
                                        ...
                                 access the resource;
                                        ...
                              R.release();
     where R is an instance of type ResourceAllocator.
         Unfortunately, the monitor concept cannot guarantee that the preceding
     access sequence will be observed. In particular, the following problems can
     occur:
     ·   A process might access a resource without first gaining access permission
         to the resource.



                                                            5.8            Monitors        231
                    monitor ResourceAllocator
                    {
                           boolean busy;
                           condition x;
                           void  acquire(int   time)    {
                              if (busy)
                                 x.wait(time);
                              busy = true;
                           }
                           void release() {
                              busy = false;
                              x.signal();
                           }
                           initialization code()  {
                              busy = false;
                           }
                    }
                   Figure 5.19   A monitor to allocate a single resource.
·   A process might never release a resource once it has been granted access
    to the resource.
·   A process might attempt to release a resource that it never requested.
·   A process might request the same resource twice (without first releasing
    the resource).
    The same difficulties are encountered with the use of semaphores, and
these difficulties are similar in nature to those that encouraged us to develop
the monitor constructs in the first place. Previously, we had to worry about
the correct use of semaphores. Now, we have to worry about the correct use of
higher-level programmer-defined operations, with which the compiler can no
longer assist us.
    One possible solution to the current problem is to include the resource-
access operations within the ResourceAllocator monitor. However, using
this solution will mean that scheduling is done according to the built-in
monitor-scheduling algorithm rather than the one we have coded.
    To ensure that the processes observe the appropriate sequences, we must
inspect all the programs that make use of the ResourceAllocator monitor
and its managed resource. We must check two conditions to establish the
correctness of this system. First, user processes must always make their calls
on  the  monitor    in  a  correct  sequence.  Second,  we  must           be  sure  that  an
uncooperative process does not simply ignore the mutual-exclusion gateway
provided by the monitor and try to access the shared resource directly, without
using the access protocols. Only if these two conditions can be ensured can we
guarantee that no time-dependent errors will occur and that the scheduling
algorithm will not be defeated.



232  Chapter 5  Process Synchronization
                                    JAVA MONITORS
     Java provides a monitor-like concurrency mechanism for thread synchro-
     nization. Every object in Java has associated with it a single lock. When a
     method is declared to be synchronized, calling the method requires owning
     the lock for the object. We declare a synchronized method by placing the
     synchronized keyword in the method definition. The following defines
     safeMethod() as synchronized, for example:
                public       class     SimpleClass  {
                   .  .   .
                   public synchronized void safeMethod() {
                      .   .  .
                      /*     Implementation    of   safeMethod()   */
                      .   .  .
                   }
                }
     Next, we create an object instance of SimpleClass, such as the following:
                SimpleClass        sc  =  new  SimpleClass();
     Invoking sc.safeMethod() method requires owning the lock on the object
     instance sc. If the lock is already owned by another thread, the thread calling
     the synchronized method blocks and is placed in the entry set for the object's
     lock. The entry set represents the set of threads waiting for the lock to become
     available. If the lock is available when a synchronized method is called,
     the calling thread becomes the owner of the object's lock and can enter the
     method. The lock is released when the thread exits the method. A thread from
     the entry set is then selected as the new owner of the lock.
     Java also provides wait() and notify() methods, which are similar in
     function to the wait() and signal() statements for a monitor. The Java
     API provides support for semaphores, condition variables, and mutex locks
     (among other concurrency mechanisms) in the java.util.concurrent
     package.
     Although this inspection may be possible for a small, static system, it is
     not reasonable for a large system or a dynamic system. This access-control
     problem can be solved only through the use of the additional mechanisms that
     are described in Chapter 14.
5.9  Synchronization Examples
     We next describe the synchronization mechanisms provided by the Windows,
     Linux, and Solaris operating systems, as well as the Pthreads API. We have
     chosen these three operating systems because they provide good examples of
     different approaches to synchronizing the kernel, and we have included the



                                       5.9  Synchronization Examples              233
Pthreads API because it is widely used for thread creation and synchronization
by developers on UNIX and Linux systems. As you will see in this section, the
synchronization methods available in these differing systems vary in subtle
and significant ways.
5.9.1  Synchronization in Windows
The Windows operating system is a multithreaded kernel that provides support
for real-time applications and multiple processors. When the Windows kernel
accesses a global resource on a single-processor system, it temporarily masks
interrupts for all interrupt handlers that may also access the global resource.
On a multiprocessor system, Windows protects access to global resources
using spinlocks, although the kernel uses spinlocks only to protect short code
segments. Furthermore, for reasons of efficiency, the kernel ensures that a
thread will never be preempted while holding a spinlock.
For thread synchronization outside the kernel, Windows provides dis-
patcher objects. Using a dispatcher object, threads synchronize according to
several different mechanisms, including mutex locks, semaphores, events, and
timers. The system protects shared data by requiring a thread to gain ownership
of a mutex to access the data and to release ownership when it is finished.
Semaphores behave as described in Section 5.6. Events are similar to condition
variables; that is, they may notify a waiting thread when a desired condition
occurs. Finally, timers are used to notify one (or more than one) thread that a
specified amount of time has expired.
Dispatcher objects may be in either a signaled state or a nonsignaled state.
An object in a signaled state is available, and a thread will not block when
acquiring the object. An object in a nonsignaled state is not available, and a
thread will block when attempting to acquire the object. We illustrate the state
transitions of a mutex lock dispatcher object in Figure 5.20.
A relationship exists between the state of a dispatcher object and the state
of a thread. When a thread blocks on a nonsignaled dispatcher object, its state
changes from ready to waiting, and the thread is placed in a waiting queue
for that object. When the state for the dispatcher object moves to signaled, the
kernel checks whether any threads are waiting on the object. If so, the kernel
moves one thread--or possibly more --from the waiting state to the ready
state, where they can resume executing. The number of threads the kernel
selects from the waiting queue depends on the type of dispatcher object for
which it is waiting. The kernel will select only one thread from the waiting
queue for a mutex, since a mutex object may be "owned" by only a single
                       owner thread releases mutex lock
       nonsignaled                                       signaled
                       thread acquires mutex lock
                       Figure 5.20  Mutex dispatcher object.



234  Chapter 5    Process Synchronization
     thread. For an event object, the kernel will select all threads that are waiting
     for the event.
     We can use a mutex lock as an illustration of dispatcher objects and
     thread states. If a thread tries to acquire a mutex dispatcher object that is in a
     nonsignaled state, that thread will be suspended and placed in a waiting queue
     for the mutex object. When the mutex moves to the signaled state (because
     another thread has released the lock on the mutex), the thread waiting at the
     front of the queue will be moved from the waiting state to the ready state and
     will acquire the mutex lock.
     A critical-section object is a user-mode mutex that can often be acquired
     and released without kernel intervention. On a multiprocessor system, a
     critical-section object first uses a spinlock while waiting for the other thread to
     release the object. If it spins too long, the acquiring thread will then allocate a
     kernel mutex and yield its CPU. Critical-section objects are particularly efficient
     because the kernel mutex is allocated only when there is contention for the
     object. In practice, there is very little contention, so the savings are significant.
     We provide a programming project at the end of this chapter that uses
     mutex locks and semaphores in the Windows API.
     5.9.2  Synchronization in Linux
     Prior to Version 2.6, Linux was a nonpreemptive kernel, meaning that a process
     running in kernel mode could not be preempted --even if a higher-priority
     process became available to run. Now, however, the Linux kernel is fully
     preemptive, so a task can be preempted when it is running in the kernel.
     Linux provides several different mechanisms for synchronization in the
     kernel. As most computer architectures provide instructions for atomic ver-
     sions of simple math operations, the simplest synchronization technique within
     the Linux kernel is an atomic integer, which is represented using the opaque
     data type atomic t. As the name implies, all math operations using atomic
     integers are performed without interruption. The following code illustrates
     declaring an atomic integer counter and then performing various atomic
     operations:
            atomic t counter;
            int   value;
            atomic set(&counter,5); /* counter = 5 */
            atomic add(10,  &counter);     /*   counter     =  counter      +     10  */
            atomic sub(4,   &counter);     /*  counter   =     counter  -      4  */
            atomic inc(&counter); /* counter = counter + 1 */
            value    =  atomic read(&counter);  /*   value     =  12    */
     Atomic integers are particularly efficient in situations where an integer variable
     --such as a counter--needs to be updated, since atomic operations do not
     require the overhead of locking mechanisms. However, their usage is limited
     to these sorts of scenarios. In situations where there are several variables
     contributing to a possible race condition, more sophisticated locking tools
     must be used.
     Mutex locks are available in Linux for protecting critical sections within the
     kernel. Here, a task must invoke the mutex lock() function prior to entering



                                         5.9  Synchronization Examples              235
a critical section and the mutex unlock() function after exiting the critical
section. If the mutex lock is unavailable, a task calling mutex lock() is put into
a sleep state and is awakened when the lock's owner invokes mutex unlock().
Linux also provides spinlocks and semaphores (as well as reader­writer
versions of these two locks) for locking in the kernel. On SMP machines, the
fundamental locking mechanism is a spinlock, and the kernel is designed
so that the spinlock is held only for short durations. On single-processor
machines, such as embedded systems with only a single processing core,
spinlocks are inappropriate for use and are replaced by enabling and disabling
kernel preemption. That is, on single-processor systems, rather than holding a
spinlock, the kernel disables kernel preemption; and rather than releasing the
spinlock, it enables kernel preemption. This is summarized below:
                     single processor             multiple processors
                 Disable kernel preemption.       Acquire spin lock.
                 Enable kernel preemption.        Release spin lock.
Linux uses an interesting approach to disable and enable kernel preemp-
tion. It provides two simple system calls--preempt disable() and pre-
empt enable()--for disabling and enabling kernel preemption. The kernel
is not preemptible, however, if a task running in the kernel is holding a lock.
To enforce this rule, each task in the system has a thread-info structure
containing a counter, preempt count, to indicate the number of locks being
held by the task. When a lock is acquired, preempt count is incremented. It
is decremented when a lock is released. If the value of preempt count for
the task currently running in the kernel is greater than 0, it is not safe to
preempt the kernel, as this task currently holds a lock. If the count is 0, the
kernel can safely be interrupted (assuming there are no outstanding calls to
preempt disable()).
Spinlocks--along with enabling and disabling kernel preemption--are
used in the kernel only when a lock (or disabling kernel preemption) is held
for a short duration. When a lock must be held for a longer period, semaphores
or mutex locks are appropriate for use.
5.9.3  Synchronization in Solaris
To control access to critical sections, Solaris provides adaptive mutex locks,
condition variables, semaphores, reader­writer locks, and turnstiles. Solaris
implements semaphores and condition variables essentially as they are pre-
sented in Sections 5.6 and 5.7 In this section, we describe adaptive mutex locks,
reader­writer locks, and turnstiles.
An     adaptive  mutex  protects      access  to  every  critical  data  item.  On  a
multiprocessor system, an adaptive mutex starts as a standard semaphore
implemented as a spinlock. If the data are locked and therefore already in use,
the adaptive mutex does one of two things. If the lock is held by a thread that
is currently running on another CPU, the thread spins while waiting for the
lock to become available, because the thread holding the lock is likely to finish
soon. If the thread holding the lock is not currently in run state, the thread



236  Chapter 5  Process Synchronization
     blocks, going to sleep until it is awakened by the release of the lock. It is put
     to sleep so that it will not spin while waiting, since the lock will not be freed
     very soon. A lock held by a sleeping thread is likely to be in this category. On
     a single-processor system, the thread holding the lock is never running if the
     lock is being tested by another thread, because only one thread can run at a
     time. Therefore, on this type of system, threads always sleep rather than spin
     if they encounter a lock.
          Solaris uses the adaptive-mutex method to protect only data that are
     accessed by short code segments. That is, a mutex is used if a lock will be
     held for less than a few hundred instructions. If the code segment is longer
     than that, the spin-waiting method is exceedingly inefficient. For these longer
     code segments, condition variables and semaphores are used. If the desired
     lock is already held, the thread issues a wait and sleeps. When a thread frees
     the lock, it issues a signal to the next sleeping thread in the queue. The extra
     cost of putting a thread to sleep and waking it, and of the associated context
     switches, is less than the cost of wasting several hundred instructions waiting
     in a spinlock.
          Reader­writer locks are used to protect data that are accessed frequently
     but  are  usually  accessed  in  a  read-only  manner.  In  these     circumstances,
     reader­writer locks are more efficient than semaphores, because multiple
     threads can read data concurrently, whereas semaphores always serialize access
     to the data. Reader­writer locks are relatively expensive to implement, so again
     they are used only on long sections of code.
          Solaris uses turnstiles to order the list of threads waiting to acquire either
     an adaptive mutex or a reader­writer lock. A turnstile is a queue structure
     containing threads blocked on a lock. For example, if one thread currently
     owns the lock for a synchronized object, all other threads trying to acquire the
     lock will block and enter the turnstile for that lock. When the lock is released,
     the kernel selects a thread from the turnstile as the next owner of the lock.
     Each synchronized object with at least one thread blocked on the object's lock
     requires a separate turnstile. However, rather than associating a turnstile with
     each synchronized object, Solaris gives each kernel thread its own turnstile.
     Because a thread can be blocked only on one object at a time, this is more
     efficient than having a turnstile for each object.
          The turnstile for the first thread to block on a synchronized object becomes
     the turnstile for the object itself. Threads subsequently blocking on the lock will
     be added to this turnstile. When the initial thread ultimately releases the lock,
     it gains a new turnstile from a list of free turnstiles maintained by the kernel. To
     prevent a priority inversion, turnstiles are organized according to a priority-
     inheritance protocol. This means that if a lower-priority thread currently holds
     a lock on which a higher-priority thread is blocked, the thread with the lower
     priority will temporarily inherit the priority of the higher-priority thread. Upon
     releasing the lock, the thread will revert to its original priority.
          Note that the locking mechanisms used by the kernel are implemented
     for user-level threads as well, so the same types of locks are available inside
     and outside the kernel. A crucial implementation difference is the priority-
     inheritance protocol. Kernel-locking routines adhere to the kernel priority-
     inheritance methods used by the scheduler, as described in Section 5.6.4.
     User-level thread-locking mechanisms do not provide this functionality.



                                           5.9   Synchronization Examples          237
To optimize Solaris performance, developers have refined and fine-tuned
the locking methods. Because locks are used frequently and typically are used
for crucial kernel functions, tuning their implementation and use can produce
great performance gains.
5.9.4    Pthreads Synchronization
Although the locking mechanisms used in Solaris are available to user-level
threads  as  well  as  kernel    threads,  basically   the  synchronization  methods
discussed thus far pertain to synchronization within the kernel. In contrast,
the Pthreads API is available for programmers at the user level and is not part
of any particular kernel. This API provides mutex locks, condition variables,
and read ­write locks for thread synchronization.
Mutex locks represent the fundamental synchronization technique used
with Pthreads. A mutex lock is used to protect critical sections of code --that
is, a thread acquires the lock before entering a critical section and releases it
upon exiting the critical section. Pthreads uses the pthread mutex t data type
for mutex locks. A mutex is created with the pthread mutex init() function.
The first parameter is a pointer to the mutex. By passing NULL as a second
parameter, we initialize the mutex to its default attributes. This is illustrated
below:
             #include <pthread.h>
             pthread      mutex  t  mutex;
             /*    create  the   mutex     lock  */
             pthread mutex init(&mutex,NULL);
The      mutex     is  acquired  and  released   with  the  pthread  mutex   lock()
and pthread mutex unlock() functions. If the mutex lock is unavailable
when pthread mutex lock() is invoked, the calling thread is blocked until
the owner invokes pthread mutex unlock(). The following code illustrates
protecting a critical section with mutex locks:
                          /*  acquire      the  mutex  lock  */
                          pthread mutex lock(&mutex);
                          /*  critical     section     */
                          /*  release      the  mutex  lock  */
                          pthread mutex unlock(&mutex);
All mutex functions return a value of 0 with correct operation; if an error
occurs, these functions return a nonzero error code. Condition variables and
read ­write locks behave similarly to the way they are described in Sections 5.8
and 5.7.2, respectively.
Many systems that implement Pthreads also provide semaphores, although
semaphores are not part of the Pthreads standard and instead belong to the
POSIX SEM extension. POSIX specifies two types of semaphores --named and



238   Chapter 5    Process Synchronization
      unnamed. The fundamental distinction between the two is that a named
      semaphore    has  an  actual  name     in  the  file  system  and   can  be  shared  by
      multiple unrelated processes. Unnamed semaphores can be used only by
      threads belonging to the same process. In this section, we describe unnamed
      semaphores.
          The  code  below  illustrates   the    sem  init()    function  for  creating    and
      initializing an unnamed semaphore:
               #include <semaphore.h>
               sem t sem;
               /*  Create   the  semaphore       and  initialize    it    to   1   */
               sem init(&sem,       0,  1);
      The sem init() function is passed three parameters:
      1.  A pointer to the semaphore
      2.  A flag indicating the level of sharing
      3.  The semaphore's initial value
      In this example, by passing the flag 0, we are indicating that this semaphore can
      be shared only by threads belonging to the process that created the semaphore.
      A nonzero value would allow other processes to access the semaphore as well.
      In addition, we initialize the semaphore to the value 1.
          In Section 5.6, we described the classical wait() and signal() semaphore
      operations. Pthreads names these operations sem wait() and sem post(),
      respectively. The following code sample illustrates protecting a critical section
      using the semaphore created above:
                            /* acquire the semaphore */
                            sem wait(&sem);
                            /*      critical     section    */
                            /* release the semaphore */
                            sem post(&sem);
      Just like mutex locks, all semaphore functions return 0 when successful, and
      nonzero when an error condition occurs.
          There are other extensions to the Pthreads API -- including spinlocks --
      but it is important to note that not all extensions are considered portable from
      one implementation to another. We provide several programming problems
      and projects at the end of this chapter that use Pthreads mutex locks and
      condition variables as well as POSIX semaphores.
5.10  Alternative Approaches
      With the emergence of multicore systems has come increased pressure to
      develop multithreaded applications that take advantage of multiple processing



                                                 5.10   Alternative Approaches          239
cores. However, multithreaded applications present an increased risk of race
conditions     and    deadlocks.   Traditionally,  techniques     such   as  mutex  locks,
semaphores, and monitors have been used to address these issues, but as the
number of processing cores increases, it becomes increasingly difficult to design
multithreaded applications that are free from race conditions and deadlocks.
     In this section, we explore various features provided in both program-
ming languages and hardware that support designing thread-safe concurrent
applications.
5.10.1      Transactional Memory
Quite often in computer science, ideas from one area of study can be used
to   solve  problems   in   other  areas.  The     concept  of    transactional  memory
originated in database theory, for example, yet it provides a strategy for process
synchronization. A memory transaction is a sequence of memory read ­write
operations that are atomic. If all operations in a transaction are completed, the
memory transaction is committed. Otherwise, the operations must be aborted
and rolled back. The benefits of transactional memory can be obtained through
features added to a programming language.
     Consider an example. Suppose we have a function update() that modifies
shared data. Traditionally, this function would be written using mutex locks
(or semaphores) such as the following:
            void    update  ()
            {
               acquire();
               /*   modify  shared     data   */
               release();
            }
However,       using  synchronization      mechanisms       such  as  mutex      locks  and
semaphores involves many potential problems, including deadlock. Addition-
ally, as the number of threads increases, traditional locking scales less well,
because the level of contention among threads for lock ownership becomes
very high.
     As an alternative to traditional locking methods, new features that take
advantage of transactional memory can be added to a programming language.
In our example, suppose we add the construct atomic{S}, which ensures
that the operations in S execute as a transaction. This allows us to rewrite the
update() function as follows:
            void    update  ()
            {
               atomic  {
                    /* modify     shared   data    */
               }
            }
     The    advantage  of   using  such    a  mechanism     rather    than   locks  is  that
the  transactional    memory      system -- not    the  developer -- is  responsible    for



240  Chapter 5  Process Synchronization
     guaranteeing atomicity. Additionally, because no locks are involved, deadlock
     is not possible. Furthermore, a transactional memory system can identify which
     statements in atomic blocks can be executed concurrently, such as concurrent
     read access to a shared variable. It is, of course, possible for a programmer
     to identify these situations and use reader­writer locks, but the task becomes
     increasingly difficult as the number of threads within an application grows.
     Transactional memory can be implemented in either software or hardware.
     Software transactional memory (STM), as the name suggests, implements
     transactional memory exclusively in software--no special hardware is needed.
     STM works by inserting instrumentation code inside transaction blocks. The
     code is inserted by a compiler and manages each transaction by examining
     where statements may run concurrently and where specific low-level locking
     is required. Hardware transactional memory (HTM) uses hardware cache
     hierarchies and cache coherency protocols to manage and resolve conflicts
     involving shared data residing in separate processors' caches. HTM requires no
     special code instrumentation and thus has less overhead than STM. However,
     HTM does require that existing cache hierarchies and cache coherency protocols
     be modified to support transactional memory.
     Transactional memory has existed for several years without widespread
     implementation. However, the growth of multicore systems and the associ-
     ated emphasis on concurrent and parallel programming have prompted a
     significant amount of research in this area on the part of both academics and
     commercial software and hardware vendors.
     5.10.2     OpenMP
     In Section 4.5.2, we provided an overview of OpenMP and its support of parallel
     programming in a shared-memory environment. Recall that OpenMP includes
     a set of compiler directives and an API. Any code following the compiler
     directive  #pragma    omp  parallel   is   identified  as  a  parallel  region  and  is
     performed by a number of threads equal to the number of processing cores
     in the system. The advantage of OpenMP (and similar tools) is that thread
     creation and management are handled by the OpenMP library and are not the
     responsibility of application developers.
     Along with its #pragma      omp  parallel compiler directive, OpenMP pro-
     vides the compiler directive #pragma      omp  critical, which specifies the code
     region following the directive as a critical section in which only one thread may
     be active at a time. In this way, OpenMP provides support for ensuring that
     threads do not generate race conditions.
     As an example of the use of the critical-section compiler directive, first
     assume that the shared variable counter can be modified in the update()
     function as follows:
                           void  update(int         value)
                           {
                                counter    +=   value;
                           }
     If the update() function can be part of--or invoked from--a parallel region,
     a race condition is possible on the variable counter.



                                           5.10      Alternative Approaches          241
The critical-section compiler directive can be used to remedy this race
condition and is coded as follows:
                          void  update(int     value)
                          {
                             #pragma  omp      critical
                             {
                                counter    +=  value;
                             }
                          }
The critical-section compiler directive behaves much like a binary semaphore
or mutex lock, ensuring that only one thread at a time is active in the critical
section. If a thread attempts to enter a critical section when another thread is
currently active in that section (that is, owns the section), the calling thread is
blocked until the owner thread exits. If multiple critical sections must be used,
each critical section can be assigned a separate name, and a rule can specify
that no more than one thread may be active in a critical section of the same
name simultaneously.
An advantage of using the critical-section compiler directive in OpenMP
is that it is generally considered easier to use than standard mutex locks.
However, a disadvantage is that application developers must still identify
possible race conditions and adequately protect shared data using the compiler
directive. Additionally, because the critical-section compiler directive behaves
much like a mutex lock, deadlock is still possible when two or more critical
sections are identified.
5.10.3  Functional Programming Languages
Most well-known programming languages--such as C, C++, Java, and C#--
are known as imperative (or procedural) languages. Imperative languages are
used for implementing algorithms that are state-based. In these languages, the
flow of the algorithm is crucial to its correct operation, and state is represented
with variables and other data structures. Of course, program state is mutable,
as variables may be assigned different values over time.
With the current emphasis on concurrent and parallel programming for
multicore systems, there has been greater focus on functional programming
languages,  which  follow    a  programming      paradigm  much  different   from
that offered by imperative languages. The fundamental difference between
imperative  and  functional     languages  is  that  functional  languages   do      not
maintain state. That is, once a variable has been defined and assigned a value, its
value is immutable --it cannot change. Because functional languages disallow
mutable state, they need not be concerned with issues such as race conditions
and deadlocks. Essentially, most of the problems addressed in this chapter are
nonexistent in functional languages.
Several functional languages are presently in use, and we briefly mention
two of them here: Erlang and Scala. The Erlang language has gained significant
attention because of its support for concurrency and the ease with which it
can be used to develop applications that run on parallel systems. Scala is a
functional language that is also object-oriented. In fact, much of the syntax of
Scala is similar to the popular object-oriented languages Java and C#. Readers



242   Chapter 5    Process Synchronization
      interested in Erlang and Scala, and in further details about functional languages
      in general, are encouraged to consult the bibliography at the end of this chapter
      for additional references.
5.11  Summary
      Given a collection of cooperating sequential processes that share data, mutual
      exclusion must be provided to ensure that a critical section of code is used by
      only one process or thread at a time. Typically, computer hardware provides
      several operations that ensure mutual exclusion. However, such hardware-
      based solutions are too complicated for most developers to use. Mutex locks
      and semaphores overcome this obstacle. Both tools can be used to solve various
      synchronization problems and can be implemented efficiently, especially if
      hardware support for atomic operations is available.
           Various synchronization problems (such as the bounded-buffer problem,
      the readers­writers problem, and the dining-philosophers problem) are impor-
      tant mainly because they are examples of a large class of concurrency-control
      problems. These problems are used to test nearly every newly proposed
      synchronization scheme.
           The operating system must provide the means to guard against timing
      errors, and several language constructs have been proposed to deal with
      these problems. Monitors provide a synchronization mechanism for sharing
      abstract data types. A condition variable provides a method by which a monitor
      function can block its execution until it is signaled to continue.
           Operating systems also provide support for synchronization. For example,
      Windows, Linux, and Solaris provide mechanisms such as semaphores, mutex
      locks, spinlocks, and condition variables to control access to shared data. The
      Pthreads API provides support for mutex locks and semaphores, as well as
      condition variables.
           Several alternative approaches focus on synchronization for multicore
      systems. One approach uses transactional memory, which may address syn-
      chronization issues using either software or hardware techniques. Another
      approach uses the compiler extensions offered by OpenMP. Finally, func-
      tional programming languages address synchronization issues by disallowing
      mutability.
Practice   Exercises
      5.1  In Section 5.4, we mentioned that disabling interrupts frequently can
           affect the system's clock. Explain why this can occur and how such
           effects can be minimized.
      5.2  Explain why Windows, Linux, and Solaris implement multiple locking
           mechanisms. Describe the circumstances under which they use spin-
           locks, mutex locks, semaphores, adaptive mutex locks, and condition
           variables. In each case, explain why the mechanism is needed.



                                                                     Exercises         243
5.3        What is the meaning of the term busy waiting? What other kinds of
           waiting are there in an operating system? Can busy waiting be avoided
           altogether? Explain your answer.
5.4        Explain why spinlocks are not appropriate for single-processor systems
           yet are often used in multiprocessor systems.
5.5        Show that, if the wait() and signal() semaphore operations are not
           executed atomically, then mutual exclusion may be violated.
5.6        Illustrate how a binary semaphore can be used to implement mutual
           exclusion among n processes.
Exercises
5.7        Race conditions are possible in many computer systems. Consider a
           banking system that maintains an account balance with two functions:
           deposit(amount) and withdraw(amount). These two functions are
           passed the amount that is to be deposited or withdrawn from the bank
           account balance. Assume that a husband and wife share a bank account.
           Concurrently, the husband calls the withdraw() function and the wife
           calls deposit(). Describe how a race condition is possible and what
           might be done to prevent the race condition from occurring.
5.8        The first known correct software solution to the critical-section problem
           for two processes was developed by Dekker. The two processes, P0 and
           P1, share the following variables:
                       boolean flag[2];        /*  initially     false     */
                       int  turn;
           The structure of process Pi (i == 0 or 1) is shown in Figure 5.21. The
           other process is Pj (j == 1 or 0). Prove that the algorithm satisfies all
           three requirements for the critical-section problem.
5.9        The first known correct software solution to the critical-section problem
           for n processes with a lower bound on waiting of n - 1 turns was
           presented by Eisenberg and McGuire. The processes share the following
           variables:
                       enum pstate {idle,          want   in,    in  cs};
                       pstate flag[n];
                       int  turn;
           All the elements of flag are initially idle. The initial value of turn is
           immaterial (between 0 and n-1). The structure of process Pi is shown in
           Figure 5.22. Prove that the algorithm satisfies all three requirements for
           the critical-section problem.
5.10       Explain why implementing synchronization primitives by disabling
           interrupts is not appropriate in a single-processor system if the syn-
           chronization primitives are to be used in user-level programs.



244  Chapter 5  Process Synchronization
                       do      {
                             flag[i]     =   true;
                             while (flag[j]) {
                                  if (turn == j) {
                                      flag[i]     =  false;
                                      while  (turn    ==    j)
                                         ;   /*  do  nothing    */
                                      flag[i]     =  true;
                                  }
                             }
                                  /*  critical       section    */
                             turn     =  j;
                             flag[i] = false;
                                  /*  remainder      section    */
                       }   while      (true);
                Figure 5.21     The structure of process Pi in Dekker's  algorithm.
     5.11  Explain why interrupts are not appropriate for implementing synchro-
           nization primitives in multiprocessor systems.
     5.12  The Linux kernel has a policy that a process cannot hold a spinlock while
           attempting to acquire a semaphore. Explain why this policy is in place.
     5.13  Describe two kernel data structures in which race conditions are possible.
           Be sure to include a description of how a race condition can occur.
     5.14  Describe how the compare and swap() instruction can be used to pro-
           vide mutual exclusion that satisfies the bounded-waiting requirement.
     5.15  Consider how to implement a mutex lock using an atomic hardware
           instruction. Assume that the following structure defining the mutex
           lock is available:
                                         typedef struct {
                                                 int available;
                                         } lock;
           (available     ==      0) indicates that the lock is available, and a value of 1
           indicates that the lock is unavailable. Using this struct, illustrate how
           the following functions can be implemented using the test and set()
           and compare and swap() instructions:
           ·    void  acquire(lock           *mutex)
           ·    void  release(lock           *mutex)
           Be sure to include any initialization that may be necessary.



                                                                                   Exercises   245
      do  {
          while (true) {
             flag[i] = want in;
             j = turn;
             while (j != i) {
                   if  (flag[j]          !=  idle)      {
                       j   =  turn;
                   else
                       j   =  (j     +  1)   %  n;
             }
             flag[i] = in cs;
             j = 0;
             while (          (j  <     n)  &&  (j   ==     i     ||  flag[j]  !=  in cs))
                   j++;
             if     (  (j     >=  n)    &&   (turn      ==     i  ||  flag[turn]   ==  idle))
                   break;
          }
             /*     critical         section     */
          j  =  (turn      +  1)     %   n;
          while       (flag[j]       ==     idle)
             j   =     (j  +  1)     %  n;
          turn     =   j;
          flag[i] = idle;
             /*     remainder           section     */
      }   while    (true);
      Figure 5.22      The structure of process Pi in Eisenberg and McGuire's algorithm.
5.16     The implementation of mutex locks provided in Section 5.5 suffers from
         busy waiting. Describe what changes would be necessary so that a
         process waiting to acquire a mutex lock would be blocked and placed
         into a waiting queue until the lock became available.
5.17     Assume that a system has multiple processing cores. For each of the
         following scenarios, describe which is a better locking mechanism--a
         spinlock or a mutex lock where waiting processes sleep while waiting
         for the lock to become available:
          ·  The lock is to be held for a short duration.
          ·  The lock is to be held for a long duration.
          ·  A thread may be put to sleep while holding the lock.



246  Chapter  5  Process Synchronization
              #define MAX PROCESSES 255
              int number of processes = 0;
              /*  the    implementation       of  fork()  calls  this      function   */
              int allocate process() {
              int new pid;
                  if   (number  of    processes   ==  MAX PROCESSES)
                       return   -1;
                  else   {
                       /*   allocate  necessary       process    resources     */
                       ++number of processes;
                       return   new   pid;
                  }
              }
              /*  the    implementation       of  exit()  calls  this      function   */
              void release process() {
                     /* release process resources */
                     --number of processes;
              }
                         Figure 5.23  Allocating and releasing processes.
     5.18  Assume that a context switch takes T time. Suggest an upper bound
           (in terms of T) for holding a spinlock. If the spinlock is held for any
           longer, a mutex lock (where waiting threads are put to sleep) is a better
           alternative.
     5.19  A  multithreaded     web   server  wishes  to   keep  track     of  the  number
           of requests it services (known as hits). Consider the two following
           strategies to prevent a race condition on the variable hits. The first
           strategy is to use a basic mutex lock when updating hits:
                  int    hits;
                  mutex lock hit lock;
                  hit lock.acquire();
                  hits++;
                  hit lock.release();
           A second strategy is to use an atomic integer:
                  atomic t hits;
                  atomic inc(&hits);
           Explain which of these two strategies is more efficient.
     5.20  Consider the code example for allocating and releasing processes shown
           in Figure 5.23.



                                                                         Exercises       247
      a.     Identify the race condition(s).
      b.     Assume you have a mutex lock named mutex with the operations
             acquire() and release(). Indicate where the locking needs to
             be placed to prevent the race condition(s).
      c.     Could we replace the integer variable
                    int     number of processes            =  0
             with the atomic integer
                    atomic t       number of processes            =   0
             to prevent the race condition(s)?
5.21  Servers can be designed to limit the number of open connections. For
      example, a server may wish to have only N socket connections at any
      point    in   time.  As  soon    as  N  connections  are    made,    the  server   will
      not accept another incoming connection until an existing connection
      is released. Explain how semaphores can be used by a server to limit the
      number of concurrent connections.
5.22  Windows Vista provides a lightweight synchronization tool called slim
      reader­writer locks. Whereas most implementations of reader­writer
      locks favor either readers or writers, or perhaps order waiting threads
      using a FIFO policy, slim reader­writer locks favor neither readers nor
      writers, nor are waiting threads ordered in a FIFO queue. Explain the
      benefits of providing such a synchronization tool.
5.23  Show how to implement the wait() and signal() semaphore oper-
      ations    in  multiprocessor         environments    using  the    test and set()
      instruction. The solution should exhibit minimal busy waiting.
5.24  Exercise 4.26 requires the parent thread to wait for the child thread to
      finish its execution before printing out the computed values. If we let the
      parent thread access the Fibonacci numbers as soon as they have been
      computed by the child thread --rather than waiting for the child thread
      to terminate--what changes would be necessary to the solution for this
      exercise? Implement your modified solution.
5.25  Demonstrate       that   monitors    and  semaphores       are  equivalent    insofar
      as  they     can  be  used   to  implement  solutions       to  the  same   types  of
      synchronization problems.
5.26  Design an algorithm for a bounded-buffer monitor in which the buffers
      (portions) are embedded within the monitor itself.
5.27  The strict mutual exclusion within a monitor makes the bounded-buffer
      monitor of Exercise 5.26 mainly suitable for small portions.
      a.     Explain why this is true.
      b.     Design a new scheme that is suitable for larger portions.
5.28  Discuss the tradeoff between fairness and throughput of operations
      in  the   readers ­ writers    problem.   Propose    a  method       for  solving  the
      readers­writers problem without causing starvation.



248  Chapter 5  Process Synchronization
     5.29  How does the signal() operation associated with monitors differ from
           the corresponding operation defined for semaphores?
     5.30  Suppose the signal() statement can appear only as the last statement
           in a monitor function. Suggest how the implementation described in
           Section 5.8 can be simplified in this situation.
     5.31  Consider a system consisting of processes P1, P2, ..., Pn, each of which has
           a unique priority number. Write a monitor that allocates three identical
           printers to these processes, using the priority numbers for deciding the
           order of allocation.
     5.32  A file is to be shared among different processes, each of which has
           a unique number. The file can be accessed simultaneously by several
           processes, subject to the following constraint: the sum of all unique
           numbers associated with all the processes currently accessing the file
           must be less than n. Write a monitor to coordinate access to the file.
     5.33  When a signal is performed on a condition inside a monitor, the signaling
           process can either continue its execution or transfer control to the process
           that is signaled. How would the solution to the preceding exercise differ
           with these two different ways in which signaling can be performed?
     5.34  Suppose we replace the wait() and signal() operations of moni-
           tors with a single construct await(B), where B is a general Boolean
           expression that causes the process executing it to wait until B becomes
           true.
           a.     Write a monitor using this scheme to implement the readers­
                  writers problem.
           b.     Explain why, in general, this construct cannot be implemented
                  efficiently.
           c.     What restrictions need to be put on the await statement so that it
                  can be implemented efficiently? (Hint: Restrict the generality of B;
                  see [Kessels (1977)].)
     5.35  Design an algorithm for a monitor that implements an alarm clock that
           enables a calling program to delay itself for a specified number of time
           units (ticks). You may assume the existence of a real hardware clock that
           invokes a function tick() in your monitor at regular intervals.
Programming Problems
     5.36  Programming Exercise 3.20 required you to design a PID manager that
           allocated  a  unique     process  identifier  to  each  process.  Exercise  4.20
           required you to modify your solution to Exercise 3.20 by writing a
           program that created a number of threads that requested and released
           process    identifiers.  Now   modify  your   solution  to  Exercise  4.20    by
           ensuring that the data structure used to represent the availability of
           process identifiers is safe from race conditions. Use Pthreads mutex
           locks, described in Section 5.9.4.



                                                     Programming Problems          249
5.37  Assume that a finite number of resources of a single resource type must
      be managed. Processes may ask for a number of these resources and will
      return them once finished. As an example, many commercial software
      packages provide a given number of licenses, indicating the number of
      applications that may run concurrently. When the application is started,
      the license count is decremented. When the application is terminated, the
      license count is incremented. If all licenses are in use, requests to start
      the application are denied. Such requests will only be granted when
      an existing license holder terminates the application and a license is
      returned.
          The following program segment is used to manage a finite number of
      instances of an available resource. The maximum number of resources
      and the number of available resources are declared as follows:
                 #define     MAX  RESOURCES   5
                 int  available resources         =  MAX RESOURCES;
      When a process wishes to obtain a number of resources, it invokes the
      decrease count() function:
          /*  decrease       available  resources    by      count  resources   */
          /*  return     0   if   sufficient  resources      available,     */
          /*  otherwise      return  -1  */
          int decrease count(int count) {
              if (available resources < count)
                    return   -1;
              else    {
                    available resources       -=  count;
                    return   0;
              }
          }
      When a process wants to return a number of resources, it              calls  the
      increase count() function:
                 /*  increase     available resources        by  count  */
                 int increase count(int count) {
                     available resources      +=     count;
                     return  0;
                 }
      The preceding program segment produces a race condition. Do the
      following:
      a.      Identify the data involved in the race condition.
      b.      Identify the location (or locations) in the code where the race
              condition occurs.



250  Chapter 5  Process Synchronization
           c.   Using a semaphore or mutex lock, fix the race condition. It is
                permissible to modify the decrease count() function so that the
                calling process is blocked until sufficient resources are available.
     5.38  The  decrease   count()     function  in  the  previous  exercise  currently
           returns 0 if sufficient resources are available and -1 otherwise. This
           leads to awkward programming for a process that wishes to obtain a
           number of resources:
                    while  (decrease count(count)         ==  -1)
                       ;
           Rewrite   the   resource-manager  code    segment  using  a  monitor       and
           condition variables so that the decrease count() function suspends
           the process until sufficient resources are available. This will allow a
           process to invoke decrease count() by simply calling
                    decrease count(count);
           The process will return from this function call only when sufficient
           resources are available.
     5.39  Exercise 4.22 asked you to design a multithreaded program that esti-
           mated  using the Monte Carlo technique. In that exercise, you were
           asked to create a single thread that generated random points, storing
           the result in a global variable. Once that thread exited, the parent thread
           performed the calcuation that estimated the value of . Modify that
           program so that you create several threads, each of which generates
           random points and determines if the points fall within the circle. Each
           thread will have to update the global count of all points that fall within
           the circle. Protect against race conditions on updates to the shared global
           variable by using mutex locks.
     5.40  Exercise  4.23  asked  you  to   design   a  program  using  OpenMP        that
           estimated  using the Monte Carlo technique. Examine your solution to
           that program looking for any possible race conditions. If you identify a
           race condition, protect against it using the strategy outlined in Section
           5.10.2.
     5.41  A barrier is a tool for synchronizing the activity of a number of threads.
           When a thread reaches a barrier point, it cannot proceed until all other
           threads have reached this point as well. When the last thread reaches
           the barrier point, all threads are released and can resume concurrent
           execution.
           Assume that the barrier is initialized to N --the number of threads that
           must wait at the barrier point:
                    init(N);
           Each thread then performs some work until it reaches the barrier point:



                                                 Programming Projects                251
        /*  do  some     work   for      awhile  */
        barrier point();
        /*  do  some     work   for      awhile  */
Using synchronization tools described in this chapter, construct a barrier
that implements the following API:
·  int  init(int         n)--Initializes the barrier to the specified size.
·  int  barrier          point(void) -- Identifies   the  barrier   point.           All
   threads are released from the barrier when the last thread reaches
   this point.
The return value of each function is used to identify error conditions.
Each function will return 0 under normal operation and will return
-1 if an error occurs. A testing harness is provided in the source code
download to test your implementation of the barrier.
Programming Projects
Project 1 --The Sleeping Teaching Assistant
A university computer science department has a teaching assistant (TA) who
helps undergraduate students with their programming assignments during
regular office hours. The TA's office is rather small and has room for only one
desk with a chair and computer. There are three chairs in the hallway outside
the office where students can sit and wait if the TA is currently helping another
student. When there are no students who need help during office hours, the
TA sits at the desk and takes a nap. If a student arrives during office hours
and finds the TA sleeping, the student must awaken the TA to ask for help. If a
student arrives and finds the TA currently helping another student, the student
sits on one of the chairs in the hallway and waits. If no chairs are available, the
student will come back at a later time.
Using POSIX threads, mutex locks, and semaphores, implement a solution
that coordinates the activities of the TA and the students. Details for this
assignment are provided below.
The Students and the TA
Using Pthreads (Section 4.4.1), begin by creating n students. Each will run as a
separate thread. The TA will run as a separate thread as well. Student threads
will alternate between programming for a period of time and seeking help
from the TA. If the TA is available, they will obtain help. Otherwise, they will
either sit in a chair in the hallway or, if no chairs are available, will resume
programming and will seek help at a later time. If a student arrives and notices
that the TA is sleeping, the student must notify the TA using a semaphore. When
the TA finishes helping a student, the TA must check to see if there are students
waiting for help in the hallway. If so, the TA must help each of these students
in turn. If no students are present, the TA may return to napping.



252  Chapter 5  Process Synchronization
     Perhaps the best option for simulating students programming--as well as
     the TA providing help to a student--is to have the appropriate threads sleep
     for a random period of time.
     POSIX Synchronization
     Coverage of POSIX mutex locks and semaphores is provided in Section 5.9.4.
     Consult that section for details.
     Project 2 --The Dining Philosophers Problem
     In Section 5.7.3, we provide an outline of a solution to the dining-philosophers
     problem using monitors. This problem will require implementing a solution
     using Pthreads mutex locks and condition variables.
     The Philosophers
     Begin by creating five philosophers, each identified by a number 0 . . 4. Each
     philosopher will run as a separate thread. Thread creation using Pthreads is
     covered in Section 4.4.1. Philosophers alternate between thinking and eating.
     To simulate both activities, have the thread sleep for a random period between
     one and three seconds. When a philosopher wishes to eat, she invokes the
     function
     pickup     forks(int     philosopher  number)
     where philosopher number identifies the number of the philosopher wishing
     to eat. When a philosopher finishes eating, she invokes
     return     forks(int     philosopher  number)
     Pthreads Condition Variables
     Condition variables in Pthreads behave similarly to those described in Section
     5.8. However, in that section, condition variables are used within the context
     of a monitor, which provides a locking mechanism to ensure data integrity.
     Since Pthreads is typically used in C programs--and since C does not have
     a monitor-- we accomplish locking by associating a condition variable with
     a mutex lock. Pthreads mutex locks are covered in Section 5.9.4. We cover
     Pthreads condition variables here.
     Condition variables in Pthreads use the pthread cond t data type and
     are initialized using the pthread cond init() function. The following code
     creates and initializes a condition variable as well as its associated mutex lock:
     pthread mutex t mutex;
     pthread cond          t  cond var;
     pthread mutex init(&mutex,NULL);
     pthread cond init(&cond var,NULL);



                                                   Programming Projects              253
      The pthread cond wait() function is used for waiting on a condition
variable. The following code illustrates how a thread can wait for the condition
a  ==    b to become true using a Pthread condition variable:
         pthread mutex lock(&mutex);
         while   (a  !=    b)
                pthread cond wait(&mutex,          &cond  var);
         pthread mutex unlock(&mutex);
      The mutex lock associated with the condition variable must be locked
before the pthread cond wait() function is called, since it is used to protect
the data in the conditional clause from a possible race condition. Once this
lock is acquired, the thread can check the condition. If the condition is not true,
the thread then invokes pthread cond wait(), passing the mutex lock and
the condition variable as parameters. Calling pthread cond wait() releases
the mutex lock, thereby allowing another thread to access the shared data and
possibly update its value so that the condition clause evaluates to true. (To
protect against program errors, it is important to place the conditional clause
within a loop so that the condition is rechecked after being signaled.)
      A  thread      that  modifies       the  shared  data    can  invoke           the
pthread cond signal()          function,  thereby  signaling   one  thread  waiting
on the condition variable. This is illustrated below:
         pthread mutex lock(&mutex);
         a = b;
         pthread cond signal(&cond var);
         pthread mutex unlock(&mutex);
      It is important to note that the call to pthread cond signal() does not
release the mutex lock. It is the subsequent call to pthread mutex unlock()
that releases the mutex. Once the mutex lock is released, the signaled thread
becomes the owner of the mutex lock and returns control from the call to
pthread cond wait().
Project 3 --Producer ­ Consumer Problem
In Section 5.7.1, we presented a semaphore-based solution to the producer­
consumer problem using a bounded buffer. In this project, you will design a
programming solution to the bounded-buffer problem using the producer and
consumer processes shown in Figures 5.9 and 5.10. The solution presented in
Section 5.7.1 uses three semaphores: empty and full, which count the number
of empty and full slots in the buffer, and mutex, which is a binary (or mutual-
exclusion) semaphore that protects the actual insertion or removal of items
in the buffer. For this project, you will use standard counting semaphores for
empty and full and a mutex lock, rather than a binary semaphore, to represent
mutex. The producer and consumer--running as separate threads--will move
items to and from a buffer that is synchronized with the empty, full, and mutex
structures. You can solve this problem using either Pthreads or the Windows
API.



254  Chapter 5    Process Synchronization
                  #include     "buffer.h"
                  /*  the      buffer  */
                  buffer item buffer[BUFFER SIZE];
                  int insert item(buffer item item) {
                      /*   insert  item     into  buffer
                      return   0   if  successful,          otherwise
                      return   -1      indicating      an   error   condition      */
                  }
                  int remove item(buffer item               *item)  {
                      /*   remove  an  object     from      buffer
                      placing      it  in   item
                      return   0   if  successful,          otherwise
                      return   -1      indicating      an   error condition        */
                  }
                               Figure 5.24  Outline of buffer operations.
     The Buffer
     Internally, the buffer will consist of a fixed-size array of type buffer item
     (which will be defined using a typedef). The array of buffer item objects
     will be manipulated as a circular queue. The definition of buffer item, along
     with the size of the buffer, can be stored in a header file such as the following:
          /* buffer.h */
          typedef int buffer item;
          #define BUFFER SIZE 5
     The  buffer     will  be  manipulated       with  two  functions,     insert  item()  and
     remove item(), which are called by the producer and consumer threads,
     respectively. A skeleton outlining these functions appears in Figure 5.24.
          The insert item() and remove item() functions will synchronize the
     producer and consumer using the algorithms outlined in Figures 5.9 and
     5.10. The buffer will also require an initialization function that initializes the
     mutual-exclusion object mutex along with the empty and full semaphores.
          The main() function will initialize the buffer and create the separate
     producer   and   consumer         threads.  Once  it   has  created   the  producer   and
     consumer threads, the main() function will sleep for a period of time and,
     upon awakening, will terminate the application. The main() function will be
     passed three parameters on the command line:
     1.   How long to sleep before terminating
     2.   The number of producer threads
     3.   The number of consumer threads



                                                      Programming Projects         255
#include "buffer.h"
int    main(int argc, char *argv[]) {
   /*  1.  Get     command  line    arguments  argv[1],argv[2],argv[3]             */
   /*  2.  Initialize       buffer  */
   /*  3.  Create      producer     thread(s)  */
   /*  4.  Create      consumer     thread(s)  */
   /*  5.  Sleep   */
   /*  6.  Exit    */
}
                       Figure 5.25  Outline of skeleton program.
A skeleton for this function appears in Figure 5.25.
The Producer and Consumer Threads
The producer thread will alternate between sleeping for a random period of
time and inserting a random integer into the buffer. Random numbers will
be produced using the rand() function, which produces random integers
between 0 and RAND MAX. The consumer will also sleep for a random period
of time and, upon awakening, will attempt to remove an item from the buffer.
An outline of the producer and consumer threads appears in Figure 5.26.
   As noted earlier, you can solve this problem using either Pthreads or the
Windows API. In the following sections, we supply more information on each
of these choices.
Pthreads Thread Creation and Synchronization
Creating threads using the Pthreads API is discussed in Section 4.4.1. Coverage
of mutex locks and semaphores using Pthreads is provided in Section 5.9.4.
Refer to those sections for specific instructions on Pthreads thread creation and
synchronization.
Windows
Section 4.4.2 discusses thread creation using the Windows API. Refer to that
section for specific instructions on creating threads.
Windows Mutex Locks
Mutex locks are a type of dispatcher object, as described in Section 5.9.1. The
following illustrates how to create a mutex lock using the CreateMutex()
function:
           #include <windows.h>
           HANDLE  Mutex;
           Mutex = CreateMutex(NULL,           FALSE,   NULL);



256  Chapter  5  Process Synchronization
              #include    <stdlib.h>          /*  required     for  rand()  */
              #include    "buffer.h"
              void  *producer(void            *param)  {
                 buffer   item  item;
                 while (true) {
                    /*  sleep   for        a  random   period  of   time  */
                    sleep(...);
                    /*  generate     a        random  number   */
                    item = rand();
                    if (insert item(item))
                        fprintf("report error condition");
                    else
                        printf("producer produced %d\n",item);
              }
              void  *consumer(void            *param)  {
                 buffer   item  item;
                 while (true) {
                    /*  sleep   for        a  random   period  of   time  */
                    sleep(...);
                    if (remove item(&item))
                        fprintf("report error condition");
                    else
                        printf("consumer consumed %d\n",item);
              }
                 Figure 5.26  An outline of the producer and consumer threads.
     The first parameter refers to a security attribute for the mutex lock. By setting
     this attribute to NULL, we disallow any children of the process creating this
     mutex lock to inherit the handle of the lock. The second parameter indicates
     whether the creator of the mutex lock is the lock's initial owner. Passing a value
     of FALSE indicates that the thread creating the mutex is not the initial owner.
     (We shall soon see how mutex locks are acquired.) The third parameter allows
     us to name the mutex. However, because we provide a value of NULL, we do
     not name the mutex. If successful, CreateMutex() returns a HANDLE to the
     mutex lock; otherwise, it returns NULL.
     In Section 5.9.1, we identified dispatcher objects as being either signaled or
     nonsignaled. A signaled dispatcher object (such as a mutex lock) is available
     for ownership. Once it is acquired, it moves to the nonsignaled state. When it
     is released, it returns to signaled.
     Mutex locks are acquired by invoking the WaitForSingleObject() func-
     tion. The function is passed the HANDLE to the lock along with a flag indicating
     how long to wait. The following code demonstrates how the mutex lock created
     above can be acquired:
              WaitForSingleObject(Mutex,               INFINITE);



                                           Programming Projects                   257
The parameter value INFINITE indicates that we will wait an infinite amount
of time for the lock to become available. Other values could be used that would
allow the calling thread to time out if the lock did not become available within
a specified time. If the lock is in a signaled state, WaitForSingleObject()
returns immediately, and the lock becomes nonsignaled. A lock is released
(moves to the signaled state) by invoking ReleaseMutex()--for example, as
follows:
          ReleaseMutex(Mutex);
Windows Semaphores
Semaphores in the Windows API are dispatcher objects and thus use the same
signaling mechanism as mutex locks. Semaphores are created as follows:
          #include <windows.h>
          HANDLE     Sem;
          Sem = CreateSemaphore(NULL,  1,  5,  NULL);
The first and last parameters identify a security attribute and a name for the
semaphore, similar to what we described for mutex locks. The second and third
parameters indicate the initial value and maximum value of the semaphore. In
this instance, the initial value of the semaphore is 1, and its maximum value
is 5. If successful, CreateSemaphore() returns a HANDLE to the mutex lock;
otherwise, it returns NULL.
    Semaphores are acquired with the same WaitForSingleObject() func-
tion as mutex locks. We acquire the semaphore Sem created in this example by
using the following statement:
          WaitForSingleObject(Semaphore, INFINITE);
If the value of the semaphore is > 0, the semaphore is in the signaled state
and thus is acquired by the calling thread. Otherwise, the calling thread blocks
indefinitely--as we are specifying INFINITE--until the semaphore returns to
the signaled state.
    The equivalent of the signal() operation for Windows semaphores is the
ReleaseSemaphore() function. This function is passed three parameters:
1.  The HANDLE of the semaphore
2.  How much to increase the value of the semaphore
3.  A pointer to the previous value of the semaphore
We can use the following statement to increase Sem by 1:
          ReleaseSemaphore(Sem,  1,  NULL);
Both ReleaseSemaphore() and ReleaseMutex() return a nonzero value if
successful and 0 otherwise.



258  Chapter 5     Process Synchronization
Bibliographical Notes
     The  mutual-exclusion         problem    was  first  discussed    in  a  classic  paper  by
     [Dijkstra (1965)]. Dekker's algorithm (Exercise 5.8)--the first correct software
     solution to the two-process mutual-exclusion problem--was developed by the
     Dutch mathematician T. Dekker. This algorithm also was discussed by [Dijkstra
     (1965)]. A simpler solution to the two-process mutual-exclusion problem has
     since been presented by [Peterson (1981)] (Figure 5.2). The semaphore concept
     was suggested by [Dijkstra (1965)].
          The classic process-coordination problems that we have described are
     paradigms for a large class of concurrency-control problems. The bounded-
     buffer  problem       and  the  dining-philosophers        problem    were  suggested    in
     [Dijkstra    (1965)]  and     [Dijkstra  (1971)].  The  readers ­ writers   problem      was
     suggested by [Courtois et al. (1971)].
          The     critical-region    concept  was       suggested  by    [Hoare  (1972)]      and
     by   [Brinch-Hansen        (1972)].      The  monitor   concept       was  developed     by
     [Brinch-Hansen        (1973)].  [Hoare   (1974)]     gave  a  complete      description  of
     the monitor.
          Some details of the locking mechanisms used in Solaris were presented
     in [Mauro and McDougall (2007)]. As noted earlier, the locking mechanisms
     used by the kernel are implemented for user-level threads as well, so the same
     types of locks are available inside and outside the kernel. Details of Windows
     2000 synchronization can be found in [Solomon and Russinovich (2000)]. [Love
     (2010)] describes synchronization in the Linux kernel.
          Information on Pthreads programming can be found in [Lewis and Berg
     (1998)] and [Butenhof (1997)]. [Hart (2005)] describes thread synchronization
     using Windows. [Goetz et al. (2006)] present a detailed discussion of concur-
     rent programming in Java as well as the java.util.concurrent package.
     [Breshears (2009)] and [Pacheco (2011)] provide detailed coverage of synchro-
     nization issues in relation to parallel programming. [Lu et al. (2008)] provide a
     study of concurrency bugs in real-world applications.
          [Adl-Tabatabai et al. (2007)] discuss transactional memory. Details on using
     OpenMP can be found at http://openmp.org. Functional programming using
     Erlang and Scala is covered in [Armstrong (2007)] and [Odersky et al. ()]
     respectively.
Bibliography
     [Adl-Tabatabai et al. (2007)]        A.-R. Adl-Tabatabai, C. Kozyrakis, and B. Saha,
         "Unlocking Concurrency", Queue, Volume 4, Number 10 (2007), pages 24­33.
     [Armstrong (2007)]         J. Armstrong, Programming Erlang Software for a Concurrent
         World, The Pragmatic Bookshelf (2007).
     [Breshears (2009)]    C. Breshears, The Art of Concurrency, O'Reilly & Associates
         (2009).
     [Brinch-Hansen (1972)]          P.  Brinch-Hansen,     "Structured    Multiprogramming",
         Communications of the ACM, Volume 15, Number 7 (1972), pages 574­578.



                                                              Bibliography              259
[Brinch-Hansen (1973)]     P. Brinch-Hansen, Operating System Principles, Prentice
Hall (1973).
[Butenhof (1997)]      D.  Butenhof,  Programming  with  POSIX  Threads,  Addison-
Wesley (1997).
[Courtois et al. (1971)]   P. J. Courtois, F. Heymans, and D. L. Parnas, "Concurrent
Control with `Readers' and `Writers'", Communications of the ACM, Volume 14,
Number 10 (1971), pages 667­668.
[Dijkstra (1965)]      E. W. Dijkstra, "Cooperating Sequential Processes", Technical
report, Technological University, Eindhoven, the Netherlands (1965).
[Dijkstra (1971)]      E. W. Dijkstra, "Hierarchical Ordering of Sequential Processes",
Acta Informatica, Volume 1, Number 2 (1971), pages 115­138.
[Goetz et al. (2006)]     B. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and
D. Lea, Java Concurrency in Practice, Addison-Wesley (2006).
[Hart (2005)]       J. M. Hart, Windows System Programming, Third Edition, Addison-
Wesley (2005).
[Hoare (1972)]       C. A. R. Hoare, "Towards a Theory of Parallel Programming", in
[Hoare and Perrott 1972] (1972), pages 61­71.
[Hoare (1974)]       C. A. R. Hoare, "Monitors: An Operating System Structuring
Concept", Communications of the ACM, Volume 17, Number 10 (1974), pages
549­557.
[Kessels (1977)]       J. L. W. Kessels, "An Alternative to Event Queues for Synchro-
nization in Monitors", Communications of the ACM, Volume 20, Number 7 (1977),
pages 500­503.
[Lewis and Berg (1998)]    B. Lewis and D. Berg, Multithreaded Programming with
Pthreads, Sun Microsystems Press (1998).
[Love (2010)]       R. Love, Linux Kernel Development, Third Edition, Developer's
Library (2010).
[Lu et al. (2008)]     S. Lu, S. Park, E. Seo, and Y. Zhou, "Learning from mistakes: a
comprehensive study on real world concurrency bug characteristics", SIGPLAN
Notices, Volume 43, Number 3 (2008), pages 329­339.
[Mauro and McDougall (2007)]          J. Mauro and R. McDougall, Solaris Internals:
Core Kernel Architecture, Prentice Hall (2007).
[Odersky et al. ()]    M. Odersky, V. Cremet, I. Dragos, G. Dubochet, B. Emir,
S. Mcdirmid, S. Micheloud, N. Mihaylov, M. Schinz, E. Stenman, L. Spoon,
and M. Zenger.
[Pacheco (2011)]       P. S. Pacheco, An Introduction to Parallel Programming, Morgan
Kaufmann (2011).
[Peterson (1981)]      G. L. Peterson, "Myths About the Mutual Exclusion Problem",
Information Processing Letters, Volume 12, Number 3 (1981).
[Solomon and Russinovich (2000)]      D. A. Solomon and M. E. Russinovich, Inside
Microsoft Windows 2000, Third Edition, Microsoft Press (2000).



