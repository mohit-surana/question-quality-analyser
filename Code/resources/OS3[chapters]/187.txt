Threads


                                                                        4C H A P T E R
Threads
     The process model introduced in Chapter 3 assumed that a process was
     an executing program with a single thread of control. Virtually all modern
     operating systems, however, provide features enabling a process to contain
     multiple  threads  of  control.  In  this  chapter,  we  introduce  many  concepts
     associated with multithreaded computer systems, including a discussion of
     the APIs for the Pthreads, Windows, and Java thread libraries. We look at a
     number of issues related to multithreaded programming and its effect on the
     design of operating systems. Finally, we explore how the Windows and Linux
     operating systems support threads at the kernel level.
     CHAPTER OBJECTIVES
     ·  To introduce the notion of a thread -- a fundamental unit of CPU utilization
        that forms the basis of multithreaded computer systems.
     ·  To discuss the APIs for the Pthreads, Windows, and Java thread libraries.
     ·  To explore several strategies that provide implicit threading.
     ·  To examine issues related to multithreaded programming.
     ·  To cover operating system support for threads in Windows and Linux.
4.1  Overview
     A thread is a basic unit of CPU utilization; it comprises a thread ID, a program
     counter, a register set, and a stack. It shares with other threads belonging
     to the same process its code section, data section, and other operating-system
     resources, such as open files and signals. A traditional (or heavyweight) process
     has a single thread of control. If a process has multiple threads of control, it
     can perform more than one task at a time. Figure 4.1 illustrates the difference
     between a traditional single-threaded process and a multithreaded process.
     4.1.1  Motivation
     Most software applications that run on modern computers are multithreaded.
     An application typically is implemented as a separate process with several
                                                                                        163



164  Chapter 4  Threads
     code             data      files  code                   data       files
     registers              stack      registers              registers  registers
                                       stack                  stack      stack
     thread                                                                         thread
     single-threaded process           multithreaded process
                Figure 4.1  Single-threaded and multithreaded processes.
     threads of control. A web browser might have one thread display images or
     text while another thread retrieves data from the network, for example. A
     word processor may have a thread for displaying graphics, another thread for
     responding to keystrokes from the user, and a third thread for performing
     spelling and grammar checking in the background. Applications can also
     be designed to leverage processing capabilities on multicore systems. Such
     applications can perform several CPU-intensive tasks in parallel across the
     multiple computing cores.
     In certain situations, a single application may be required to perform
     several similar tasks. For example, a web server accepts client requests for
     web pages, images, sound, and so forth. A busy web server may have several
     (perhaps thousands of) clients concurrently accessing it. If the web server ran
     as a traditional single-threaded process, it would be able to service only one
     client at a time, and a client might have to wait a very long time for its request
     to be serviced.
     One solution is to have the server run as a single process that accepts
     requests. When the server receives a request, it creates a separate process
     to service that request. In fact, this process-creation method was in common
     use before threads became popular. Process creation is time consuming and
     resource intensive, however. If the new process will perform the same tasks as
     the existing process, why incur all that overhead? It is generally more efficient
     to use one process that contains multiple threads. If the web-server process is
     multithreaded, the server will create a separate thread that listens for client
     requests. When a request is made, rather than creating another process, the
     server creates a new thread to service the request and resume listening for
     additional requests. This is illustrated in Figure 4.2.
     Threads also play a vital role in remote procedure call (RPC) systems. Recall
     from Chapter 3 that RPCs allow interprocess communication by providing a
     communication mechanism similar to ordinary function or procedure calls.
     Typically, RPC servers are multithreaded. When a server receives a message, it



                                                             4.1         Overview         165
                                                  (2) create new
                   (1) request                    thread to service
                                                       the request
         client                  server                                      thread
                                 (3) resume listening
                                 for additional
                                 client requests
                   Figure 4.2    Multithreaded server architecture.
services the message using a separate thread. This allows the server to service
several concurrent requests.
    Finally, most operating-system kernels are now multithreaded. Several
threads operate in the kernel, and each thread performs a specific task, such
as managing devices, managing memory, or interrupt handling. For example,
Solaris has a set of threads in the kernel specifically for interrupt handling;
Linux uses a kernel thread for managing the amount of free memory in the
system.
4.1.2    Benefits
The benefits of multithreaded programming can be broken down into four
major categories:
1.  Responsiveness. Multithreading an interactive application may allow
    a    program   to  continue  running  even    if   part  of      it  is  blocked  or  is
    performing a lengthy operation, thereby increasing responsiveness to
    the user. This quality is especially useful in designing user interfaces. For
    instance, consider what happens when a user clicks a button that results
    in the performance of a time-consuming operation. A single-threaded
    application would be unresponsive to the user until the operation had
    completed. In contrast, if the time-consuming operation is performed in
    a separate thread, the application remains responsive to the user.
2.  Resource sharing. Processes can only share resources through techniques
    such as shared memory and message passing. Such techniques must
    be explicitly arranged by the programmer. However, threads share the
    memory and the resources of the process to which they belong by default.
    The benefit of sharing code and data is that it allows an application to
    have several different threads of activity within the same address space.
3.  Economy. Allocating memory and resources for process creation is costly.
    Because threads share the resources of the process to which they belong,
    it is more economical to create and context-switch threads. Empirically
    gauging the difference in overhead can be difficult, but in general it is
    significantly more time consuming to create and manage processes than
    threads. In Solaris, for example, creating a process is about thirty times



166  Chapter 4        Threads
         single core  T1       T2      T3       T4  T1        T2  T3         T4       T1  ...
                                                        time
                      Figure 4.3   Concurrent execution on a single-core system.
         slower than is creating a thread, and context switching is about five times
         slower.
     4.  Scalability.     The     benefits  of  multithreading    can        be   even    greater  in  a
         multiprocessor architecture, where threads may be running in parallel
         on different processing cores. A single-threaded process can run on only
         one processor, regardless how many are available. We explore this issue
         further in the following section.
4.2  Multicore Programming
     Earlier in the history of computer design, in response to the need for more
     computing performance, single-CPU systems evolved into multi-CPU systems.
     A more recent, similar trend in system design is to place multiple computing
     cores on a single chip. Each core appears as a separate processor to the
     operating system (Section 1.3.2). Whether the cores appear across CPU chips or
     within CPU chips, we call these systems multicore or multiprocessor systems.
     Multithreaded programming provides a mechanism for more efficient use
     of these multiple computing cores and improved concurrency. Consider an
     application with four threads. On a system with a single computing core,
     concurrency merely means that the execution of the threads will be interleaved
     over time (Figure 4.3), because the processing core is capable of executing only
     one thread at a time. On a system with multiple cores, however, concurrency
     means that the threads can run in parallel, because the system can assign a
     separate thread to each core (Figure 4.4).
         Notice the distinction between parallelism and concurrency in this discus-
     sion. A system is parallel if it can perform more than one task simultaneously.
     In contrast, a concurrent system supports more than one task by allowing all
     the tasks to make progress. Thus, it is possible to have concurrency without
     parallelism. Before the advent of SMP and multicore architectures, most com-
     puter systems had only a single processor. CPU schedulers were designed to
     provide the illusion of parallelism by rapidly switching between processes in
                       core 1      T1       T3      T1        T3  T1         ...
                       core 2      T2       T4      T2        T4  T2         ...
                                                        time
                      Figure 4.4   Parallel execution on a        multicore  system.



                                              4.2  Multicore Programming            167
                                 AMDAHL'S LAW
Amdahl's Law is a formula that identifies potential performance gains from
adding additional computing cores to an application that has both serial
(nonparallel) and parallel components. If S is the portion of the application
that must be performed serially on a system with N processing cores, the
formula appears as follows:
speedup            1
             S  +  (1-S)
                      N
As an example, assume we have an application that is 75 percent parallel and
25 percent serial. If we run this application on a system with two processing
cores, we can get a speedup of 1.6 times. If we add two additional cores (for
a total of four), the speedup is 2.28 times.
    One interesting fact about Amdahl's Law is that as N approaches infinity,
the speedup converges to 1/S. For example, if 40 percent of an application
is  performed   serially,  the  maximum       speedup  is  2.5  times,  regardless  of
the number of processing cores we add. This is the fundamental principle
behind     Amdahl's   Law:  the  serial  portion   of  an  application  can  have   a
disproportionate effect on the performance we gain by adding additional
computing cores.
    Some argue that Amdahl's Law does not take into account the hardware
performance enhancements used in the design of contemporary multicore
systems. Such arguments suggest Amdahl's Law may cease to be applicable
as the number of processing cores continues to increase on modern computer
systems.
the system, thereby allowing each process to make progress. Such processes
were running concurrently, but not in parallel.
As systems have grown from tens of threads to thousands of threads, CPU
designers have improved system performance by adding hardware to improve
thread performance. Modern Intel CPUs frequently support two threads per
core, while the Oracle T4 CPU supports eight threads per core. This support
means that multiple threads can be loaded into the core for fast switching.
Multicore computers will no doubt continue to increase in core counts and
hardware thread support.
4.2.1  Programming Challenges
The trend towards multicore systems continues to place pressure on system
designers and application programmers to make better use of the multiple
computing  cores.  Designers     of  operating     systems  must  write  scheduling
algorithms that use multiple processing cores to allow the parallel execution
shown in Figure 4.4. For application programmers, the challenge is to modify
existing programs as well as design new programs that are multithreaded.
In general, five areas present challenges in programming for multicore
systems:



168  Chapter 4      Threads
     1.     Identifying tasks. This involves examining applications to find areas
            that can be divided into separate, concurrent tasks. Ideally, tasks are
            independent of one another and thus can run in parallel on individual
            cores.
     2.     Balance. While identifying tasks that can run in parallel, programmers
            must also ensure that the tasks perform equal work of equal value. In
            some instances, a certain task may not contribute as much value to the
            overall process as other tasks. Using a separate execution core to run that
            task may not be worth the cost.
     3.     Data splitting. Just as applications are divided into separate tasks, the
            data accessed and manipulated by the tasks must be divided to run on
            separate cores.
     4.     Data dependency. The data accessed by the tasks must be examined for
            dependencies between two or more tasks. When one task depends on
            data from another, programmers must ensure that the execution of the
            tasks is synchronized to accommodate the data dependency. We examine
            such strategies in Chapter 5.
     5.     Testing and debugging. When a program is running in parallel on
            multiple cores, many different execution paths are possible. Testing and
            debugging such concurrent programs is inherently more difficult than
            testing and debugging single-threaded applications.
     Because of these challenges, many software developers argue that the advent of
     multicore systems will require an entirely new approach to designing software
     systems in the future. (Similarly, many computer science educators believe that
     software development must be taught with increased emphasis on parallel
     programming.)
     4.2.2  Types of Parallelism
     In  general,   there  are  two  types    of  parallelism:  data  parallelism  and  task
     parallelism. Data parallelism focuses on distributing subsets of the same data
     across multiple computing cores and performing the same operation on each
     core. Consider, for example, summing the contents of an array of size N. On a
     single-core system, one thread would simply sum the elements [0] . . . [N - 1].
     On a dual-core system, however, thread A, running on core 0, could sum the
     elements [0] . . . [N/2 - 1] while thread B, running on core 1, could sum the
     elements [N/2] . . . [N - 1]. The two threads would be running in parallel on
     separate computing cores.
         Task parallelism involves distributing not data but tasks (threads) across
     multiple computing cores. Each thread is performing a unique operation.
     Different threads may be operating on the same data, or they may be operating
     on different data. Consider again our example above. In contrast to that
     situation, an example of task parallelism might involve two threads, each
     performing a unique statistical operation on the array of elements. The threads
     again  are     operating   in  parallel  on  separate  computing  cores,  but  each  is
     performing a unique operation.



                                           4.3    Multithreading Models                    169
     Fundamentally, then, data parallelism involves the distribution of data
     across multiple cores and task parallelism on the distribution of tasks across
     multiple cores. In practice, however, few applications strictly follow either data
     or task parallelism. In most instances, applications use a hybrid of these two
     strategies.
4.3  Multithreading Models
     Our discussion so far has treated threads in a generic sense. However, support
     for threads may be provided either at the user level, for user threads, or by the
     kernel, for kernel threads. User threads are supported above the kernel and
     are managed without kernel support, whereas kernel threads are supported
     and managed directly by the operating system. Virtually all contemporary
     operating    systems -- including  Windows,  Linux,  Mac   OS  X,  and  Solaris --
     support kernel threads.
     Ultimately, a relationship must exist between user threads and kernel
     threads. In this section, we look at three common ways of establishing such a
     relationship: the many-to-one model, the one-to-one model, and the many-to-
     many model.
     4.3.1  Many-to-One Model
     The many-to-one model (Figure 4.5) maps many user-level threads to one
     kernel thread. Thread management is done by the thread library in user space,
     so it is efficient (we discuss thread libraries in Section 4.4). However, the entire
     process will block if a thread makes a blocking system call. Also, because only
     one thread can access the kernel at a time, multiple threads are unable to run in
     parallel on multicore systems. Green threads--a thread library available for
     Solaris systems and adopted in early versions of Java--used the many-to-one
     model. However, very few systems continue to use the model because of its
     inability to take advantage of multiple processing cores.
                                                  user thread
                                        k  kernel thread
                              Figure 4.5   Many-to-one model.



170  Chapter 4  Threads
                                                   user thread
                           k     k        k     k  kernel thread
                              Figure 4.6  One-to-one model.
     4.3.2  One-to-One Model
     The one-to-one model (Figure 4.6) maps each user thread to a kernel thread. It
     provides more concurrency than the many-to-one model by allowing another
     thread to run when a thread makes a blocking system call. It also allows
     multiple threads to run in parallel on multiprocessors. The only drawback to
     this model is that creating a user thread requires creating the corresponding
     kernel thread. Because the overhead of creating kernel threads can burden the
     performance of an application, most implementations of this model restrict the
     number of threads supported by the system. Linux, along with the family of
     Windows operating systems, implement the one-to-one model.
     4.3.3  Many-to-Many Model
     The many-to-many model (Figure 4.7) multiplexes many user-level threads to
     a smaller or equal number of kernel threads. The number of kernel threads
     may be specific to either a particular application or a particular machine (an
     application may be allocated more kernel threads on a multiprocessor than on
     a single processor).
     Let's consider the effect of this design on concurrency. Whereas the many-
     to-one model allows the developer to create as many user threads as she wishes,
     it does not result in true concurrency, because the kernel can schedule only
     one thread at a time. The one-to-one model allows greater concurrency, but the
     developer has to be careful not to create too many threads within an application
     (and in some instances may be limited in the number of threads she can
                                                   user thread
                              k     k        k     kernel thread
                              Figure 4.7  Many-to-many model.



                                                           4.4  Thread Libraries         171
                                                           user thread
                       k  k  k                          k  kernel thread
                          Figure 4.8  Two-level model.
     create). The many-to-many model suffers from neither of these shortcomings:
     developers can create as many user threads as necessary, and the corresponding
     kernel threads can run in parallel on a multiprocessor. Also, when a thread
     performs a blocking system call, the kernel can schedule another thread for
     execution.
     One variation on the many-to-many model still multiplexes many user-
     level threads to a smaller or equal number of kernel threads but also allows a
     user-level thread to be bound to a kernel thread. This variation is sometimes
     referred to as the two-level model (Figure 4.8). The Solaris operating system
     supported the two-level model in versions older than Solaris 9. However,
     beginning with Solaris 9, this system uses the one-to-one model.
4.4  Thread Libraries
     A thread library provides the programmer with an API for creating and
     managing threads. There are two primary ways of implementing a thread
     library. The first approach is to provide a library entirely in user space with no
     kernel support. All code and data structures for the library exist in user space.
     This means that invoking a function in the library results in a local function
     call in user space and not a system call.
     The second approach is to implement a kernel-level library supported
     directly by the operating system. In this case, code and data structures for
     the library exist in kernel space. Invoking a function in the API for the library
     typically results in a system call to the kernel.
     Three main thread libraries are in use today: POSIX Pthreads, Windows, and
     Java. Pthreads, the threads extension of the POSIX standard, may be provided
     as either a user-level or a kernel-level library. The Windows thread library
     is a kernel-level library available on Windows systems. The Java thread API
     allows threads to be created and managed directly in Java programs. However,
     because in most instances the JVM is running on top of a host operating system,
     the Java thread API is generally implemented using a thread library available
     on the host system. This means that on Windows systems, Java threads are
     typically implemented using the Windows API; UNIX and Linux systems often
     use Pthreads.



172  Chapter 4   Threads
          For POSIX and Windows threading, any data declared globally--that is,
     declared outside of any function--are shared among all threads belonging to
     the same process. Because Java has no notion of global data, access to shared
     data must be explicitly arranged between threads. Data declared local to a
     function are typically stored on the stack. Since each thread has its own stack,
     each thread has its own copy of local data.
          In the remainder of this section, we describe basic thread creation using
     these three thread libraries. As an illustrative example, we design a multi-
     threaded program that performs the summation of a non-negative integer in a
     separate thread using the well-known summation function:
                 N
          sum =        i
                 i =0
     For example, if N were 5, this function would represent the summation of
     integers from 0 to 5, which is 15. Each of the three programs will be run with
     the upper bounds of the summation entered on the command line. Thus, if the
     user enters 8, the summation of the integer values from 0 to 8 will be output.
          Before we proceed with our examples of thread creation, we introduce
     two general strategies for creating multiple threads: asynchronous threading
     and synchronous threading. With asynchronous threading, once the parent
     creates a child thread, the parent resumes its execution, so that the parent
     and child execute concurrently. Each thread runs independently of every other
     thread, and the parent thread need not know when its child terminates. Because
     the threads are independent, there is typically little data sharing between
     threads. Asynchronous threading is the strategy used in the multithreaded
     server illustrated in Figure 4.2.
          Synchronous threading occurs when the parent thread creates one or more
     children and then must wait for all of its children to terminate before it resumes
     --the so-called fork-join strategy. Here, the threads created by the parent
     perform work concurrently, but the parent cannot continue until this work
     has been completed. Once each thread has finished its work, it terminates
     and joins with its parent. Only after all of the children have joined can the
     parent resume execution. Typically, synchronous threading involves significant
     data sharing among threads. For example, the parent thread may combine the
     results calculated by its various children. All of the following examples use
     synchronous threading.
     4.4.1     Pthreads
     Pthreads refers to the POSIX standard (IEEE 1003.1c) defining an API for thread
     creation   and    synchronization.  This  is  a  specification  for  thread  behavior,
     not  an   implementation.  Operating-system      designers      may  implement      the
     specification in any way they wish. Numerous systems implement the Pthreads
     specification; most are UNIX-type systems, including Linux, Mac OS X, and
     Solaris. Although Windows doesn't support Pthreads natively, some third-
     party implementations for Windows are available.
          The C program shown in Figure 4.9 demonstrates the basic Pthreads API for
     constructing a multithreaded program that calculates the summation of a non-
     negative integer in a separate thread. In a Pthreads program, separate threads



                                                               4.4  Thread Libraries      173
#include       <pthread.h>
#include       <stdio.h>
int      sum;  /*  this         data   is     shared      by  the   thread(s)     */
void     *runner(void           *param);          /*  threads      call  this  function   */
int      main(int     argc,         char   *argv[])
{
    pthread t      tid;         /*  the     thread     identifier        */
    pthread attr          t     attr;     /*  set     of  thread    attributes        */
    if (argc != 2) {
         fprintf(stderr,"usage: a.out <integer value>\n");
         return -1;
    }
    if (atoi(argv[1]) < 0) {
         fprintf(stderr,"%d must be >= 0\n",atoi(argv[1]));
         return -1;
    }
    /*   get   the     default        attributes          */
    pthread attr init(&attr);
    /*   create     the         thread    */
    pthread create(&tid,&attr,runner,argv[1]);
    /*   wait     for     the       thread    to  exit    */
    pthread join(tid,NULL);
    printf("sum = %d\n",sum);
}
/*   The      thread   will         begin     control     in  this  function      */
void     *runner(void           *param)
{
    int   i,   upper         =  atoi(param);
    sum = 0;
    for   (i   =   1;     i     <=  upper;    i++)
         sum   +=  i;
    pthread exit(0);
}
              Figure 4.9     Multithreaded C program using the Pthreads API.
begin execution in a specified function. In Figure 4.9, this is the runner()
function. When this program begins, a single thread of control begins in
main(). After some initialization, main() creates a second thread that begins
control in the runner() function. Both threads share the global data sum.
   Let's look more closely at this program. All Pthreads programs must
include  the   pthread.h        header     file.  The  statement    pthread    t  tid     declares



174  Chapter 4  Threads
                #define NUM THREADS 10
                /*   an  array     of  threads    to  be  joined  upon        */
                pthread t workers[NUM THREADS];
                for  (int  i    =  0;  i  <  NUM THREADS;    i++)
                    pthread join(workers[i], NULL);
                         Figure 4.10   Pthread code for joining ten threads.
     the identifier for the thread we will create. Each thread has a set of attributes,
     including stack size and scheduling information.        The  pthread          attr    t  attr
     declaration represents the attributes for the thread. We set the attributes in the
     function call pthread attr init(&attr). Because we did not explicitly set
     any attributes, we use the default attributes provided. (In Chapter 6, we discuss
     some of the scheduling attributes provided by the Pthreads API.) A separate
     thread is created with the pthread create() function call. In addition to
     passing the thread identifier and the attributes for the thread, we also pass the
     name of the function where the new thread will begin execution--in this case,
     the runner() function. Last, we pass the integer parameter that was provided
     on the command line, argv[1].
     At this point, the program has two threads: the initial (or parent) thread
     in main() and the summation (or child) thread performing the summation
     operation in the runner() function. This program follows the fork-join strategy
     described  earlier:  after    creating  the  summation  thread,          the  parent  thread
     will wait for it to terminate by calling the pthread join() function. The
     summation thread will terminate when it calls the function pthread exit().
     Once the summation thread has returned, the parent thread will output the
     value of the shared data sum.
     This example program creates only a single thread. With the growing
     dominance of multicore systems, writing programs containing several threads
     has become increasingly common. A simple method for waiting on several
     threads using the pthread join() function is to enclose the operation within
     a simple for loop. For example, you can join on ten threads using the Pthread
     code shown in Figure 4.10.
     4.4.2  Windows Threads
     The technique for creating threads using the Windows thread library is similar
     to the Pthreads technique in several ways. We illustrate the Windows thread
     API in the C program shown in Figure 4.11. Notice that we must include the
     windows.h header file when using the Windows API.
     Just as in the Pthreads version shown in Figure 4.9, data shared by the
     separate threads--in this case, Sum--are declared globally (the DWORD data
     type is an unsigned 32-bit integer). We also define the Summation() function
     that is to be performed in a separate thread. This function is passed a pointer
     to a void, which Windows defines as LPVOID. The thread performing this
     function sets the global data Sum to the value of the summation from 0 to the
     parameter passed to Summation().



                                                             4.4  Thread Libraries      175
#include <windows.h>
#include <stdio.h>
DWORD     Sum;    /*   data      is  shared     by  the      thread(s)   */
/*   the    thread     runs      in  this  separate          function    */
DWORD     WINAPI      Summation(LPVOID          Param)
{
    DWORD Upper = *(DWORD*)Param;
    for   (DWORD      i   =  0;  i   <=    Upper;   i++)
        Sum   +=   i;
    return    0;
}
int    main(int       argc,      char   *argv[])
{
    DWORD ThreadId;
    HANDLE    ThreadHandle;
    int Param;
    if (argc != 2) {
        fprintf(stderr,"An integer                  parameter        is  required\n");
        return -1;
    }
    Param = atoi(argv[1]);
    if (Param < 0) {
        fprintf(stderr,"An integer                  >=    0   is  required\n");
        return -1;
    }
    /*   create    the       thread    */
    ThreadHandle          =  CreateThread(
        NULL,     /*   default       security       attributes       */
        0,    /*  default        stack    size  */
        Summation,        /*     thread    function       */
        &Param,       /*  parameter        to   thread       function    */
        0,    /*  default        creation      flags    */
        &ThreadId);          /*  returns       the  thread        identifier  */
    if (ThreadHandle != NULL) {
          /*  now     wait    for    the   thread     to     finish  */
        WaitForSingleObject(ThreadHandle,INFINITE);
        /*    close    the    thread      handle    */
        CloseHandle(ThreadHandle);
        printf("sum = %d\n",Sum);
    }
}
          Figure 4.11     Multithreaded C program using the Windows API.



176  Chapter 4    Threads
         Threads  are  created  in  the  Windows     API   using  the  CreateThread()
     function, and--just as in Pthreads--a set of attributes for the thread is passed
     to this function. These attributes include security information, the size of the
     stack, and a flag that can be set to indicate if the thread is to start in a suspended
     state. In this program, we use the default values for these attributes. (The
     default values do not initially set the thread to a suspended state and instead
     make it eligible to be run by the CPU scheduler.) Once the summation thread
     is created, the parent must wait for it to complete before outputting the value
     of Sum, as the value is set by the summation thread. Recall that the Pthread
     program (Figure 4.9) had the parent thread wait for the summation thread
     using the pthread join() statement. We perform the equivalent of this in the
     Windows API using the WaitForSingleObject() function, which causes the
     creating thread to block until the summation thread has exited.
         In situations that require waiting for multiple threads to complete, the
     WaitForMultipleObjects() function is used. This function is passed four
     parameters:
     1.  The number of objects to wait for
     2.  A pointer to the array of objects
     3.  A flag indicating whether all objects have been signaled
     4.  A timeout duration (or INFINITE)
     For example, if THandles is an array of thread HANDLE objects of size N, the
     parent thread can wait for all its child threads to complete with this statement:
            WaitForMultipleObjects(N, THandles, TRUE, INFINITE);
     4.4.3  Java Threads
     Threads are the fundamental model of program execution in a Java program,
     and the Java language and its API provide a rich set of features for the creation
     and management of threads. All Java programs comprise at least a single thread
     of control--even a simple Java program consisting of only a main() method
     runs as a single thread in the JVM. Java threads are available on any system that
     provides a JVM including Windows, Linux, and Mac OS X. The Java thread API
     is available for Android applications as well.
         There are two techniques for creating threads in a Java program. One
     approach is to create a new class that is derived from the Thread class and
     to override its run() method. An alternative--and more commonly used--
     technique is to define a class that implements the Runnable interface. The
     Runnable interface is defined as follows:
                       public   interface       Runnable
                       {
                           public   abstract         void  run();
                       }
     When a class implements Runnable, it must define a run() method. The code
     implementing the run() method is what runs as a separate thread.



                                                           4.5     Implicit Threading       177
             Figure  4.12  shows  the  Java  version   of  a    multithreaded   program     that
     determines the summation of a non-negative integer. The Summation class
     implements the Runnable interface. Thread creation is performed by creating
     an object instance of the Thread class and passing the constructor a Runnable
     object.
             Creating a Thread object does not specifically create the new thread; rather,
     the start() method creates the new thread. Calling the start() method for
     the new object does two things:
         1.  It allocates memory and initializes a new thread in the JVM.
         2.  It calls the run() method, making the thread eligible to be run by the JVM.
             (Note again that we never call the run() method directly. Rather, we call
             the start() method, and it calls the run() method on our behalf.)
             When the summation program runs, the JVM creates two threads. The first
     is the parent thread, which starts execution in the main() method. The second
     thread is created when the start() method on the Thread object is invoked.
     This child thread begins execution in the run() method of the Summation class.
     After outputting the value of the summation, this thread terminates when it
     exits from its run() method.
             Data sharing between threads occurs easily in Windows and Pthreads, since
     shared data are simply declared globally. As a pure object-oriented language,
     Java has no such notion of global data. If two or more threads are to share
     data in a Java program, the sharing occurs by passing references to the shared
     object to the appropriate threads. In the Java program shown in Figure 4.12,
     the main thread and the summation thread share the object instance of the Sum
     class. This shared object is referenced through the appropriate getSum() and
     setSum() methods. (You might wonder why we don't use an Integer object
     rather than designing a new sum class. The reason is that the Integer class is
     immutable--that is, once its value is set, it cannot change.)
             Recall that the parent threads in the Pthreads and Windows libraries
     use     pthread  join()      and  WaitForSingleObject()          (respectively)   to  wait
     for the summation threads to finish before proceeding. The join() method
     in  Java  provides    similar     functionality.  (Notice  that  join()   can   throw  an
     InterruptedException, which we choose to ignore.) If the parent must wait
     for several threads to finish, the join() method can be enclosed in a for loop
     similar to that shown for Pthreads in Figure 4.10.
4.5  Implicit Threading
     With the continued growth of multicore processing, applications containing
     hundreds -- or        even  thousands -- of  threads     are  looming  on  the    horizon.
     Designing such applications is not a trivial undertaking: programmers must
     address not only the challenges outlined in Section 4.2 but additional difficulties
     as well. These difficulties, which relate to program correctness, are covered in
     Chapters 5 and 7.
             One way to address these difficulties and better support the design of
     multithreaded applications is to transfer the creation and management of



178  Chapter 4    Threads
     class       Sum
     {
        private       int      sum;
        public        int   getSum()        {
           return      sum;
        }
        public        void     setSum(int        sum)  {
           this.sum         =  sum;
        }
     }
     class       Summation         implements      Runnable
     {
        private       int      upper;
        private       Sum      sumValue;
        public        Summation(int            upper,  Sum     sumValue)         {
           this.upper          =   upper;
           this.sumValue              =  sumValue;
        }
        public        void     run()     {
           int    sum    =     0;
           for    (int      i  =   0;    i  <=  upper;    i++)
                 sum  +=    i;
           sumValue.setSum(sum);
        }
     }
     public       class     Driver
     {
        public        static       void     main(String[]      args)     {
           if    (args.length            >  0)  {
              if  (Integer.parseInt(args[0])                   <  0)
                 System.err.println(args[0]                 +  "  must       be     >=  0.");
              else    {
                 Sum  sumObject          =  new    Sum();
                 int  upper        =  Integer.parseInt(args[0]);
                 Thread     thrd      =  new    Thread(new     Summation(upper,         sumObject));
                 thrd.start();
                 try  {
                      thrd.join();
                      System.out.println
                                   ("The    sum    of  "+upper+"         is  "+sumObject.getSum());
              }   catch     (InterruptedException              ie)    {  }
              }
           }
           else
              System.err.println("Usage:                  Summation          <integer   value>");  }
     }
              Figure 4.12      Java program for the summation of a non-negative integer.



                                  4.5  Implicit Threading                          179
            THE JVM AND THE HOST OPERATING SYSTEM
The JVM is typically implemented on top of a host operating system (see
Figure 16.10). This setup allows the JVM to hide the implementation details
of the underlying operating system and to provide a consistent, abstract
environment that allows Java programs to operate on any platform that
supports a JVM. The specification for the JVM does not indicate how Java
threads are to be mapped to the underlying operating system, instead leaving
that decision to the particular implementation of the JVM. For example, the
Windows XP operating system uses the one-to-one model; therefore, each
Java thread for a JVM running on such a system maps to a kernel thread. On
operating systems that use the many-to-many model (such as Tru64 UNIX), a
Java thread is mapped according to the many-to-many model. Solaris initially
implemented the JVM using the many-to-one model (the green threads library,
mentioned earlier). Later releases of the JVM were implemented using the
many-to-many model. Beginning with Solaris 9, Java threads were mapped
using the one-to-one model. In addition, there may be a relationship between
the Java thread library and the thread library on the host operating system.
For example, implementations of a JVM for the Windows family of operating
systems might use the Windows API when creating Java threads; Linux,
Solaris, and Mac OS X systems might use the Pthreads API.
threading from application developers to compilers and run-time libraries.
This strategy, termed implicit threading, is a popular trend today. In this
section, we explore three alternative approaches for designing multithreaded
programs that can take advantage of multicore processors through implicit
threading.
4.5.1  Thread Pools
In Section 4.1, we described a multithreaded web server. In this situation,
whenever the server receives a request, it creates a separate thread to service
the request. Whereas creating a separate thread is certainly superior to creating
a separate process, a multithreaded server nonetheless has potential problems.
The first issue concerns the amount of time required to create the thread,
together with the fact that the thread will be discarded once it has completed
its work. The second issue is more troublesome. If we allow all concurrent
requests to be serviced in a new thread, we have not placed a bound on the
number of threads concurrently active in the system. Unlimited threads could
exhaust system resources, such as CPU time or memory. One solution to this
problem is to use a thread pool.
The general idea behind a thread pool is to create a number of threads at
process startup and place them into a pool, where they sit and wait for work.
When a server receives a request, it awakens a thread from this pool--if one
is available --and passes it the request for service. Once the thread completes
its service, it returns to the pool and awaits more work. If the pool contains no
available thread, the server waits until one becomes free.



180  Chapter 4  Threads
         Thread pools offer these benefits:
     1.  Servicing a request with an existing thread is faster than waiting to create
         a thread.
     2.  A thread pool limits the number of threads that exist at any one point.
         This is particularly important on systems that cannot support a large
         number of concurrent threads.
     3.  Separating the task to be performed from the mechanics of creating the
         task allows us to use different strategies for running the task. For example,
         the task could be scheduled to execute after a time delay or to execute
         periodically.
         The number of threads in the pool can be set heuristically based on factors
     such as the number of CPUs in the system, the amount of physical memory,
     and the expected number of concurrent client requests. More sophisticated
     thread-pool architectures can dynamically adjust the number of threads in the
     pool according to usage patterns. Such architectures provide the further benefit
     of having a smaller pool--thereby consuming less memory--when the load
     on the system is low. We discuss one such architecture, Apple's Grand Central
     Dispatch, later in this section.
         The Windows API provides several functions related to thread pools. Using
     the thread pool API is similar to creating a thread with the Thread Create()
     function, as described in Section 4.4.2. Here, a function that is to run as a
     separate thread is defined. Such a function may appear as follows:
         DWORD          WINAPI  PoolFunction(AVOID        Param)  {
                /*
                *   this  function       runs  as  a    separate  thread.
                */
         }
     A pointer to PoolFunction() is passed to one of the functions in the thread
     pool API, and a thread from the pool executes this function. One such member
     in the thread pool API is the QueueUserWorkItem() function, which is passed
     three parameters:
     ·   LPTHREAD START ROUTINE        Function--a pointer to the function that is to
         run as a separate thread
     ·   PVOID  Param--the parameter passed to Function
     ·   ULONG  Flags -- flags  indicating   how   the  thread  pool  is  to  create   and
         manage execution of the thread
     An example of invoking a function is the following:
         QueueUserWorkItem(&PoolFunction,          NULL,  0);
     This causes a thread from the thread pool to invoke PoolFunction() on behalf
     of the programmer. In this instance, we pass no parameters to PoolFunc-



                                            4.5      Implicit Threading             181
tion(). Because we specify 0 as a flag, we provide the thread pool with no
special instructions for thread creation.
Other members in the Windows thread pool API include utilities that invoke
functions at periodic intervals or when an asynchronous I/O request completes.
The java.util.concurrent package in the Java API provides a thread-pool
utility as well.
4.5.2  OpenMP
OpenMP is a set of compiler directives as well as an API for programs written
in C, C++, or FORTRAN that provides support for parallel programming in
shared-memory environments. OpenMP identifies parallel regions as blocks
of code that may run in parallel. Application developers insert compiler
directives into their code at parallel regions, and these directives instruct the
OpenMP run-time library to execute the region in parallel. The following C
program illustrates a compiler directive above the parallel region containing
the printf() statement:
       #include        <omp.h>
       #include        <stdio.h>
       int        main(int  argc,  char    *argv[])
       {
          /*      sequential    code  */
          #pragma omp        parallel
          {
                  printf("I  am a parallel  region.");
          }
          /*      sequential    code  */
          return       0;
       }
When OpenMP encounters the directive
#pragma           omp  parallel
it creates as many threads are there are processing cores in the system. Thus, for
a dual-core system, two threads are created, for a quad-core system, four are
created; and so forth. All the threads then simultaneously execute the parallel
region. As each thread exits the parallel region, it is terminated.
OpenMP provides several additional directives for running code regions
in parallel, including parallelizing loops. For example, assume we have two
arrays a and b of size N. We wish to sum their contents and place the results
in array c. We can have this task run in parallel by using the following code
segment, which contains the compiler directive for parallelizing for loops:



182  Chapter 4     Threads
     #pragma            omp  parallel    for
     for       (i    =  0;   i  <    N;  i++)  {
            c[i]     =  a[i]    +    b[i];
     }
     OpenMP divides the work contained in the for loop among the threads it has
     created in response to the directive
          #pragma       omp  parallel       for
     In addition to providing directives for parallelization, OpenMP allows devel-
     opers to choose among several levels of parallelism. For example, they can set
     the number of threads manually. It also allows developers to identify whether
     data are shared between threads or are private to a thread. OpenMP is available
     on several open-source and commercial compilers for Linux, Windows, and
     Mac OS X systems. We encourage readers interested in learning more about
     OpenMP to consult the bibliography at the end of the chapter.
     4.5.3     Grand Central Dispatch
     Grand Central Dispatch (GCD)--a technology for Apple's Mac OS X and iOS
     operating systems--is a combination of extensions to the C language, an API,
     and a run-time library that allows application developers to identify sections
     of code to run in parallel. Like OpenMP, GCD manages most of the details of
     threading.
          GCD identifies extensions to the C and C++ languages known as blocks. A
     block is simply a self-contained unit of work. It is specified by a caret ^ inserted
     in front of a pair of braces { }. A simple example of a block is shown below:
          ^{   printf("I        am   a   block");   }
          GCD schedules blocks for run-time execution by placing them on a dispatch
     queue. When it removes a block from a queue, it assigns the block to an
     available thread from the thread pool it manages. GCD identifies two types of
     dispatch queues: serial and concurrent.
          Blocks placed on a serial queue are removed in FIFO order. Once a block has
     been removed from the queue, it must complete execution before another block
     is removed. Each process has its own serial queue (known as its main queue).
     Developers can create additional serial queues that are local to particular
     processes. Serial queues are useful for ensuring the sequential execution of
     several tasks.
          Blocks placed on a concurrent queue are also removed in FIFO order, but
     several blocks may be removed at a time, thus allowing multiple blocks to
     execute in parallel. There are three system-wide concurrent dispatch queues,
     and  they   are    distinguished    according     to  priority:  low,  default,  and    high.
     Priorities represent an approximation of the relative importance of blocks.
     Quite simply, blocks with a higher priority should be placed on the high-
     priority dispatch queue.
          The  following     code    segment   illustrates  obtaining       the  default-priority
     concurrent      queue      and  submitting     a  block  to      the   queue     using  the
     dispatch async() function:



                                                            4.6     Threading Issues        183
         dispatch queue t         queue     =  dispatch get global queue
            (DISPATCH     QUEUE   PRIORITY         DEFAULT,      0);
         dispatch async(queue,           ^{    printf("I        am   a  block.");      });
     Internally,   GCD's  thread  pool  is   composed   of  POSIX       threads.  GCD  actively
     manages      the  pool,  allowing  the    number   of  threads     to  grow  and  shrink
     according to application demand and system capacity.
     4.5.4     Other Approaches
     Thread pools, OpenMP, and Grand Central Dispatch are just a few of many
     emerging technologies for managing multithreaded applications. Other com-
     mercial approaches include parallel and concurrent libraries, such as Intel's
     Threading Building Blocks (TBB) and several products from Microsoft. The Java
     language and API have seen significant movement toward supporting concur-
     rent programming as well. A notable example is the java.util.concurrent
     package, which supports implicit thread creation and management.
4.6  Threading Issues
     In  this  section,   we  discuss   some   of  the  issues   to   consider    in  designing
     multithreaded programs.
     4.6.1     The fork() and exec() System Calls
     In Chapter 3, we described how the fork() system call is used to create a
     separate, duplicate process. The semantics of the fork() and exec() system
     calls change in a multithreaded program.
         If one thread in a program calls fork(), does the new process duplicate
     all threads, or is the new process single-threaded? Some UNIX systems have
     chosen to have two versions of fork(), one that duplicates all threads and
     another that duplicates only the thread that invoked the fork() system call.
         The exec() system call typically works in the same way as described
     in Chapter 3. That is, if a thread invokes the exec() system call, the program
     specified in the parameter to exec() will replace the entire process--including
     all threads.
         Which of the two versions of fork() to use depends on the application.
     If exec() is called immediately after forking, then duplicating all threads is
     unnecessary, as the program specified in the parameters to exec() will replace
     the process. In this instance, duplicating only the calling thread is appropriate.
     If, however, the separate process does not call exec() after forking, the separate
     process should duplicate all threads.
     4.6.2     Signal Handling
     A signal is used in UNIX systems to notify a process that a particular event has
     occurred. A signal may be received either synchronously or asynchronously,



184  Chapter 4    Threads
     depending on the source of and the reason for the event being signaled. All
     signals, whether synchronous or asynchronous, follow the same pattern:
     1.   A signal is generated by the occurrence of a particular event.
     2.   The signal is delivered to a process.
     3.   Once delivered, the signal must be handled.
          Examples of synchronous signal include illegal memory access and divi-
     sion by 0. If a running program performs either of these actions, a signal
     is generated. Synchronous signals are delivered to the same process that
     performed the operation that caused the signal (that is the reason they are
     considered synchronous).
          When a signal is generated by an event external to a running process, that
     process receives the signal asynchronously. Examples of such signals include
     terminating a process with specific keystrokes (such as <control><C>) and
     having a timer expire. Typically, an asynchronous signal is sent to another
     process.
          A signal may be handled by one of two possible handlers:
     1.   A default signal handler
     2.   A user-defined signal handler
          Every signal has a default signal handler that the kernel runs when
     handling that signal. This default action can be overridden by a user-defined
     signal handler that is called to handle the signal. Signals are handled in
     different ways. Some signals (such as changing the size of a window) are
     simply ignored; others (such as an illegal memory access) are handled by
     terminating the program.
          Handling signals in single-threaded programs is straightforward: signals
     are  always  delivered  to  a  process.  However,    delivering  signals  is  more
     complicated in multithreaded programs, where a process may have several
     threads. Where, then, should a signal be delivered?
          In general, the following options exist:
     1.   Deliver the signal to the thread to which the signal applies.
     2.   Deliver the signal to every thread in the process.
     3.   Deliver the signal to certain threads in the process.
     4.   Assign a specific thread to receive all signals for the process.
          The method for delivering a signal depends on the type of signal generated.
     For example, synchronous signals need to be delivered to the thread causing
     the signal and not to other threads in the process. However, the situation with
     asynchronous signals is not as clear. Some asynchronous signals--such as a
     signal that terminates a process (<control><C>, for example)--should be
     sent to all threads.



                                                    4.6  Threading Issues           185
    The standard UNIX function for delivering a signal is
       kill(pid  t  pid,      int  signal)
This function specifies the process (pid) to which a particular signal (signal) is
to be delivered. Most multithreaded versions of UNIX allow a thread to specify
which signals it will accept and which it will block. Therefore, in some cases,
an asynchronous signal may be delivered only to those threads that are not
blocking it. However, because signals need to be handled only once, a signal is
typically delivered only to the first thread found that is not blocking it. POSIX
Pthreads provides the following function, which allows a signal to be delivered
to a specified thread (tid):
       pthread kill(pthread t        tid,   int  signal)
    Although Windows does not explicitly provide support for signals, it
allows us to emulate them using asynchronous procedure calls (APCs). The
APC facility enables a user thread to specify a function that is to be called
when the user thread receives notification of a particular event. As indicated
by its name, an APC is roughly equivalent to an asynchronous signal in UNIX.
However, whereas UNIX must contend with how to deal with signals in a
multithreaded environment, the APC facility is more straightforward, since an
APC is delivered to a particular thread rather than a process.
4.6.3  Thread Cancellation
Thread cancellation involves terminating a thread before it has completed. For
example, if multiple threads are concurrently searching through a database and
one thread returns the result, the remaining threads might be canceled. Another
situation might occur when a user presses a button on a web browser that stops
a web page from loading any further. Often, a web page loads using several
threads--each image is loaded in a separate thread. When a user presses the
stop button on the browser, all threads loading the page are canceled.
    A thread that is to be canceled is often referred to as the target thread.
Cancellation of a target thread may occur in two different scenarios:
1.  Asynchronous      cancellation.  One    thread  immediately  terminates         the
    target thread.
2.  Deferred cancellation. The target thread periodically checks whether it
    should terminate, allowing it an opportunity to terminate itself in an
    orderly fashion.
    The difficulty with cancellation occurs in situations where resources have
been allocated to a canceled thread or where a thread is canceled while in
the midst of updating data it is sharing with other threads. This becomes
especially troublesome with asynchronous cancellation. Often, the operating
system will reclaim system resources from a canceled thread but will not
reclaim all resources. Therefore, canceling a thread asynchronously may not
free a necessary system-wide resource.



186  Chapter 4        Threads
     With deferred cancellation, in contrast, one thread indicates that a target
     thread is to be canceled, but cancellation occurs only after the target thread has
     checked a flag to determine whether or not it should be canceled. The thread
     can perform this check at a point at which it can be canceled safely.
     In Pthreads, thread cancellation is initiated using the pthread cancel()
     function.  The         identifier  of     the  target  thread  is  passed   as  a  parameter  to
     the function. The following code illustrates creating--and then canceling--
     a thread:
        pthread t                tid;
        /*         create         the   thread      */
        pthread create(&tid,                        0,   worker,    NULL);
        .       .     .
        /*         cancel         the   thread      */
        pthread cancel(tid);
     Invoking pthread cancel()indicates only a request to cancel the target
     thread, however; actual cancellation depends on how the target thread is set
     up to handle the request. Pthreads supports three cancellation modes. Each
     mode is defined as a state and a type, as illustrated in the table below. A thread
     may set its cancellation state and type using an API.
                         Mode                          State                     Type
                            Off                     Disabled                     ­
                      Deferred                      Enabled                  Deferred
                Asynchronous                        Enabled             Asynchronous
     As    the        table      illustrates,  Pthreads     allows  threads  to  disable  or  enable
     cancellation. Obviously, a thread cannot be canceled if cancellation is disabled.
     However, cancellation requests remain pending, so the thread can later enable
     cancellation and respond to the request.
     The default cancellation type is deferred cancellation. Here, cancellation
     occurs only when a thread reaches a cancellation point. One technique for
     establishing a cancellation point is to invoke the pthread testcancel()
     function. If a cancellation request is found to be pending, a function known
     as a cleanup handler is invoked. This function allows any resources a thread
     may have acquired to be released before the thread is terminated.
     The following code illustrates how a thread may respond to a cancellation
     request using deferred cancellation:
     while         (1)      {
        /*         do       some     work   for     awhile    */
        /*         .     .  .    */
        /*         check         if  there     is   a   cancellation         request    */
        pthread testcancel();
     }



                                                     4.6  Threading Issues         187
Because of the issues described earlier, asynchronous cancellation is not
recommended in Pthreads documentation. Thus, we do not cover it here. An
interesting note is that on Linux systems, thread cancellation using the Pthreads
API is handled through signals (Section 4.6.2).
4.6.4  Thread-Local Storage
Threads belonging to a process share the data of the process. Indeed, this
data sharing provides one of the benefits of multithreaded programming.
However, in some circumstances, each thread might need its own copy of
certain data. We will call such data thread-local storage (or TLS.) For example,
in a transaction-processing system, we might service each transaction in a
separate thread. Furthermore, each transaction might be assigned a unique
identifier. To associate each thread with its unique identifier, we could use
thread-local storage.
It is easy to confuse TLS with local variables. However, local variables
are visible only during a single function invocation, whereas TLS data are
visible across function invocations. In some ways, TLS is similar to static
data. The difference is that TLS data are unique to each thread. Most thread
libraries--including Windows and Pthreads--provide some form of support
for thread-local storage; Java provides support as well.
4.6.5  Scheduler Activations
A final issue to be considered with multithreaded programs concerns com-
munication between the kernel and the thread library, which may be required
by the many-to-many and two-level models discussed in Section 4.3.3. Such
coordination allows the number of kernel threads to be dynamically adjusted
to help ensure the best performance.
Many systems implementing either the many-to-many or the two-level
model place an intermediate data structure between the user and kernel
threads. This data structure --typically known as a lightweight process, or
LWP--is shown in Figure 4.13. To the user-thread library, the LWP appears to
be a virtual processor on which the application can schedule a user thread to
run. Each LWP is attached to a kernel thread, and it is kernel threads that the
                                      user thread
                       LWP            lightweight process
                       k              kernel thread
                       Figure 4.13  Lightweight process (LWP).



188  Chapter 4  Threads
     operating system schedules to run on physical processors. If a kernel thread
     blocks (such as while waiting for an I/O operation to complete), the LWP blocks
     as well. Up the chain, the user-level thread attached to the LWP also blocks.
     An application may require any number of LWPs to run efficiently. Consider
     a CPU-bound application running on a single processor. In this scenario, only
     one thread can run at at a time, so one LWP is sufficient. An application that is
     I/O-intensive may require multiple LWPs to execute, however. Typically, an LWP
     is required for each concurrent blocking system call. Suppose, for example, that
     five different file-read requests occur simultaneously. Five LWPs are needed,
     because all could be waiting for I/O completion in the kernel. If a process has
     only four LWPs, then the fifth request must wait for one of the LWPs to return
     from the kernel.
     One scheme for communication between the user-thread library and the
     kernel is known as scheduler activation. It works as follows: The kernel
     provides   an  application  with  a  set  of  virtual  processors  (LWPs),  and      the
     application can schedule user threads onto an available virtual processor.
     Furthermore, the kernel must inform an application about certain events. This
     procedure is known as an upcall. Upcalls are handled by the thread library
     with an upcall handler, and upcall handlers must run on a virtual processor.
     One event that triggers an upcall occurs when an application thread is about to
     block. In this scenario, the kernel makes an upcall to the application informing
     it that a thread is about to block and identifying the specific thread. The kernel
     then allocates a new virtual processor to the application. The application runs
     an upcall handler on this new virtual processor, which saves the state of the
     blocking thread and relinquishes the virtual processor on which the blocking
     thread is running. The upcall handler then schedules another thread that is
     eligible to run on the new virtual processor. When the event that the blocking
     thread was waiting for occurs, the kernel makes another upcall to the thread
     library informing it that the previously blocked thread is now eligible to run.
     The upcall handler for this event also requires a virtual processor, and the kernel
     may allocate a new virtual processor or preempt one of the user threads and
     run the upcall handler on its virtual processor. After marking the unblocked
     thread as eligible to run, the application schedules an eligible thread to run on
     an available virtual processor.
4.7  Operating-System Examples
     At this point, we have examined a number of concepts and issues related to
     threads. We conclude the chapter by exploring how threads are implemented
     in Windows and Linux systems.
     4.7.1  Windows Threads
     Windows implements the Windows API, which is the primary API for the
     family of Microsoft operating systems (Windows 98, NT, 2000, and XP, as well
     as Windows 7). Indeed, much of what is mentioned in this section applies to
     this entire family of operating systems.
     A Windows application runs as a separate process, and each process may
     contain one or more threads. The Windows API for creating threads is covered in



                                  4.7  Operating-System Examples                   189
Section 4.4.2. Additionally, Windows uses the one-to-one mapping described
in Section 4.3.2, where each user-level thread maps to an associated kernel
thread.
   The general components of a thread include:
·  A thread ID uniquely identifying the thread
·  A register set representing the status of the processor
·  A user stack, employed when the thread is running in user mode, and a
   kernel stack, employed when the thread is running in kernel mode
·  A private storage area used by various run-time libraries and dynamic link
   libraries (DLLs)
The register set, stacks, and private storage area are known as the context of
the thread.
   The primary data structures of a thread include:
·  ETHREAD --executive thread block
·  KTHREAD --kernel thread block
·  TEB--thread environment block
   The key components of the ETHREAD include a pointer to the process
to which the thread belongs and the address of the routine in which the
thread starts control. The ETHREAD also contains a pointer to the corresponding
KTHREAD.
   The KTHREAD includes scheduling and synchronization information for
the thread. In addition, the KTHREAD includes the kernel stack (used when the
thread is running in kernel mode) and a pointer to the TEB.
   The ETHREAD and the KTHREAD exist entirely in kernel space; this means
that only the kernel can access them. The TEB is a user-space data structure
that is accessed when the thread is running in user mode. Among other fields,
the TEB contains the thread identifier, a user-mode stack, and an array for
thread-local storage. The structure of a Windows thread is illustrated in Figure
4.14.
4.7.2    Linux Threads
Linux provides the fork() system call with the traditional functionality of
duplicating a process, as described in Chapter 3. Linux also provides the ability
to create threads using the clone() system call. However, Linux does not
distinguish between processes and threads. In fact, Linux uses the term task
--rather than process or thread-- when referring to a flow of control within a
program.
   When clone() is invoked, it is passed a set of flags that determine how
much sharing is to take place between the parent and child tasks. Some of these
flags are listed in Figure 4.15. For example, suppose that clone() is passed
the flags CLONE FS, CLONE VM, CLONE SIGHAND, and CLONE FILES. The parent
and child tasks will then share the same file-system information (such as the
current working directory), the same memory space, the same signal handlers,



190  Chapter 4  Threads
                   ETHREAD
                thread start
                   address
                pointer to
                parent process                    KTHREAD
                                                  scheduling
                                                  and
                                           synchronization
                   ·                              information
                   ·
                   ·                              kernel                TEB
                                                  stack
                                                                        thread identifier
                                                  ·                     user
                                                  ·                     stack
                                                  ·
                                                                        thread-local
                                                                        storage
                                                                               ·
                                                                               ·
                                                                               ·
                                   kernel  space                        user space
                   Figure 4.14             Data structures of a Windows thread.
     and the same set of open files. Using clone() in this fashion is equivalent to
     creating a thread as described in this chapter, since the parent task shares most
     of its resources with its child task. However, if none of these flags is set when
     clone() is invoked, no sharing takes place, resulting in functionality similar
     to that provided by the fork() system call.
     The varying level of sharing is possible because of the way a task is
     represented in the Linux kernel. A unique kernel data structure (specifically,
     struct  task  struct) exists for each task in the system. This data structure,
     instead of storing data for the task, contains pointers to other data structures
     where these data are stored --for example, data structures that represent the list
     of open files, signal-handling information, and virtual memory. When fork()
     is invoked, a new task is created, along with a copy of all the associated data
                             flag                              meaning
                   CLONE_FS                File-system information is shared.
                   CLONE_VM                The same memory space is shared.
                   CLONE_SIGHAND                  Signal handlers are shared.
                   CLONE_FILES                    The set of open files is shared.
                Figure 4.15  Some of the flags passed when clone() is invoked.



                                                       Practice Exercises               191
     structures of the parent process. A new task is also created when the clone()
     system call is made. However, rather than copying all data structures, the new
     task points to the data structures of the parent task, depending on the set of
     flags passed to clone().
4.8  Summary
     A thread is a flow of control within a process. A multithreaded process contains
     several different flows of control within the same address space. The benefits of
     multithreading include increased responsiveness to the user, resource sharing
     within the process, economy, and scalability factors, such as more efficient use
     of multiple processing cores.
          User-level threads are threads that are visible to the programmer and are
     unknown to the kernel. The operating-system kernel supports and manages
     kernel-level threads. In general, user-level threads are faster to create and
     manage than are kernel threads, because no intervention from the kernel is
     required.
          Three different types of models relate user and kernel threads. The many-
     to-one model maps many user threads to a single kernel thread. The one-to-one
     model maps each user thread to a corresponding kernel thread. The many-to-
     many model multiplexes many user threads to a smaller or equal number of
     kernel threads.
          Most modern operating systems provide kernel support for threads. These
     include Windows, Mac OS X, Linux, and Solaris.
          Thread libraries provide the application programmer with an API for
     creating and managing threads. Three primary thread libraries are in common
     use: POSIX Pthreads, Windows threads, and Java threads.
          In addition to explicitly creating threads using the API provided by a
     library, we can use implicit threading, in which the creation and management
     of threading is transferred to compilers and run-time libraries. Strategies for
     implicit threading include thread pools, OpenMP, and Grand Central Dispatch.
          Multithreaded programs introduce many challenges for programmers,
     including  the   semantics  of  the  fork()  and  exec()  system  calls.  Other
     issues include signal handling, thread cancellation, thread-local storage, and
     scheduler activations.
Practice  Exercises
     4.1  Provide two programming examples in which multithreading provides
          better performance than a single-threaded solution.
     4.2  What are two differences between user-level threads and kernel-level
          threads? Under what circumstances is one type better than the other?
     4.3  Describe the actions taken by a kernel to context-switch between kernel-
          level threads.
     4.4  What resources are used when a thread is created? How do they differ
          from those used when a process is created?



192  Chapter 4  Threads
     4.5   Assume that an operating system maps user-level threads to the kernel
           using the many-to-many model and that the mapping is done through
           LWPs. Furthermore, the system allows developers to create real-time
           threads for use in real-time systems. Is it necessary to bind a real-time
           thread to an LWP? Explain.
Exercises
     4.6   Provide two programming examples in which multithreading does not
           provide better performance than a single-threaded solution.
     4.7   Under what circumstances does a multithreaded solution using multi-
           ple kernel threads provide better performance than a single-threaded
           solution on a single-processor system?
     4.8   Which of the following components of program state are shared across
           threads in a multithreaded process?
           a.   Register values
           b.   Heap memory
           c.   Global variables
           d.   Stack memory
     4.9   Can a multithreaded solution using multiple user-level threads achieve
           better  performance     on  a  multiprocessor  system  than  on   a  single-
           processor system? Explain.
     4.10  In Chapter 3, we discussed Google's Chrome browser and its practice
           of opening each new website in a separate process. Would the same
           benefits have been achieved if instead Chrome had been designed to
           open each new website in a separate thread? Explain.
     4.11  Is it possible to have concurrency but not parallelism? Explain.
     4.12  Using Amdahl's Law, calculate the speedup gain of an application that
           has a 60 percent parallel component for (a) two processing cores and (b)
           four processing cores.
     4.13  Determine if the following problems exhibit task or data parallelism:
           ·    The multithreaded statistical program described in Exercise 4.21
           ·    The multithreaded Sudoku validator described in Project 1 in this
                chapter
           ·    The multithreaded sorting program described in Project 2 in this
                chapter
           ·    The multithreaded web server described in Section 4.1
     4.14  A system with two dual-core processors has four processors available
           for scheduling. A CPU-intensive application is running on this system.
           All input is performed at program start-up, when a single file must
           be opened. Similarly, all output is performed just before the program



                                                                     Exercises       193
      terminates,   when    the  program        results  must  be   written   to  a  single
      file. Between startup and termination, the program is entirely CPU-
      bound. Your task is to improve the performance of this application
      by multithreading it. The application runs on a system that uses the
      one-to-one threading model (each user thread maps to a kernel thread).
      ·   How many threads will you create to perform the input and output?
          Explain.
      ·   How many threads will you create for the CPU-intensive portion of
          the application? Explain.
4.15  Consider the following code segment:
                pid t      pid;
                pid     =   fork();
                if   (pid   ==   0)  {  /*      child    process     */
                    fork();
                    thread create(           .  .  .);
                }
                fork();
      a.  How many unique processes are created?
      b.  How many unique threads are created?
4.16  As  described     in  Section  4.7.2,  Linux       does  not  distinguish   between
      processes and threads. Instead, Linux treats both in the same way,
      allowing a task to be more akin to a process or a thread depending on the
      set of flags passed to the clone() system call. However, other operating
      systems,    such  as  Windows,    treat      processes   and   threads  differently.
      Typically, such systems use a notation in which the data structure for
      a process contains pointers to the separate threads belonging to the
      process. Contrast these two approaches for modeling processes and
      threads within the kernel.
4.17  The program shown in Figure 4.16 uses the Pthreads API. What would
      be the output from the program at LINE             C and LINE  P?
4.18  Consider a multicore system and a multithreaded program written
      using the many-to-many threading model. Let the number of user-level
      threads in the program be greater than the number of processing cores
      in the system. Discuss the performance implications of the following
      scenarios.
      a.  The number of kernel threads allocated to the program is less than
          the number of processing cores.
      b.  The number of kernel threads allocated to the program is equal to
          the number of processing cores.
      c.  The number of kernel threads allocated to the program is greater
          than the number of processing cores but less than the number of
          user-level threads.



194  Chapter 4    Threads
           #include   <pthread.h>
           #include   <stdio.h>
           #include <types.h>
           int value = 0;
           void   *runner(void         *param);      /*  the  thread     */
           int   main(int    argc,      char  *argv[])
           {
           pid t pid;
           pthread t tid;
           pthread attr      t  attr;
              pid  =  fork();
              if   (pid  ==     0)  {  /*  child     process   */
                  pthread attr init(&attr);
                  pthread create(&tid,&attr,runner,NULL);
                  pthread join(tid,NULL);
                  printf("CHILD:        value     =     %d",value);   /*     LINE  C     */
              }
              else if (pid > 0) { /* parent process */
                  wait(NULL);
                  printf("PARENT:          value     =   %d",value);     /*  LINE     P  */
              }
           }
           void   *runner(void         *param)    {
              value   =  5;
              pthread exit(0);
           }
                           Figure 4.16     C program for Exercise 4.17.
     4.19  Pthreads   provides      an  API   for    managing    thread      cancellation.   The
           pthread setcancelstate() function is used to set the cancellation
           state. Its prototype appears as follows:
                 pthread setcancelstate(int              state,    int   *oldstate)
           The two possible values for the state are PTHREAD CANCEL ENABLE and
           PTHREAD CANCEL DISABLE.
                 Using the code segment shown in Figure 4.17, provide examples of
           two operations that would be suitable to perform between the calls to
           disable and enable thread cancellation.



                                                 Programming Problems              195
int oldstate;
pthread setcancelstate(PTHREAD CANCEL DISABLE, &oldstate);
/*    What  operations      would  be    performed  here?     */
pthread setcancelstate(PTHREAD CANCEL               ENABLE,   &oldstate);
                   Figure 4.17  C program for Exercise 4.19.
Programming Problems
4.20  Modify programming problem Exercise 3.20 from Chapter 3, which asks
      you to design a pid manager. This modification will consist of writing
      a multithreaded program that tests your solution to Exercise 3.20. You
      will create a number of threads--for example, 100--and each thread will
      request a pid, sleep for a random period of time, and then release the pid.
      (Sleeping for a random period of time approximates the typical pid usage
      in which a pid is assigned to a new process, the process executes and
      then terminates, and the pid is released on the process's termination.) On
      UNIX and Linux systems, sleeping is accomplished through the sleep()
      function, which is passed an integer value representing the number of
      seconds to sleep. This problem will be modified in Chapter 5.
4.21  Write a multithreaded program that calculates various statistical values
      for a list of numbers. This program will be passed a series of numbers on
      the command line and will then create three separate worker threads.
      One thread will determine the average of the numbers, the second
      will determine the maximum value, and the third will determine the
      minimum value. For example, suppose your program is passed the
      integers
                   90 81 78 95 79 72 85
      The program will report
            The    average  value        is  82
            The    minimum  value        is  72
            The    maximum  value        is  95
      The variables representing the average, minimum, and maximum values
      will be stored globally. The worker threads will set these values, and the
      parent thread will output the values once the workers have exited. (We
      could obviously expand this program by creating additional threads
      that determine other statistical values, such as median and standard
      deviation.)
4.22  An interesting way of calculating  is to use a technique known as Monte
      Carlo, which involves randomization. This technique works as follows:
      Suppose you have a circle inscribed within a square, as shown in Figure



196  Chapter 4    Threads
                     (-1, 1)                                    (1, 1)
                                               (0, 0)
                     (-1, -1)                                   (1, -1)
                     Figure 4.18  Monte Carlo technique for calculating pi.
           4.18. (Assume that the radius of this circle is 1.) First, generate a series of
           random points as simple (x, y) coordinates. These points must fall within
           the Cartesian coordinates that bound the square. Of the total number of
           random points that are generated, some will occur within the circle.
           Next, estimate  by performing the following calculation:
                   = 4× (number of points in circle) / (total number of points)
           Write a multithreaded version of this algorithm that creates a separate
           thread to generate a number of random points. The thread will count
           the number of points that occur within the circle and store that result
           in a global variable. When this thread has exited, the parent thread will
           calculate and output the estimated value of . It is worth experimenting
           with the number of random points generated. As a general rule, the
           greater the number of points, the closer the approximation to .
           In the source-code download for this text, we provide a sample program
           that provides a technique for generating random numbers, as well as
           determining if the random (x, y) point occurs within the circle.
           Readers interested in the details of the Monte Carlo method for esti-
           mating  should consult the bibliography at the end of this chapter. In
           Chapter 5, we modify this exercise using relevant material from that
           chapter.
     4.23  Repeat Exercise 4.22, but instead of using a separate thread to generate
           random points, use OpenMP to parallelize the generation of points. Be
           careful not to place the calculcation of  in the parallel region, since you
           want to calculcate  only once.
     4.24  Write  a  multithreaded  program    that    outputs  prime        numbers.  This
           program should work as follows: The user will run the program and
           will enter a number on the command line. The program will then create
           a separate thread that outputs all the prime numbers less than or equal
           to the number entered by the user.
     4.25  Modify the socket-based date server (Figure 3.21) in Chapter 3 so that
           the server services each client request in a separate thread.



                                                             Programming Projects       197
4.26  The  Fibonacci  sequence  is      the  series  of      numbers  0, 1, 1, 2, 3, 5, 8, ....
      Formally, it can be expressed as:
                    f ib0  =0
                    f ib1  =1
                    f ibn  =  f i bn-1  +  f i bn-2
      Write a multithreaded program that generates the Fibonacci sequence.
      This program should work as follows: On the command line, the user
      will enter the number of Fibonacci numbers that the program is to
      generate.  The  program   will       then      create  a  separate  thread  that  will
      generate the Fibonacci numbers, placing the sequence in data that can
      be shared by the threads (an array is probably the most convenient
      data structure). When the thread finishes execution, the parent thread
      will output the sequence generated by the child thread. Because the
      parent thread cannot begin outputting the Fibonacci sequence until the
      child thread finishes, the parent thread will have to wait for the child
      thread to finish. Use the techniques described in Section 4.4 to meet this
      requirement.
4.27  Exercise 3.25 in Chapter 3 involves designing an echo server using the
      Java threading API. This server is single-threaded, meaning that the
      server cannot respond to concurrent echo clients until the current client
      exits. Modify the solution to Exercise 3.25 so that the echo server services
      each client in a separate request.
Programming Projects
Project 1 --Sudoku Solution Validator
A Sudoku puzzle uses a 9 × 9 grid in which each column and row, as well as
each of the nine 3 × 3 subgrids, must contain all of the digits 1 · · · 9. Figure
4.19 presents an example of a valid Sudoku puzzle. This project consists of
designing a multithreaded application that determines whether the solution to
a Sudoku puzzle is valid.
      There are several different ways of multithreading this application. One
suggested strategy is to create threads that check the following criteria:
·     A thread to check that each column contains the digits 1 through 9
·     A thread to check that each row contains the digits 1 through 9
·     Nine threads to check that each of the 3 × 3 subgrids contains the digits 1
      through 9
      This would result in a total of eleven separate threads for validating a
Sudoku puzzle. However, you are welcome to create even more threads for
this project. For example, rather than creating one thread that checks all nine



198  Chapter 4     Threads
                               6       2  4    5  3  9   1   8     7
                               5       1  9    7  2  8   6   3     4
                               8       3  7    6  1  4   2   9     5
                               1       4  3    8  6  5   7   2     9
                               9       5  8    2  4  7   3   6     1
                               7       6  2    3  9  1   4   5     8
                               3       7  1    9  5  6   8   4     2
                               4       9  6    1  8  2   5   7     3
                               2       8  5    4  7  3   9   1     6
                   Figure 4.19            Solution to a 9 × 9 Sudoku puzzle.
     columns, you could create nine separate threads and have each of them check
     one column.
     Passing Parameters to Each Thread
     The parent thread will create the worker threads, passing each worker the
     location that it must check in the Sudoku grid. This step will require passing
     several parameters to each thread. The easiest approach is to create a data
     structure using a struct. For example, a structure to pass the row and column
     where a thread must begin validating would appear as follows:
                /* structure for          passing    data    to  threads      */
                typedef     struct
                {
                   int row;
                   int column;
                } parameters;
     Both Pthreads and Windows programs will create worker threads using a
     strategy similar to that shown below:
     parameters    *data            =  (parameters   *)  malloc(sizeof(parameters));
     data->row = 1;
     data->column           =  1;
     /*  Now       create      the     thread  passing   it  data     as  a   parameter  */
     The data pointer will be passed to either the pthread create() (Pthreads)
     function or the CreateThread() (Windows) function, which in turn will pass
     it as a parameter to the function that is to run as a separate thread.
     Returning Results to the Parent Thread
     Each worker thread is assigned the task of determining the validity of a
     particular region of the Sudoku puzzle. Once a worker has performed this



                                                                    Bibliographical    Notes         199
                                         Original List
                                7,  12,  19, 3, 18, 4, 2,  6,  15,  8
               Sorting                                                     Sorting
               Thread0                                                     Thread1
               7, 12, 19,  3,  18                                      4, 2, 6, 15, 8
                                         Merge Thread
                                2, 3, 4, 6, 7, 8, 12, 15, 18, 19
                                         Sorted List
                           Figure 4.20   Multithreaded sorting.
check, it must pass its results back to the parent. One good way to handle this
is to create an array of integer values that is visible to each thread. The ith
index in this array corresponds to the ith worker thread. If a worker sets its
corresponding value to 1, it is indicating that its region of the Sudoku puzzle
is valid. A value of 0 would indicate otherwise. When all worker threads have
completed, the parent thread checks each entry in the result array to determine
if the Sudoku puzzle is valid.
Project 2 --Multithreaded Sorting Application
Write a multithreaded sorting program that works as follows: A list of integers
is divided into two smaller lists of equal size. Two separate threads (which we
will term sorting threads) sort each sublist using a sorting algorithm of your
choice. The two sublists are then merged by a third thread--a merging thread
--which merges the two sublists into a single sorted list.
Because global data are shared cross all threads, perhaps the easiest way
to set up the data is to create a global array. Each sorting thread will work on
one half of this array. A second global array of the same size as the unsorted
integer array will also be established. The merging thread will then merge
the two sublists into this second array. Graphically, this program is structured
according to Figure 4.20.
This programming project will require passing parameters to each of the
sorting threads. In particular, it will be necessary to identify the starting index
from which each thread is to begin sorting. Refer to the instructions in Project
1 for details on passing parameters to a thread.
The parent thread will output the sorted array once all sorting threads have
exited.
Bibliographical Notes
Threads  have  had  a  long     evolution,  starting                   as  "cheap      concurrency"  in
programming languages and moving to "lightweight processes," with early
examples that included the Thoth system ([Cheriton et al. (1979)]) and the Pilot



200  Chapter 4  Threads
     system ([Redell et al. (1980)]). [Binding (1985)] described moving threads into
     the UNIX kernel. Mach ([Accetta et al. (1986)], [Tevanian et al. (1987)]), and V
     ([Cheriton (1988)]) made extensive use of threads, and eventually almost all
     major operating systems implemented them in some form or another.
     [Vahalia (1996)] covers threading in several versions of UNIX. [McDougall
     and Mauro (2007)] describes developments in threading the Solaris kernel.
     [Russinovich and Solomon (2009)] discuss threading in the Windows operating
     system family. [Mauerer (2008)] and [Love (2010)] explain how Linux handles
     threading, and [Singh (2007)] covers threads in Mac OS X.
     Information         on    Pthreads      programming  is   given   in   [Lewis    and  Berg
     (1998)]  and  [Butenhof       (1997)].  [Oaks   and  Wong    (1999)]   and  [Lewis    and
     Berg (2000)] discuss multithreading in Java. [Goetz et al. (2006)] present a
     detailed discussion of concurrent programming in Java. [Hart (2005)] describes
     multithreading using Windows. Details on using OpenMP can be found at
     http://openmp.org.
     An analysis of an optimal thread-pool size can be found in [Ling et al.
     (2000)]. Scheduler activations were first presented in [Anderson et al. (1991)],
     and [Williams (2002)] discusses scheduler activations in the NetBSD system.
     [Breshears (2009)] and [Pacheco (2011)] cover parallel programming in
     detail. [Hill and Marty (2008)] examine Amdahl's Law with respect to multicore
     systems. The Monte Carlo technique for estimating  is further discussed in
     http://math.fullerton.edu/mathews/n2003/montecarlopimod.html.
Bibliography
     [Accetta et al. (1986)]      M. Accetta, R. Baron, W. Bolosky, D. B. Golub, R. Rashid,
     A.  Tevanian,      and    M.  Young,    "Mach:  A    New  Kernel  Foundation     for  UNIX
     Development",       Proceedings     of  the  Summer  USENIX  Conference     (1986),   pages
     93­112.
     [Anderson et al. (1991)]      T. E. Anderson, B. N. Bershad, E. D. Lazowska, and
     H. M. Levy, "Scheduler Activations: Effective Kernel Support for the User-Level
     Management of Parallelism", Proceedings of the ACM Symposium on Operating
     Systems Principles (1991), pages 95­109.
     [Binding (1985)]    C. Binding, "Cheap Concurrency           in   C",  SIGPLAN   Notices,
     Volume 20, Number 9 (1985), pages 21­27.
     [Breshears (2009)]       C. Breshears, The Art of Concurrency, O'Reilly & Associates
     (2009).
     [Butenhof (1997)]        D.  Butenhof,  Programming  with  POSIX       Threads,  Addison-
     Wesley (1997).
     [Cheriton (1988)]        D. Cheriton, "The V Distributed System", Communications of
     the ACM, Volume 31, Number 3 (1988), pages 314­333.
     [Cheriton et al. (1979)]      D. R. Cheriton, M. A. Malcolm, L. S. Melen, and G. R.
     Sager, "Thoth, a Portable Real-Time Operating System", Communications of the
     ACM, Volume 22, Number 2 (1979), pages 105­115.



                                                                 Bibliography           201
[Goetz et al. (2006)]      B. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and
D. Lea, Java Concurrency in Practice, Addison-Wesley (2006).
[Hart (2005)]     J. M. Hart, Windows System Programming, Third Edition, Addison-
Wesley (2005).
[Hill and Marty (2008)]    M. Hill and M. Marty, "Amdahl's Law in the Multicore
Era", IEEE Computer, Volume 41, Number 7 (2008), pages 33­38.
[Lewis and Berg (1998)]    B. Lewis and D. Berg, Multithreaded Programming with
Pthreads, Sun Microsystems Press (1998).
[Lewis and Berg (2000)]    B. Lewis and D. Berg, Multithreaded Programming with
Java Technology, Sun Microsystems Press (2000).
[Ling et al. (2000)]       Y. Ling, T. Mullen, and X. Lin, "Analysis of Optimal Thread
Pool Size", Operating System Review, Volume 34, Number 2 (2000), pages 42­55.
[Love (2010)]     R. Love, Linux Kernel Development, Third Edition, Developer's
Library (2010).
[Mauerer (2008)]        W. Mauerer, Professional Linux Kernel Architecture, John Wiley
and Sons (2008).
[McDougall and Mauro (2007)]         R. McDougall and J. Mauro, Solaris Internals,
Second Edition, Prentice Hall (2007).
[Oaks and Wong (1999)]     S. Oaks and H. Wong, Java Threads, Second Edition,
O'Reilly & Associates (1999).
[Pacheco (2011)]        P. S. Pacheco, An Introduction to Parallel Programming, Morgan
Kaufmann (2011).
[Redell et al. (1980)]     D. D. Redell, Y. K. Dalal, T. R. Horsley, H. C. Lauer, W. C.
Lynch, P. R. McJones, H. G. Murray, and S. P. Purcell, "Pilot: An Operating System
for a Personal Computer", Communications of the ACM, Volume 23, Number 2
(1980), pages 81­92.
[Russinovich and Solomon (2009)]           M. E. Russinovich and D. A. Solomon, Win-
dows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,
Microsoft Press (2009).
[Singh (2007)]        A. Singh, Mac OS X Internals: A Systems Approach, Addison-
Wesley (2007).
[Tevanian et al. (1987)]   A. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black,
E. Cooper, and M. W. Young, "Mach Threads and the Unix Kernel: The Battle
for Control", Proceedings of the Summer USENIX Conference (1987).
[Vahalia (1996)]       U.  Vahalia,  Unix  Internals:  The  New  Frontiers,  Prentice  Hall
(1996).
[Williams (2002)]       N. Williams, "An Implementation of Scheduler Activations
on the NetBSD Operating System", 2002 USENIX Annual Technical Conference,
FREENIX Track (2002).



