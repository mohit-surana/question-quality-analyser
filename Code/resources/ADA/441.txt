Approximation Algorithms for NP -Hard Problems
In this section, we discuss a different approach to handling difficult problems
      of combinatorial optimization, such as the traveling salesman problem and the
      knapsack problem. As we pointed out in Section 11.3, the decision versions of
      these problems are NP-complete. Their optimization versions fall in the class of
      NP-hard problems--problems that are at least as hard as NP-complete problems.2
      Hence, there are no known polynomial-time algorithms for these problems, and
      there are serious theoretical reasons to believe that such algorithms do not exist.
      What then are our options for handling such problems, many of which are of
      significant practical importance?
2.    The notion of an NP-hard problem can be defined more formally by extending the notion of polynomial
      reducibility to problems that are not necessarily in class NP, including optimization problems of the
      type discussed in this section (see [Gar79, Chapter 5]).
     If an instance of the problem in question is very small, we might be able to
     solve it by an exhaustive-search algorithm (Section 3.4). Some such problems can
     be solved by the dynamic programming technique we demonstrated in Section 8.2.
     But even when this approach works in principle, its practicality is limited by
     dependence on the instance parameters being relatively small. The discovery of
     the branch-and-bound technique has proved to be an important breakthrough,
     because this technique makes it possible to solve many large instances of difficult
     optimization problems in an acceptable amount of time. However, such good
     performance cannot usually be guaranteed.
     There is a radically different way of dealing with difficult optimization prob-
     lems: solve them approximately by a fast algorithm. This approach is particularly
     appealing for applications where a good but not necessarily optimal solution will
     suffice. Besides, in real-life applications, we often have to operate with inaccurate
     data to begin with. Under such circumstances, going for an approximate solution
     can be a particularly sensible choice.
     Although approximation algorithms run a gamut in level of sophistication,
     most of them are based on some problem-specific heuristic. A heuristic is a
     common-sense rule drawn from experience rather than from a mathematically
     proved assertion. For example, going to the nearest unvisited city in the traveling
     salesman problem is a good illustration of this notion. We discuss an algorithm
     based on this heuristic later in this section.
     Of course, if we use an algorithm whose output is just an approximation of the
     actual optimal solution, we would like to know how accurate this approximation
     is. We can quantify the accuracy of an approximate solution sa to a problem of
     minimizing some function f by the size of the relative error of this approximation,
               r e(sa )                      =  f  (sa)  -f   (s)  ,
                                                      f  (s)
     where s is an exact solution to the problem. Alternatively, since re(sa) = f (sa)/
     f (s) - 1, we can simply use the accuracy ratio
               r (sa )                             =  f (sa)
                                                      f (s)
     as a measure of accuracy of sa. Note that for the sake of scale uniformity, the
     accuracy ratio of approximate solutions to maximization problems is usually com-
     puted as
               r (sa )                             =  f (s)
                                                      f (sa)
     to make this ratio greater than or equal to 1, as it is for minimization problems.
     Obviously, the closer r(sa) is to 1, the better the approximate solution is.
     For most instances, however, we cannot compute the accuracy ratio, because we
     typically do not know f (s), the true optimal value of the objective function.
     Therefore, our hope should lie in obtaining a good upper bound on the values
     of r(sa). This leads to the following definitions.
DEFINITION         A polynomial-time approximation algorithm is said to be a c-
approximation algorithm, where c  1, if the accuracy ratio of the approximation
it produces does not exceed c for any instance of the problem in question:
                      r(sa)  c.                                             (12.3)
The best (i.e., the smallest) value of c for which inequality (12.3) holds for all
instances of the problem is called the performance ratio of the algorithm and
denoted RA.
The performance ratio serves as the principal metric indicating the quality of
the approximation algorithm. We would like to have approximation algorithms
with RA as close to 1 as possible. Unfortunately, as we shall see, some approxima-
tion algorithms have infinitely large performance ratios (RA = ). This does not
necessarily rule out using such algorithms, but it does call for a cautious treatment
of their outputs.
There are two important facts about difficult combinatorial optimization
problems worth keeping in mind. First, although the difficulty level of solving
most such problems exactly is the same to within a polynomial-time transforma-
tion of one problem to another, this equivalence does not translate into the realm
of approximation algorithms. Finding good approximate solutions is much easier
for some of these problems than for others. Second, some of the problems have
special classes of instances that are both particularly important for real-life appli-
cations and easier to solve than their general counterparts. The traveling salesman
problem is a prime example of this situation.
Approximation Algorithms for the Traveling
Salesman Problem
We solved the traveling salesman problem by exhaustive search in Section 3.4,
mentioned its decision version as one of the most well-known NP-complete
problems in Section 11.3, and saw how its instances can be solved by a branch-
and-bound algorithm in Section 12.2. Here, we consider several approximation
algorithms, a small sample of dozens of such algorithms suggested over the years
for this famous problem. (For a much more detailed discussion of the topic, see
[Law85], [Hoc97], [App07], and [Gut07].)
But first let us answer the question of whether we should hope to find a
polynomial-time approximation algorithm with a finite performance ratio on all
instances of the traveling salesman problem. As the following theorem [Sah76]
shows, the answer turns out to be no, unless P = N P .
THEOREM 1          If P = NP, there exists no c-approximation algorithm for the
traveling salesman problem, i.e., there exists no polynomial-time approximation
algorithm for this problem so that for all instances
                      f (sa)  cf (s)
for some constant c.
     PROOF        By way of contradiction, suppose that such an approximation algorithm
     A and a constant c exist. (Without loss of generality, we can assume that c is a
     positive integer.) We will show that this algorithm could then be used for solving
     the Hamiltonian circuit problem in polynomial time. We will take advantage of
     a variation of the transformation used in Section 11.3 to reduce the Hamiltonian
     circuit problem to the traveling salesman problem. Let G be an arbitrary graph
     with n vertices. We map G to a complete weighted graph G by assigning weight 1 to
     each edge in G and adding an edge of weight cn + 1 between each pair of vertices
     not adjacent in G. If G has a Hamiltonian circuit, its length in G is n; hence, it is
     the exact solution s to the traveling salesman problem for G . Note that if sa is
     an approximate solution obtained for G           by algorithm A, then f (sa)  cn by the
     assumption. If G does not have a Hamiltonian circuit in G, the shortest tour in
     G will contain at least one edge of weight cn + 1, and hence f (sa)  f (s) > cn.
     Taking into account the two derived inequalities, we could solve the Hamiltonian
     circuit problem for graph G in polynomial time by mapping G to G , applying
     algorithm A to get tour sa in G , and comparing its length with cn. Since the
     Hamiltonian circuit problem is NP-complete, we have a contradiction unless P =
     NP.
     Greedy Algorithms for the TSP     The simplest approximation algorithms for the
     traveling salesman problem are based on the greedy technique. We will discuss
     here two such algorithms.
     Nearest-neighbor algorithm
     The following well-known greedy algorithm is based on the nearest-neighbor
     heuristic: always go next to the nearest unvisited city.
          Step 1  Choose an arbitrary city as the start.
          Step 2  Repeat the following operation until all the cities have been visited:
                  go to the unvisited city nearest the one visited last (ties can be broken
                  arbitrarily).
          Step 3  Return to the starting city.
     EXAMPLE 1     For the instance represented by the graph in Figure 12.10, with a as
     the starting vertex, the nearest-neighbor algorithm yields the tour (Hamiltonian
     circuit) sa: a - b - c - d - a of length 10.
                                    a              1      b
                                 6                           2
                                       3              3
                                    d                     c
                                                   1
     FIGURE 12.10  Instance of the traveling salesman problem.
The optimal solution, as can be easily checked by exhaustive search, is the tour
s: a - b - d - c - a of length 8. Thus, the accuracy ratio of this approximation is
                    r (sa )    =        f (sa)  =   10    =  1.25
                                        f (s)          8
(i.e., tour sa is 25% longer than the optimal tour s).
Unfortunately, except for its simplicity, not many good things can be said
about the nearest-neighbor algorithm. In particular, nothing can be said in general
about the accuracy of solutions obtained by this algorithm because it can force us
to traverse a very long edge on the last leg of the tour. Indeed, if we change the
weight of edge (a, d) from 6 to an arbitrary large number w  6 in Example 1,
the algorithm will still yield the tour a - b - c - d - a of length 4 + w, and the
optimal solution will still be a - b - d - c - a of length 8. Hence,
                               r (sa )  =  f  (sa)  =  4  +  w  ,
                                           f  (s)         8
which can be made as large as we wish by choosing an appropriately large value
of w. Hence, RA =  for this algorithm (as it should be according to Theorem 1).
Multifragment-heuristic algorithm
Another natural greedy algorithm for the traveling salesman problem considers
it as the problem of finding a minimum-weight collection of edges in a given
complete weighted graph so that all the vertices have degree 2. (With this emphasis
on edges rather than vertices, what other greedy algorithm does it remind you
of?) An application of the greedy technique to this problem leads to the following
algorithm [Ben90].
Step 1  Sort the edges in increasing order of their weights. (Ties can be broken
        arbitrarily.) Initialize the set of tour edges to be constructed to the
        empty set.
Step 2  Repeat this step n times, where n is the number of cities in the instance
        being solved: add the next edge on the sorted edge list to the set of tour
        edges, provided this addition does not create a vertex of degree 3 or a
        cycle of length less than n; otherwise, skip the edge.
Step 3  Return the set of tour edges.
As an example, applying the algorithm to the graph in Figure 12.10 yields
{(a, b), (c, d), (b, c), (a, d)}. This set of edges forms the same tour as the one pro-
duced by the nearest-neighbor algorithm. In general, the multifragment-heuristic
algorithm tends to produce significantly better tours than the nearest-neighbor
algorithm, as we are going to see from the experimental data quoted at the end of
this section. But the performance ratio of the multifragment-heuristic algorithm
is also unbounded, of course.
     There is, however, a very important subset of instances, called Euclidean, for
     which we can make a nontrivial assertion about the accuracy of both the nearest-
     neighbor and multifragment-heuristic algorithms. These are the instances in which
     intercity distances satisfy the following natural conditions:
     triangle inequality d[i, j ]  d[i, k] + d[k, j ]    for any triple of cities i, j, and
     k (the distance between cities i and j cannot exceed the length of a two-leg
     path from i to some intermediate city k to j )
     symmetry d[i, j ] = d[j, i]         for any pair of cities i and j (the distance from i
     to j is the same as the distance from j to i)
     A substantial majority of practical applications of the traveling salesman prob-
     lem are its Euclidean instances. They include, in particular, geometric ones, where
     cities correspond to points in the plane and distances are computed by the standard
     Euclidean formula. Although the performance ratios of the nearest-neighbor and
     multifragment-heuristic algorithms remain unbounded for Euclidean instances,
     their accuracy ratios satisfy the following inequality for any such instance with
     n  2 cities:
                                  f (sa)    1(   log2 n  + 1),
                                  f (s)     2
     where f (sa) and f (s) are the lengths of the heuristic tour and shortest tour,
     respectively (see [Ros77] and [Ong84]).
     Minimum-Spanning-Tree­Based Algorithms There are approximation algori-
     thms for the traveling salesman problem that exploit a connection between Hamil-
     tonian circuits and spanning trees of the same graph. Since removing an edge from
     a Hamiltonian circuit yields a spanning tree, we can expect that the structure of
     a minimum spanning tree provides a good basis for constructing a shortest tour
     approximation. Here is an algorithm that implements this idea in a rather straight-
     forward fashion.
     Twice-around-the-tree algorithm
     Step 1        Construct a minimum spanning tree of the graph corresponding to a
                   given instance of the traveling salesman problem.
     Step 2        Starting at an arbitrary vertex, perform a walk around the minimum
                   spanning tree recording all the vertices passed by. (This can be done
                   by a DFS traversal.)
     Step 3        Scan the vertex list obtained in Step 2 and eliminate from it all repeated
                   occurrences of the same vertex except the starting one at the end of
                   the list. (This step is equivalent to making shortcuts in the walk.) The
                   vertices remaining on the list will form a Hamiltonian circuit, which is
                   the output of the algorithm.
     EXAMPLE 2         Let us apply this algorithm to the graph in Figure 12.11a. The
     minimum spanning tree of this graph is made up of edges (a, b), (b, c), (b, d), and
     (d, e) (Figure 12.11b). A twice-around-the-tree walk that starts and ends at a is
                 a        12            e                             a           e
                       9        9
        8      4                           7   11
                 b        8             d                             b           d
                    6               10
                          c                                                  c
                          (a)                                                (b)
FIGURE  12.11     Illustration  of the twice-around-the-tree algorithm. (a) Graph.   (b)  Walk
                  around the    minimum spanning tree with the shortcuts.
                                a,  b,     c,  b,  d,  e,  d,     b,     a.
Eliminating the second b (a shortcut from c to d), the second d, and the third b (a
shortcut from e to a) yields the Hamiltonian circuit
                                        a,     b,  c,  d,  e,  a
of length 39.
The tour obtained in Example 2 is not optimal. Although that instance is small
enough to find an optimal solution by either exhaustive search or branch-and-
bound, we refrained from doing so to reiterate a general point. As a rule, we do
not know what the length of an optimal tour actually is, and therefore we cannot
compute the accuracy ratio f (sa)/f (s). For the twice-around-the-tree algorithm,
we can at least estimate it above, provided the graph is Euclidean.
THEOREM 2           The twice-around-the-tree algorithm is a 2-approximation algo-
rithm for the traveling salesman problem with Euclidean distances.
PROOF      Obviously, the twice-around-the-tree algorithm is polynomial time if we
use a reasonable algorithm such as Prim's or Kruskal's in Step 1. We need to show
that for any Euclidean instance of the traveling salesman problem, the length of a
tour sa obtained by the twice-around-the-tree algorithm is at most twice the length
of the optimal tour s, i.e.,
                                              f (sa)  2f (s).
Since removing any edge from s yields a spanning tree T of weight w(T ), which
must be greater than or equal to the weight of the graph's minimum spanning tree
w(T ), we get the inequality
                                    f (s) > w(T )  w(T ).
     This inequality implies that
     2f (s) > 2w(T ) = the length of the walk obtained in Step 2 of the algorithm.
     The possible shortcuts outlined in Step 3 of the algorithm to obtain sa cannot
     increase the total length of the walk in a Euclidean graph, i.e.,
     the length of the walk obtained in Step 2  the length of the tour sa.
     Combining the last two inequalities, we get the inequality
                                   2f (s) > f (sa),
     which is, in fact, a slightly stronger assertion than the one we needed to prove.
     Christofides Algorithm  There is an approximation algorithm with a better per-
     formance ratio for the Euclidean traveling salesman problem--the well-known
     Christofides algorithm [Chr76]. It also uses a minimum spanning tree but does
     this in a more sophisticated way than the twice-around-the-tree algorithm. Note
     that a twice-around-the-tree walk generated by the latter algorithm is an Eule-
     rian circuit in the multigraph obtained by doubling every edge in the graph given.
     Recall that an Eulerian circuit exists in a connected multigraph if and only if all
     its vertices have even degrees. The Christofides algorithm obtains such a multi-
     graph by adding to the graph the edges of a minimum-weight matching of all the
     odd-degree vertices in its minimum spanning tree. (The number of such vertices
     is always even and hence this can always be done.) Then the algorithm finds an
     Eulerian circuit in the multigraph and transforms it into a Hamiltonian circuit by
     shortcuts, exactly the same way it is done in the last step of the twice-around-the-
     tree algorithm.
     EXAMPLE 3        Let us trace the Christofides algorithm in Figure 12.12 on the same
     instance (Figure 12.12a) used for tracing the twice-around-the-tree algorithm in
     Figure 12.11. The graph's minimum spanning tree is shown in Figure 12.12b. It has
     four odd-degree vertices: a, b, c, and e. The minimum-weight matching of these
     four vertices consists of edges (a, b) and (c, e). (For this tiny instance, it can be
     found easily by comparing the total weights of just three alternatives: (a, b) and
     (c, e), (a, c) and (b, e), (a, e) and (b, c).) The traversal of the multigraph, starting
     at vertex a, produces the Eulerian circuit a - b - c - e - d - b - a, which, after
     one shortcut, yields the tour a - b - c - e - d - a of length 37.
     The performance ratio of the Christofides algorithm on Euclidean instances
     is 1.5 (see, e.g., [Pap82]). It tends to produce significantly better approximations
     to optimal tours than the twice-around-the-tree algorithm does in empirical tests.
     (We quote some results of such tests at the end of this subsection.) The quality of
     a tour obtained by this heuristic can be further improved by optimizing shortcuts
     made on the last step of the algorithm as follows: examine the multiply-visited
     cities in some arbitrary order and for each make the best possible shortcut. This
                              a           12          e
                                       9       9
                           8  4                          7     11
                              b           8           d
                                    6             10
                                          c
                                          (a)
                  a           e                       a                    e
        4      4                 7        11      4                9          7  11
                  b     8     d                       b                    d
                     6                                      6
                        c                                      c
                        (b)                                           (c)
FIGURE  12.12  Application of the Christofides algorithm. (a) Graph. (b) Minimum
               spanning tree with added edges (in dash) of a minimum-weight matching
               of all odd-degree vertices. (c) Hamiltonian circuit obtained.
enhancement would have not improved the tour a - b - c - e - d - a obtained in
Example 3 from a - b - c - e - d - b - a because shortcutting the second occur-
rence of b happens to be better than shortcutting its first occurrence. In general,
however, this enhancement tends to decrease the gap between the heuristic and
optimal tour lengths from about 15% to about 10%, at least for randomly gener-
ated Euclidean instances [Joh07a].
Local Search Heuristics       For Euclidean instances, surprisingly good approxima-
tions to optimal tours can be obtained by iterative-improvement algorithms, which
are also called local search heuristics. The best-known of these are the 2-opt, 3-
opt, and Lin-Kernighan algorithms. These algorithms start with some initial tour,
e.g., constructed randomly or by some simpler approximation algorithm such as
the nearest-neighbor. On each iteration, the algorithm explores a neighborhood
around the current tour by replacing a few edges in the current tour by other
edges. If the changes produce a shorter tour, the algorithm makes it the current
                          C4       C3                 C4              C3
                          C1       C2                 C1              C2
                              (a)                              (b)
     FIGURE 12.13  2-change: (a) Original tour. (b) New tour.
     tour and continues by exploring its neighborhood in the same manner; otherwise,
     the current tour is returned as the algorithm's output and the algorithm stops.
     The 2-opt algorithm works by deleting a pair of nonadjacent edges in a tour
     and reconnecting their endpoints by the different pair of edges to obtain another
     tour (see Figure 12.13). This operation is called the 2-change. Note that there is
     only one way to reconnect the endpoints because the alternative produces two
     disjoint fragments.
     EXAMPLE 4     If we start with the nearest-neighbor tour a - b - c - d - e - a in
     the graph of Figure 12.11, whose length lnn is equal to 39, the 2-opt algorithm will
     move to the next tour as shown in Figure 12.14.
     To generalize the notion of the 2-change, one can consider the k-change for
     any k  2. This operation replaces up to k edges in a current tour. In addition to
     2-changes, only the 3-changes have proved to be of practical interest. The two
     principal possibilities of 3-changes are shown in Figure 12.15.
     There are several other local search algorithms for the traveling salesman
     problem. The most prominent of them is the Lin-Kernighan algorithm [Lin73],
     which for two decades after its publication in 1973 was considered the best algo-
     rithm to obtain high-quality approximations of optimal tours. The Lin-Kernighan
     algorithm is a variable-opt algorithm: its move can be viewed as a 3-opt move
     followed by a sequence of 2-opt moves. Because of its complexity, we have to re-
     frain from discussing this algorithm here. The excellent survey by Johnson and
     McGeoch [Joh07a] contains an outline of the algorithm and its modern exten-
     sions as well as methods for its efficient implementation. This survey also contain
     results from the important empirical studies about performance of many heuris-
     tics for the traveling salesman problem, including of course, the Lin-Kernighan
     algorithm. We conclude our discussion by quoting some of these data.
     Empirical Results The traveling salesman problem has been the subject of in-
     tense study for the last 50 years. This interest was driven by a combination of pure
           a      12      e         a        12         e
        4                    7         9                   7
                                                              l = 42 > lnn =  39
           b              d         b        8          d
               6      10               6
                  c                          c
           a      12      e         a        12         e
        4                    7            9      9
                                                              l = 46 > lnn =  39
           b              d         b                   d
               6      10               6            10
                  c                          c
           a      12      e         a        12         e
        4                    7   4                  11
                                                              l = 45 > lnn =  39
           b              d         b        8          d
               6      10                            10
                  c                          c
           a      12      e         a                   e
        4                    7   4     8         9         7  l = 38 < lnn =  39
                                                              (new tour)
           b              d         b                   d
               6      10                            10
                  c                          c
FIGURE  12.14  2-changes from    the nearest-neighbor tour of the graph in    Figure  12.11.
     C5       C4                C5         C4                     C5               C4
C6                C3        C6                 C3             C6                       C3
     C1       C2                C1         C2                     C1               C2
         (a)                          (b)                                     (c)
              FIGURE 12.15  3-change: (a) Original tour. (b), (c) New tours.
              theoretical interest and serious practical needs stemming from such newer ap-
              plications as circuit-board and VLSI-chip fabrication, X-ray crystallography, and
              genetic engineering. Progress in developing effective heuristics, their efficient im-
              plementation by using sophisticated data structures, and the ever-increasing power
              of computers have led to a situation that differs drastically from a pessimistic pic-
              ture painted by the worst-case theoretical results. This is especially true for the
              most important applications class of instances of the traveling salesman problem:
              points in the two-dimensional plane with the standard Euclidean distances be-
              tween them.
                  Nowadays, Euclidean instances with up to 1000 cities can be solved exactly
              in quite a reasonable amount of time--typically, in minutes or faster on a good
              workstation--by such optimization packages as Concord [App]. In fact, according
              to the information on the Web site maintained by the authors of that package, the
              largest instance of the traveling salesman problem solved exactly as of January
              2010 was a tour through 85,900 points in a VLSI application. It significantly ex-
              ceeded the previous record of the shortest tour through all 24,978 cities in Sweden.
              There should be little doubt that the latest record will also be eventually super-
              seded and our ability to solve ever larger instances exactly will continue to expand.
              This remarkable progress does not eliminate the usefulness of approximation al-
              gorithms for such problems, however. First, some applications lead to instances
              that are still too large to be solved exactly in a reasonable amount of time. Second,
              one may well prefer spending seconds to find a tour that is within a few percent
              of optimum than to spend many hours or even days of computing time to find the
              shortest tour exactly.
                  But how can one tell how good or bad the approximate solution is if we do not
              know the length of an optimal tour? A convenient way to overcome this difficulty
              is to solve the linear programming problem describing the instance in question by
              ignoring the integrality constraints. This provides a lower bound--called the Held-
              Karp bound--on the length of the shortest tour. The Held-Karp bound is typically
              very close (less than 1%) to the length of an optimal tour, and this bound can be
              computed in seconds or minutes unless the instance is truly huge. Thus, for a tour
            TABLE 12.1    Average tour quality and running times for various
                          heuristics on the 10,000-city random uniform
                          Euclidean instances [Joh07a]
                              % excess over the         Running time
            Heuristic         Held-Karp bound           (seconds)
            nearest neighbor  24.79                               0.28
            multifragment     16.42                               0.20
            Christofides      9.81                                1.04
            2-opt             4.70                                1.41
            3-opt             2.88                                1.50
            Lin-Kernighan     2.00                                2.06
    sa obtained by some heuristic, we estimate the accuracy ratio r(sa) = f (sa)/f (s)
    from above by the ratio f (sa)/H K(s), where f (sa) is the length of the heuristic
    tour sa and H K(s) is the Held-Karp lower bound on the shortest-tour length.
    The results (see Table 12.1) from a large empirical study [Joh07a] indicate the
    average tour quality and running times for the discussed heuristics.3 The instances
    in the reported sample have 10,000 cities generated randomly and uniformly as
    integral-coordinate points in the plane, with the Euclidean distances rounded
    to the nearest integer. The quality of tours generated by the heuristics remain
    about the same for much larger instances (up to a million cities) as long as they
    belong to the same type of instances. The running times quoted are for expert
    implementations run on a Compaq ES40 with 500 Mhz Alpha processors and 2
    gigabytes of main memory or its equivalents.
    Asymmetric instances of the traveling salesman problem--i.e., those with a
    nonsymmetic matrix of intercity distances--have proved to be significantly harder
    to solve, both exactly and approximately, than Euclidean instances. In partic-
    ular, exact optimal solutions for many 316-city asymmetric instances remained
    unknown at the time of the state-of-the-art survey by Johnson et al. [Joh07b].
    Approximation Algorithms for the Knapsack Problem
    The knapsack problem, another well-known NP-hard problem, was also intro-
    duced in Section 3.4: given n items of known weights w1, . . . , wn and values
    v1, . . . , vn and a knapsack of weight capacity W, find the most valuable sub-
    set of the items that fits into the knapsack. We saw how this problem can be
    solved by exhaustive search (Section 3.4), dynamic programming (Section 8.2),
3.  We did not include the results for the twice-around-the-tree heuristic because of the inferior quality
    of its approximations with the average excess of about 40%. Nor did we quote the results for the
    most sophisticated local search heuristics with the average excess over optimum of less than a fraction
    of 1%.
     and branch-and-bound (Section 12.2). Now we will solve this problem by approx-
     imation algorithms.
     Greedy Algorithms for the Knapsack Problem We can think of several greedy
     approaches to this problem. One is to select the items in decreasing order of
     their weights; however, heavier items may not be the most valuable in the set.
     Alternatively, if we pick up the items in decreasing order of their value, there is
     no guarantee that the knapsack's capacity will be used efficiently. Can we find a
     greedy strategy that takes into account both the weights and values? Yes, we can,
     by computing the value-to-weight ratios vi/wi, i = 1, 2, . . . , n, and selecting the
     items in decreasing order of these ratios. (In fact, we already used this approach in
     designing the branch-and-bound algorithm for the problem in Section 12.2.) Here
     is the algorithm based on this greedy heuristic.
     Greedy algorithm for the discrete knapsack problem
     Step 1 Compute the value-to-weight ratios ri = vi/wi, i = 1, . . . , n, for the
             items given.
     Step 2 Sort the items in nonincreasing order of the ratios computed in Step 1.
             (Ties can be broken arbitrarily.)
     Step 3  Repeat the following operation until no item is left in the sorted list:
             if the current item on the list fits into the knapsack, place it in the
             knapsack and proceed to the next item; otherwise, just proceed to the
             next item.
     EXAMPLE 5  Let us consider the instance of the knapsack problem with the
     knapsack capacity 10 and the item information as follows:
                                item          weight   value
                                        1     7        $42
                                        2     3        $12
                                        3     4        $40
                                        4     5        $25
     Computing the value-to-weight ratios and sorting the items in nonincreasing order
     of these efficiency ratios yields
                          item  weight        value    value/weight
                          1                4  $40             10
                          2                7  $42             6
                          3                5  $25             5
                          4                3  $12             4
The greedy algorithm will select the first item of weight 4, skip the next item of
weight 7, select the next item of weight 5, and skip the last item of weight 3. The
solution obtained happens to be optimal for this instance (see Section 12.2, where
we solved the same instance by the branch-and-bound algorithm).
Does this greedy algorithm always yield an optimal solution? The answer, of
course, is no: if it did, we would have a polynomial-time algorithm for the NP-
hard problem. In fact, the following example shows that no finite upper bound on
the accuracy of its approximate solutions can be given either.
EXAMPLE 6
item               weight  value  value/weight
1                  1       2      2             The knapsack capacity is W > 2.
2                  W       W      1
Since the items are already ordered as required, the algorithm takes the first item
and skips the second one; the value of this subset is 2. The optimal selection con-
sists of item 2 whose value is W. Hence, the accuracy ratio r(sa) of this approximate
solution is W/2, which is unbounded above.
It is surprisingly easy to tweak this greedy algorithm to get an approximation
algorithm with a finite performance ratio. All it takes is to choose the better of
two alternatives: the one obtained by the greedy algorithm or the one consisting
of a single item of the largest value that fits into the knapsack. (Note that for
the instance of the preceding example, the second alternative is better than the
first one.) It is not difficult to prove that the performance ratio of this enhanced
greedy algorithm is 2. That is, the value of an optimal subset s will never be more
than twice as large as the value of the subset sa obtained by this enhanced greedy
algorithm, and 2 is the smallest multiple for which such an assertion can be made.
It is instructive to consider the continuous version of the knapsack problem
as well. In this version, we are permitted to take arbitrary fractions of the items
given. For this version of the problem, it is natural to modify the greedy algorithm
as follows.
Greedy algorithm for the continuous knapsack problem
Step 1       Compute the value-to-weight ratios vi/wi, i = 1, . . . , n, for the items
             given.
Step 2       Sort the items in nonincreasing order of the ratios computed in Step 1.
             (Ties can be broken arbitrarily.)
Step 3       Repeat the following operation until the knapsack is filled to its full
             capacity or no item is left in the sorted list: if the current item on the
             list fits into the knapsack in its entirety, take it and proceed to the next
             item; otherwise, take its largest fraction to fill the knapsack to its full
             capacity and stop.
     For example, for the four-item instance used in Example 5 to illustrate the
     greedy algorithm for the discrete version, the algorithm will take the first item of
     weight 4 and then 6/7 of the next item on the sorted list to fill the knapsack to its
     full capacity.
     It should come as no surprise that this algorithm always yields an optimal
     solution to the continuous knapsack problem. Indeed, the items are ordered
     according to their efficiency in using the knapsack's capacity. If the first item on
     the sorted list has weight w1 and value v1, no solution can use w1 units of capacity
     with a higher payoff than v1. If we cannot fill the knapsack with the first item
     or its fraction, we should continue by taking as much as we can of the second-
     most efficient item, and so on. A formal rendering of this proof idea is somewhat
     involved, and we will leave it for the exercises.
     Note also that the optimal value of the solution to an instance of the contin-
     uous knapsack problem can serve as an upper bound on the optimal value of the
     discrete version of the same instance. This observation provides a more sophisti-
     cated way of computing upper bounds for solving the discrete knapsack problem
     by the branch-and-bound method than the one used in Section 12.2.
     Approximation       Schemes      We  now   return  to  the   discrete   version  of  the  knap-
     sack problem. For this problem, unlike the traveling salesman problem, there exist
     polynomial-time approximation schemes, which are parametric families of algo-
     rithms that allow us to get approximations sa(k) with any predefined accuracy level:
                         f (s)        1+  1/ k  for any instance of size n,
                         f (sa(k))
     where k is an integer parameter in the range 0  k < n. The first approximation
     scheme was suggested by S. Sahni in 1975 [Sah75]. This algorithm generates all
     subsets of k items or less, and for each one that fits into the knapsack it adds the
     remaining items as the greedy algorithm would do (i.e., in nonincreasing order
     of their value-to-weight ratios). The subset of the highest value obtained in this
     fashion is returned as the algorithm's output.
     EXAMPLE 7       A small example of an approximation scheme with k = 2 is pro-
     vided in Figure 12.16. The algorithm yields {1, 3, 4}, which is the optimal solution
     for this instance.
     You can be excused for not being overly impressed by this example. And,
     indeed, the importance of this scheme is mostly theoretical rather than practical.
     It lies in the fact that, in addition to approximating the optimal solution with any
     predefined accuracy level, the time efficiency of this algorithm is polynomial in n.
     Indeed, the total number of subsets the algorithm generates before adding extra
     elements is
     k            n      k     n(n - 1) . . . (n - j + 1)   k          k
                  j  =                    j!                      nj         nk = (k + 1)nk.
     j =0                j =0                               j =0       j =0
item       weight  value          value/weight            subset  added items   value
    1         4    $40                  10                          1, 3, 4     $69
    2         7    $42                  6                 {1}       3, 4        $69
    3         5    $25                  5                 {2}       4           $46
    4         1    $4                   4                 {3}       1, 4        $69
                                                          {4}       1, 3        $69
                 capacity W = 10                          {1, 2}  not feasible
                                                          {1, 3}    4           $69
                                                          {1, 4}    3           $69
                                                          {2, 3}  not feasible
                                                          {2, 4}                $46
                                                          {3, 4}    1           $69
                   (a)                                              (b)
FIGURE 12.16     Example of applying Sahni's approximation scheme for k = 2. (a) Instance.
                 (b) Subsets generated by the algorithm.
For each of those subsets, it needs O(n) time to determine the subset's possible
extension. Thus, the algorithm's efficiency is in O(knk+1). Note that although it is
polynomial in n, the time efficiency of Sahni's scheme is exponential in k. More
sophisticated approximation schemes, called fully polynomial schemes, do not
have this shortcoming. Among several books that discuss such algorithms, the
monographs [Mar90] and [Kel04] are especially recommended for their wealth of
other material about the knapsack problem.
Exercises 12.3
1.     a.  Apply the nearest-neighbor algorithm to the instance defined by the inter-
           city distance matrix below. Start the algorithm at the first city, assuming
           that the cities are numbered from 1 to 5.
                                    0       14  4     10       
                                    14      0   5     8   7    
                                    4       5   0     9   16
                                    10      8   9     0   32
                                            7   16    32  0
       b. Compute the accuracy ratio of this approximate solution.
     2.  a.  Write pseudocode for the nearest-neighbor algorithm.         Assume    that    its
             input is given by an n × n intercity distance matrix.
         b. What is the time efficiency of the nearest-neighbor algorithm?
     3.  Apply the twice-around-the-tree algorithm to the graph in Figure 12.11a with
         a walk around the minimum spanning tree that starts at the same vertex a but
         differs from the walk in Figure 12.11b. Is the length of the obtained tour the
         same as the length of the tour in Figure 12.11b?
     4.  Prove that making a shortcut of the kind used by the twice-around-the-tree
         algorithm cannot increase the tour's length in a Euclidean graph.
     5.  What is the time efficiency class of the greedy algorithm for the knapsack
         problem?
     6.  Prove that the performance ratio RA of the enhanced greedy algorithm for
         the knapsack problem is equal to 2.
     7.  Consider the greedy algorithm for the bin-packing problem, which is called
         the first-fit (FF) algorithm: place each of the items in the order given into the
         first bin the item fits in; when there are no such bins, place the item in a new
         bin and add this bin to the end of the bin list.
         a.  Apply FF  to the instance
                       s1 = 0.4,   s2 = 0.7,    s3 = 0.2,   s4 = 0.1,     s5 = 0.5
             and determine whether the solution obtained is optimal.
         b. Determine the worst-case time efficiency of FF.
         c.  Prove that FF is a 2-approximation algorithm.
     8.  The first-fit decreasing (FFD) approximation algorithm for the bin-packing
         problem starts by sorting the items in nonincreasing order of their sizes and
         then acts as the first-fit algorithm.
         a.  Apply FFD to the instance
                       s1 = 0.4,   s2 = 0.7,    s3 = 0.2,   s4 = 0.1,     s5 = 0.5
             and determine whether the solution obtained is optimal.
         b.  Does FFD always yield an optimal solution? Justify your answer.
         c.  Prove that FFD is a 1.5-approximation algorithm.
         d.  Run  an  experiment   to  determine     which  of  the  two  algorithms--FF    or
             FFD--yields more accurate approximations on a random sample of the
             problem's instances.
     9.  a.  Design a simple 2-approximation algorithm for finding a minimum vertex
             cover (a vertex cover with the smallest number of vertices) in a given graph.
         b.  Consider the following approximation algorithm for finding a maximum
             independent set (an independent set with the largest number of vertices) in
             a given graph. Apply the 2-approximation algorithm of part (a) and output
               all the vertices that are not in the obtained vertex cover. Can we claim that
               this algorithm is a 2-approximation algorithm, too?
      10.  a.  Design a polynomial-time greedy algorithm for the graph-coloring prob-
               lem.
           b.  Show that the performance ratio of your approximation algorithm is in-
               finitely large.
