Closed Hashing \(Open Addressing\)
In closed hashing, all keys are stored in the hash table itself without the use
     of linked lists. (Of course, this implies that the table size m must be at least as
     large as the number of keys n.) Different strategies can be employed for collision
     resolution. The simplest one--called linear probing--checks the cell following
     the one where the collision occurs. If that cell is empty, the new key is installed
     there; if the next cell is already occupied, the availability of that cell's immediate
     successor is checked, and so on. Note that if the end of the hash table is reached,
     the search is wrapped to the beginning of the table; i.e., it is treated as a circular
     array. This method is illustrated in Figure 7.6 with the same word list and hash
     function used above to illustrate separate chaining.
     To search for a given key K, we start by computing h(K) where h is the hash
     function used in the table construction. If the cell h(K) is empty, the search is
     unsuccessful. If the cell is not empty, we must compare K with the cell's occupant:
     if they are equal, we have found a matching key; if they are not, we compare K
     with a key in the next cell and continue in this manner until we encounter either
     a matching key (a successful search) or an empty cell (unsuccessful search). For
     example, if we search for the word LIT in the table of Figure 7.6, we will get h(LIT)
     = (12 + 9 + 20) mod 13 = 2 and, since cell 2 is empty, we can stop immediately.
     However, if we search for KID with h(KID) = (11 + 9 + 4) mod 13 = 11, we will
     have to compare KID with ARE, SOON, PARTED, and A before we can declare the
     search unsuccessful.
     Although the search and insertion operations are straightforward for this
     version of hashing, deletion is not. For example, if we simply delete the key ARE
     from the last state of the hash table in Figure 7.6, we will be unable to find the key
     SOON afterward. Indeed, after computing h(SOON) = 11, the algorithm would find
     this location empty and report the unsuccessful search result. A simple solution
     keys                  A    FOOL     AND  HIS        MONEY     ARE  SOON            PARTED
     hash addresses        1       9     6    10           7       11          11       12
     0           1   2     3    4     5  6    7          8      9       10         11   12
                 A
                 A                                              FOOL
                 A                       AND                    FOOL
                 A                       AND                    FOOL    HIS
                 A                       AND  MONEY             FOOL    HIS
                 A                       AND  MONEY             FOOL    HIS        ARE
                 A                       AND  MONEY             FOOL    HIS        ARE  SOON
     PARTED      A                       AND  MONEY             FOOL    HIS        ARE  SOON
     FIGURE 7.6  Example of   a hash table construction  with linear probing.
    is to use "lazy deletion," i.e., to mark previously occupied locations by a special
    symbol to distinguish them from locations that have not been occupied.
    The mathematical analysis of linear probing is a much more difficult problem
    than that of separate chaining.3 The simplified versions of these results state that
    the average number of times the algorithm must access the hash table with the
    load factor  in successful and unsuccessful searches is, respectively,
                 S  1(1 +  1        )       and        U     1(1 +           1     )   (7.5)
                 2         1-                                2           (1  - )2
    (and the accuracy of these approximations increases with larger sizes of the hash
    table). These numbers are surprisingly small even for densely populated tables,
    i.e., for large percentage values of :
                           1        (1  +        1  )     1  (1  +       1   )
                           2                1-            2         (1-)2
                 50%                        1.5                     2.5
                 75%                        2.5                     8.5
                 90%                        5.5                  50.5
    Still, as the hash table gets closer to being full, the performance of linear prob-
    ing deteriorates because of a phenomenon called clustering. A cluster in linear
    probing is a sequence of contiguously occupied cells (with a possible wrapping).
    For example, the final state of the hash table of Figure 7.6 has two clusters. Clus-
    ters are bad news in hashing because they make the dictionary operations less
    efficient. As clusters become larger, the probability that a new element will be
    attached to a cluster increases; in addition, large clusters increase the probabil-
    ity that two clusters will coalesce after a new key's insertion, causing even more
    clustering.
    Several other collision resolution strategies have been suggested to alleviate
    this problem. One of the most important is double hashing. Under this scheme, we
    use another hash function, s(K), to determine a fixed increment for the probing
    sequence to be used after a collision at location l = h(K):
                 (l + s(K)) mod m,               (l + 2s(K)) mod m,             ... .  (7.6)
    To guarantee that every location in the table is probed by sequence (7.6), the incre-
    ment s(k) and the table size m must be relatively prime, i.e., their only common
    divisor must be 1. (This condition is satisfied automatically if m itself is prime.)
    Some functions recommended in the literature are s(k) = m - 2 - k mod (m - 2)
    and s(k) = 8 - (k mod 8) for small tables and s(k) = k mod 97 + 1 for larger ones.
3.  This problem was solved in 1962 by a young graduate student in mathematics named Donald E.
    Knuth. Knuth went on to become one of the most important computer scientists of our time. His
    multivolume treatise The Art of Computer Programming [KnuI, KnuII, KnuIII, KnuIV] remains the
    most comprehensive and influential book on algorithmics ever published.
     Mathematical analysis of double hashing has proved to be quite difficult. Some
     partial results and considerable practical experience with the method suggest that
     with good hashing functions--both primary and secondary--double hashing is su-
     perior to linear probing. But its performance also deteriorates when the table gets
     close to being full. A natural solution in such a situation is rehashing: the current
     table is scanned, and all its keys are relocated into a larger table.
         It is worthwhile to compare the main properties of hashing with balanced
     search trees--its principal competitor for implementing dictionaries.
         Asymptotic time efficiency  With hashing, searching, insertion, and deletion
         can be implemented to take    (1) time on the average but          (n) time in the very
         unlikely worst case. For balanced search trees, the average time efficiencies
         are  (log n) for both the average and worst cases.
         Ordering preservation       Unlike    balanced  search  trees,     hashing  does   not
         assume existence of key ordering and usually does not preserve it. This makes
         hashing less suitable for applications that need to iterate over the keys in or-
         der or require range queries such as counting the number of keys between
         some lower and upper bounds.
         Since its discovery in the 1950s by IBM researchers, hashing has found many
     important applications. In particular, it has become a standard technique for stor-
     ing a symbol table--a table of a computer program's symbols generated during
     compilation. Hashing is quite handy for such AI applications as checking whether
     positions generated by a chess-playing computer program have already been con-
     sidered. With some modifications, it has also proved to be useful for storing very
     large dictionaries on disks; this variation of hashing is called extendible hashing.
     Since disk access is expensive compared with probes performed in the main mem-
     ory, it is preferable to make many more probes than disk accesses. Accordingly, a
     location computed by a hash function in extendible hashing indicates a disk ad-
     dress of a bucket that can hold up to b keys. When a key's bucket is identified,
     all its keys are read into main memory and then searched for the key in question.
     In the next section, we discuss B-trees, a principal alternative for storing large
     dictionaries.
     Exercises 7.3
     1.  For the input 30, 20, 56, 75, 31, 19 and hash function h(K) = K mod 11
         a.  construct the open hash table.
         b. find the largest number of key comparisons in a successful search in this
             table.
         c.  find the average number of key comparisons in a successful search in this
             table.
     2.  For the input 30, 20, 56, 75, 31, 19 and hash function h(K) = K mod 11
         a.  construct the closed hash table.
     b. find the largest number of key comparisons in a successful search in this
         table.
     c.  find the average number of key comparisons in a successful search in this
         table.
3.   Why is it not a good idea for a hash function to depend on just one letter (say,
     the first one) of a natural-language word?
4.   Find the probability of all n keys being hashed to the same cell of a hash table
     of size m if the hash function distributes keys evenly among all the cells of the
     table.
5.   Birthday paradox      The birthday paradox asks how many people should be
     in a room so that the chances are better than even that two of them will have
     the same birthday (month and day). Find the quite unexpected answer to this
     problem. What implication for hashing does this result have?
6.   Answer the following questions for the separate-chaining version of hashing.
     a.  Where would you insert keys if you knew that all the keys in the dictionary
         are distinct? Which dictionary operations, if any, would benefit from this
         modification?
     b.  We could keep keys of the same linked list sorted. Which of the dictio-
         nary operations would benefit from this modification? How could we take
         advantage of this if all the keys stored in the entire table need to be sorted?
7.   Explain how to use hashing to check whether all elements of a list are distinct.
     What is the time efficiency of this application? Compare its efficiency with
     that of the brute-force algorithm (Section 2.3) and of the presorting-based
     algorithm (Section 6.1).
8.   Fill in the following table with the average-case (as the first entry) and worst-
     case (as the second entry) efficiency classes for the five implementations of
     the ADT dictionary:
                        unordered  ordered       binary  balanced
                           array   array    search tree  search tree  hashing
             search
             insertion
             deletion
9.   We have discussed hashing in the context of techniques based on space­time
     trade-offs. But it also takes advantage of another general strategy. Which one?
10.  Write a computer program that uses hashing for the following problem. Given
     a natural-language text, generate a list of distinct words with the number of
     occurrences of each word in the text. Insert appropriate counters in the pro-
     gram to compare the empirical efficiency of hashing with the corresponding
     theoretical results.
              p0       K1  p1    ..        .  pi ­1   Ki  pi      ...       pn ­2           Kn ­1  pn ­1
          T0               T1                 Ti ­1           Ti       Tn ­2                       Tn ­1
          FIGURE 7.7   Parental  node  of  a B-tree.
     