Overview of Operating Systems


                                                          Chapte                          r  3
Overview of
Operating Systems
W hen we want to describe a computing environment, we need to look at
     both the computer system and its users: How is the computer system
     built? How is it installed to work with other systems? What are the
services it provides to its users? All these features of a computing environment
influence the design of an operating system because the OS has to provide a
suitable combination of efficient use of the computer's resources and convenience
of its users--what we called the notion of effective utilization of a computer system
in Chapter 1--and also prevent interference in the activities of its users.
     Throughout  the  history  of  computing,  computing  environments       have
changed as computer architecture and users' expectations have changed. New
notions of effective utilization emerged with each new computing environment,
so a new class of operating systems was developed, which used new concepts and
techniques to achieve effective utilization.
     Modern computing environments support diverse applications, so they pos-
sess features of several of the classical computing environments. Consequently,
many of the concepts and techniques of the classical computing environments
can be found in the strategies modern operating systems employ. To simplify the
study of modern operating systems, in this chapter we present an overview of
the concepts and techniques of the classical computing environments and discuss
which of them find a place in a modern operating system.
3.1  COMPUTING ENVIRONMENTS AND
     NATURE OF COMPUTATIONS                                                            ·
A computing environment consists of a computer system, its interfaces with other
systems, and the services provided by its operating system to its users and their
programs. Computing environments evolve continuously to provide better quality
of service to users; however, the operating system has to perform more com-
plex tasks as computer systems become more powerful, their interfaces with I/O
devices and with other computer systems become more complex, and its users
demand new services.
                                                                                                51



52  Part 1  Overview
                      The nature of computations in a computing environment, and the manner in
            which users realize them, depends on features of the computing environment.
            In a typical modern computing environment, a user initiates diverse activi-
            ties simultaneously; e.g., he may run a mail handler, edit a few files, initiate
            computations, listen to music or watch a video, and browse the Internet at
            the same time. The operating system has to provide the resources required by
            each of these activities, such as the CPU and memory, and I/O devices located
            either within the same computer system or in another computer system that
            can be accessed over the Internet, so that the activities progress to the user's
            satisfaction.
                      We will begin the discussion of operating systems by taking a quick look at
            how computing environments evolved to their present form.
            Noninteractive Computing Environments      These are the earliest forms of com-
            puting environments. In these environments, a user submits both a computation
            in the form of a program and its data together to the operating system. The com-
            putation is performed by the operating system and its results are presented back
            to the user. The user has no contact with the computation during its execution.
            Hence these computations can be viewed as passive entities, to be interpreted
            and realized by the operating system. Examples of noninteractive computations
            are scientific computations involving number crunching and database updates
            performed overnight. In these computing environments, the operating system
            focuses on efficient use of resources.
                      Computations used in a noninteractive environment are in the form of a
            program or a job. A program is a set of functions or modules that can be exe-
            cuted by itself. A job is a sequence of programs that together achieve a desired
            goal; a program in a job is executed only if previous programs in the job have
            executed successfully. For example, consider compilation, linking, and execution
            of a C++ program. A job to achieve these actions would consist of execution
            of a C++ compiler, followed by execution of a linker to link the program with
            functions from libraries, followed by execution of the linked program. Here, link-
            ing is meaningful only if the program is compiled successfully, and execution is
            meaningful only if linking is successful.
            Interactive Computing Environments         In these computing environments, a user
            may interact with a computation while it is in progress. The nature of an inter-
            action between a user and his computation depends on how the computation is
            coded; e.g., a user may input the name of a data file to a computation during its
            execution, or may directly input some data to it, and the computation may display
            a result after processing the data. The operating system focuses on reducing the
            average amount of time required to implement an interaction between a user and
            his computation.
                      A user also interacts with the OS to initiate a computation, typically each user
            command to the OS calls for separate execution of a program. Here the notion
            of a job is not important because a user would himself consider the dependence
            of programs while issuing the next command. For example, if a C++ program
            is to be compiled, linked, and executed, a user would attempt linking only if



                                                      Chapter 3      Overview of Operating  Systems  53
Table 3.1    Computations in an OS
Computation  Description
Program      A program is a set of functions or modules, including some
             functions or modules obtained from libraries.
Job          A job is a sequence of programs that together achieve a
             common goal. It is not meaningful to execute a program in a
             job unless previous programs in the job have been executed
             successfully.
Process      A process is an execution of a program.
Subrequest   A subrequest is the presentation of a computational
             requirement by a user to a process. Each subrequest produces a
             single response, which consists of a set of results or actions.
the program had compiled successfully. Hence operating systems for interactive
environments deal exclusively with execution of programs, not jobs. OS literature
uses the term process for an execution of a program in an interactive environment.
In principle, the term process is applicable in both noninteractive and interactive
environments. However, we will follow the convention and use it only in the
context of interactive computing environments.
A user's interaction with a process consists of presentation of a computational
requirement--a subrequest--by the user to the process, and a response by the
process. Depending on the nature of a subrequest, the response may be in the form
of a set of results, or a set of actions such as file operations or database updates.
Table 3.1 describes the program, job, process, and subrequest computations.
Real-Time, Distributed, and Embedded Environments     Some computations have
special requirements, hence special computing environments are developed to
service them. A real-time computation is one that works under specific time con-
straints, so its actions are effective only if they are completed within a specified
interval of time. For example, a computation that periodically samples the data
from an instrument and stores the samples in a file must finish storing a sam-
ple before it is due to take the next sample. The operating system in a real-time
environment uses special techniques to ensure that computations are completed
within their time constraints. The distributed computing environment enables a
computation to use resources located in several computer systems through a net-
work. In the embedded computing environment, the computer system is a part of
a specific hardware system, such as a household appliance, a subsystem of an
automobile, or a handheld device such as a personal digital assistant (PDA), and
runs computations that effectively control the system. The computer is typically
an inexpensive one with a minimal configuration; its OS has to meet the time
constraints arising from the nature of the system being controlled.
Modern Computing Environments  To support diverse applications, the comput-
ing environment of a modern computer has features of several of the computing
environments described earlier. Consequently, its operating system has to employ



54  Part 1  Overview
            complex strategies to manage user computations and resources; e.g., it has to
            reduce the average amount of time required to implement an interaction between
            a user and a computation, and also ensure efficient use of resources.
                      We study the strategies used in modern operating systems in two stages:
            In this chapter, we first study the operating system strategies used in each of the
            computing environments mentioned earlier, and then see which of them are useful
            in a modern computing environment. In later chapters, we discuss the design of
            the strategies used in modern operating systems.
            3.2       CLASSES OF OPERATING SYSTEMS                                                  ·
            Classes of operating systems have evolved over time as computer systems and
            users' expectations of them have developed; i.e., as computing environments have
            evolved. As we study some of the earlier classes of operating systems, we need
            to understand that each was designed to work with computer systems of its own
            historical period; thus we will have to look at architectural features representative
            of computer systems of the period.
                      Table 3.2 lists five fundamental classes of operating systems that are named
            according to their defining features. The table shows when operating systems of
            each class first came into widespread use; what fundamental effectiveness crite-
            rion, or prime concern, motivated its development; and what key concepts were
            developed to address that prime concern.
                      Computing hardware was expensive in the early days of computing, so
            the batch processing and multiprogramming operating systems focused on effi-
            cient use of the CPU and other resources in the computer system. Computing
            environments were noninteractive in this era. In the 1970s, computer hardware
            became cheaper, so efficient use of a computer was no longer the prime concern
            and the focus shifted to productivity of computer users. Interactive comput-
            ing environments were developed and time-sharing operating systems facilitated
            Table 3.2     Key  Features of  Classes of Operating Systems
            OS class           Period           Prime concern     Key concepts
            Batch processing   1960s            CPU idle time     Automate transition
                                                                  between jobs
            Multiprogramming   1960s            Resource          Program priorities,
                                                utilization       preemption
            Time-sharing       1970s            Good response     Time slice, round-robin
                                                time              scheduling
            Real time          1980s            Meeting time      Real-time scheduling
                                                constraints
            Distributed        1990s            Resource sharing  Distributed control,
                                                                  transparency



                                                        Chapter 3   Overview of Operating  Systems  55
better productivity by providing quick response to subrequests made to processes.
The 1980s saw emergence of real-time applications for controlling or tracking of
real-world activities, so operating systems had to focus on meeting the time con-
straints of such applications. In the 1990s, further declines in hardware costs led
to development of distributed systems, in which several computer systems, with
varying sophistication of resources, facilitated sharing of resources across their
boundaries through networking.
The following paragraphs elaborate on key concepts of the five classes of
operating systems mentioned in Table 3.2.
Batch Processing Systems  In a batch processing operating system, the prime
concern is CPU efficiency. The batch processing system operates in a strict one-
job-at-a-time manner; within a job, it executes the programs one after another.
Thus only one program is under execution at any time. The opportunity to
enhance CPU efficiency is limited to efficiently initiating the next program when
one program ends, and the next job when one job ends, so that the CPU does not
remain idle.
Multiprogramming Systems  A multiprogramming operating system focuses on
efficient use of both the CPU and I/O devices. The system has several programs
in a state of partial completion at any time. The OS uses program priorities and
gives the CPU to the highest-priority program that needs it. It switches the CPU
to a low-priority program when a high-priority program starts an I/O operation,
and switches it back to the high-priority program at the end of the I/O operation.
These actions achieve simultaneous use of I/O devices and the CPU.
Time-Sharing Systems      A time-sharing operating system focuses on facilitating
quick response to subrequests made by all processes, which provides a tangible
benefit to users. It is achieved by giving a fair execution opportunity to each
process through two means: The OS services all processes by turn, which is called
round-robin scheduling. It also prevents a process from using too much CPU time
when scheduled to execute, which is called time-slicing. The combination of these
two techniques ensures that no process has to wait long for CPU attention.
Real-Time     Systems  A  real-time  operating  system  is  used    to  implement     a
computer application for controlling or tracking of real-world activities. The
application needs to complete its computational tasks in a timely manner to keep
abreast of external events in the activity that it controls. To facilitate this, the
OS permits a user to create several processes within an application program, and
uses real-time scheduling to interleave the execution of processes such that the
application can complete its execution within its time constraint.
Distributed Systems    A distributed operating system permits a user to access
resources located in other computer systems conveniently and reliably. To enhance
convenience, it does not expect a user to know the location of resources in the
system, which is called transparency. To enhance efficiency, it may execute parts of
a computation in different computer systems at the same time. It uses distributed
control; i.e., it spreads its decision-making actions across different computers in



56  Part 1  Overview
            the system so that failures of individual computers or the network does not cripple
            its operation.
                      In Sections 3.4­3.8, we will examine each of the five fundamental OS classes
            in greater detail.
            3.3       EFFICIENCY, SYSTEM PERFORMANCE,
                      AND USER SERVICE                                                              ·
            Measurement provides a method of assessing selected aspects of an operating sys-
            tem's functioning. In Chapter 1, we defined efficiency of use and user convenience
            as two of the fundamental goals of an OS. However, to a system administrator
            the performance of a system in its environment is more important than merely
            efficiency of use, hence in this section we discuss measures of efficiency, system
            performance, and user service. Table 3.3 summarizes these measures.
            Efficiency  The way to evaluate efficiency of use of a resource is to see how
            much of the resource is unused or wasted, and, in the amount of resource that is
            used, check how much of it is put to productive use. As an example of efficiency,
            consider use of the CPU. Some amount of CPU time is wasted because the CPU
            does not have enough work to do. This happens when all user processes in the
            system are either performing I/O operations or waiting for the users to supply
            data. Of the CPU time that is used, some amount of time is used by the OS itself
            in performing interrupt servicing and scheduling. This constitutes the overhead
            of OS operation. The remaining CPU time is used for executing user processes.
            To evaluate efficiency of CPU use, we should consider what fraction or percentage
            of the total CPU time is used for executing user processes. Efficiency of use of other
            resources such as memory and I/O devices can be similarly determined: Deduct
            the amount of unused resource and the OS overhead from the total resource and
            consider what fraction or percentage the result is of the total resource.
                      Using the notion of efficiency of use, we briefly discuss the fundamental
            trade-off between efficiency of use and user convenience: A multiprogramming
            system has several user programs at any time and switches between them to
            obtain efficient use of both the CPU and I/O devices. The CPU is given to the
            Table 3.3       Measures of Efficiency, System Performance,
            and User Service
            Aspect              Measure            Description
            Efficiency of use   CPU efficiency     Percent utilization of the CPU
                                Memory efficiency  Percent utilization of memory
            System performance  Throughput         Amount of work done per unit time
            User service        Turnaround time    Time to complete a job or a process
                                Response time      Time to implement one subrequest



                                                    Chapter 3    Overview of Operating  Systems  57
highest-priority program in the system whenever it wants, and it can use the CPU
for as long as it wants. A time-sharing system, however, restricts the amount
of CPU time a scheduled process can use. It preempts a process that uses too
much CPU time and schedules another process. The preempted process may
be scheduled again sometime in future. This feature increases the OS overhead
in interrupt servicing and scheduling, thereby affecting efficiency of CPU use.
However, it provides good response times to all processes, which is a feature
desired by users of the OS.
System Performance  Once we decide on the suitable combination of CPU effi-
ciency and user service, it is important to know how well the OS is performing.
The notion of performance depends on the computing environment and indicates
the rate at which a computer system accomplishes work during its operation.
An operating system typically uses a measure of efficiency to tune its func-
tioning for better performance. For example, if memory efficiency is low, the
operating system may load more user programs in memory. In turn, it may lead
to better performance of the system by increasing the rate at which the system
completes user computations. If CPU efficiency is low, the operating system may
investigate its causes--either too few programs in memory or programs spending
too much time in waiting for I/O to complete--and take corrective actions where
possible.
System performance is characterized as the amount of work done per unit
time. It is typically measured as throughput.
Definition 3.1 Throughput    The average number of jobs, programs, processes,
or subrequests completed by a system in unit time.
The unit of work used for measuring throughput depends on the computing
environment. In a noninteractive environment, throughput of an OS is measured
in terms of the number of jobs or programs completed per unit time. In an inter-
active environment, it may be measured in terms of the number of subrequests
completed per unit time. In a specialized computing environment, performance
may be measured in terms meaningful to the application; for example, in a bank-
ing environment, it could be the number of transactions per unit time. Throughput
can also be used as a measure of performance for I/O devices. For example, the
throughput of a disk can be measured as the number of I/O operations completed
per unit time or the number of bytes transferred per unit time.
User Service  Some aspects of user convenience are intangible and thus impos-
sible to measure numerically; e.g., a feature like user friendly interfaces cannot be
quantified. However, there are some measurable aspects of user convenience, so
we can define appropriate measures for them. User service, which indicates how
quickly a user's computation has been completed by the OS, is one such aspect.
We define two measures of user service--turnaround time, in noninteractive com-
puting environments, and response time, in interactive computing environments.
A smaller turnaround time or response time implies better user service.



58  Part 1  Overview
            Definition 3.2 Turnaround Time  The time from submission of a job, program,
            or process by a user to the time its results become available to the user.
            Definition 3.3 Response Time    The time from submission of a subrequest by
            a user to the time a process responds to it.
                      Specialized measures of user service may be defined for use in specific
            computing environments. Two such examples are deadline overrun in a real-time
            operating system and computation speedup in a distributed operating system.
            Deadline overrun indicates by how much time the OS was late in completing the
            execution of a computation with time constraints, so a negative deadline overrun
            indicates good user service. Computation speedup indicates by what factor the
            execution of an application was speeded up because its processes were executed
            at the same time in different computers of a distributed system; a larger value of
            computation speedup implies better user service.
            3.4       BATCH PROCESSING SYSTEMS                                                       ·
            Computer systems of the 1960s were noninteractive. Punched cards were the pri-
            mary input medium, so a job and its data consisted of a deck of cards. A computer
            operator would load the cards into the card reader to set up the execution of a
            job. This action wasted precious CPU time; batch processing was introduced to
            prevent this wastage.
                      A batch is a sequence of user jobs formed for processing by the operating
            system. A computer operator formed a batch by arranging a few user jobs in a
            sequence and inserting special marker cards to indicate the start and end of the
            batch. When the operator gave a command to initiate processing of a batch, the
            batching kernel set up the processing of the first job of the batch. At the end of
            the job, it initiated execution of the next job, and so on, until the end of the batch.
            Thus the operator had to intervene only at the start and end of a batch.
                      Card readers and printers were a performance bottleneck in the 1960s,
            so batch processing systems employed the notion of virtual card readers and
            printers (described in Section 1.3.2) through magnetic tapes, to improve the
            system's throughput. A batch of jobs was first recorded on a magnetic tape, using
            a less powerful and cheap computer. The batch processing system processed these
            jobs from the tape, which was faster than processing them from cards, and wrote
            their results on another magnetic tape. These were later printed and released to
            users. Figure 3.1 shows the factors that make up the turnaround time of a job.
                      User jobs could not interfere with each other's execution directly because
            they did not coexist in a computer's memory. However, since the card reader
            was the only input device available to users, commands, user programs, and data
            were all derived from the card reader, so if a program in a job tried to read more
            data than provided in the job, it would read a few cards of the following job! To
            protect against such interference between jobs, a batch processing system required



                                                              Chapter 3          Overview of Operating Systems  59
                              Batch          Batch            Result
                              formation      execution        printing
                          t0             t1               t2            t3
                  Job is      Batch is                        Results are
                  submitted   formed                          returned to user
                                         Turnaround time
Figure  3.1  Turnaround time in a batch processing system.
             //  JOB ···                          "Start of job" statement
             //  EXEC     FORTRAN                 Execute the Fortran compiler
                              Fortran
                              program
             //  EXEC                             Execute just compiled program
                              Data for
                              Fortran
                              program
             /*                                   "End of data" statement
             /&                                   "End of job" statement
Figure  3.2  Control statements in IBM 360/370 systems.
a user to insert a set of control statements in the deck of cards constituting a job.
The command interpreter, which was a component of the batching kernel, read
a card when the currently executing program in the job wanted the next card.
If the card contained a control statement, it analyzed the control statement and
performed appropriate actions; otherwise, it passed the card to the currently
executing program. Figure 3.2 shows a simplified set of control statements used
to compile and execute a Fortran program. If a program tried to read more data
than provided, the command interpreter would read the /*, /&                     and //  JOB
cards. On seeing one of these cards, it would realize that the program was trying
to read more cards than provided, so it would abort the job.
     A modern OS would not be designed for batch processing, but the tech-
nique is still useful in financial and scientific computation where the same kind
of processing or analysis is to be performed on several sets of data. Use of batch
processing in such environments would eliminate time-consuming initialization
of the financial or scientific analysis separately for each set of data.
3.5     MULTIPROGRAMMING SYSTEMS                                                                                ·
Multiprogramming  operating              systems  were    developed         to  provide  efficient
resource utilization in a noninteractive environment. A multiprogramming OS



60          Part 1  Overview
                    Multiprogramming       Multiprogramming                                Multiprogramming
                    kernel                         kernel                                  kernel
            I/O     program1          I/O          program1  CPU                           program1
    CPU             program2          I/O          program2  I/O                           program2
                    program3          CPU          program3                                program3
    (a)                               (b)                    (c)
Figure 3.3  Operation of a multiprogramming system: (a) program2 is in execution while     program1 is performing an I/O
operation; (b) program2 initiates an I/O operation, program3 is scheduled; (c) program1's  I/O operation completes and it  is
scheduled.
                    has many user programs in the memory of the computer at any time, hence the
                    name multiprogramming. It employs the DMA mode of I/O (see Section 2.2.4),
                    so it can perform I/O operations of some program(s) while using the CPU to
                    execute some other program. This arrangement makes efficient use of both the
                    CPU and I/O devices. The I/O and computational activities in several programs
                    are in progress at any time, so it also leads to high system performance. We discuss
                    this aspect in Section 3.5.1.
                              Figure 3.3 illustrates operation of a multiprogramming OS. The memory
                    contains three programs. An I/O operation is in progress for program1, while the
                    CPU is executing program2. The CPU is switched to program3 when program2
                    initiates an I/O operation, and it is switched to program1 when program1's I/O
                    operation completes. The multiprogramming kernel performs scheduling, mem-
                    ory management and I/O management. It uses a simple scheduling policy, which
                    we will discuss in Section 3.5.1, and performs simple partitioned or pool-based
                    allocation of memory and I/O devices. Since several programs are in memory at
                    the same time, the instructions, data, and I/O operations of a program should be
                    protected against interference by other programs. We shall shortly see how it is
                    achieved.
                              A computer must possess the features summarized in Table 3.4 to support
                    multiprogramming (see Section 2.2). The DMA makes multiprogramming fea-
                    sible by permitting concurrent operation of the CPU and I/O devices. Memory
                    protection prevents a program from accessing memory locations that lie outside
                    the range of addresses defined by contents of the base register and size register
                    of the CPU. The kernel and user modes of the CPU provide an effective method
                    of preventing interference between programs. Recall from Section 2.2 that the
                    OS puts the CPU in the user mode while executing user programs, and that
                    instructions that load an address into the base register and a number into the
                    size register of the CPU, respectively, are privileged instructions. If a program
                    tries to undermine memory protection by changing contents of the base and size
                    registers through these instructions, a program interrupt would be raised because



                                               Chapter 3             Overview of Operating  Systems  61
Table 3.4        Architectural Support for Multiprogramming
Feature            Description
DMA                The CPU initiates an I/O operation when an I/O instruction
                   is executed. The DMA implements the data transfer
                   involved in the I/O operation without involving the CPU
                   and raises an I/O interrupt when the data transfer completes.
Memory protection  A program can access only the part of memory defined by
                   contents of the base register and size register.
Kernel and user    Certain instructions, called privileged instructions, can be
modes of CPU       performed only when the CPU is in the kernel mode. A
                   program interrupt is raised if a program tries to execute a
                   privileged instruction when the CPU is in the user mode.
the CPU is in the user mode; the kernel would abort the program while servicing
this interrupt.
The turnaround time of a program is the appropriate measure of user service
in a multiprogramming system. It depends on the total number of programs in
the system, the manner in which the kernel shares the CPU between programs,
and the program's own execution requirements.
3.5.1 Priority of Programs
An appropriate measure of performance of a multiprogramming OS is through-
put, which is the ratio of the number of programs processed and the total time
taken to process them. Throughput of a multiprogramming OS that processes n
programs in the interval between times t0 and tf is n/(tf - t0). It may be larger
than the throughput of a batch processing system because activities in several
programs may take place simultaneously--one program may execute instruc-
tions on the CPU, while some other programs perform I/O operations. However,
actual throughput depends on the nature of programs being processed, i.e., how
much computation and how much I/O they perform, and how well the kernel can
overlap their activities in time.
The OS keeps a sufficient number of programs in memory at all times, so that
the CPU and I/O devices will have sufficient work to perform. This number is
called the degree of multiprogramming. However, merely a high degree of multi-
programming cannot guarantee good utilization of both the CPU and I/O devices,
because the CPU would be idle if each of the programs performed I/O operations
most of the time, or the I/O devices would be idle if each of the programs per-
formed computations most of the time. So the multiprogramming OS employs the
two techniques described in Table 3.5 to ensure an overlap of CPU and I/O activ-
ities in programs: It uses an appropriate program mix, which ensures that some
of the programs in memory are CPU-bound programs, which are programs that



62  Part 1  Overview
            Table 3.5       Techniques of Multiprogramming
            Technique          Description
            Appropriate        The kernel keeps a mix of CPU-bound and I/O-bound programs
            program mix        in memory, where
                               ·  A CPU-bound program is a program involving a lot of
                                  computation and very little I/O. It uses the CPU in long
                                  bursts--that is, it uses the CPU for a long time before
                                  starting an I/O operation.
                               ·  An I/O-bound program involves very little computation and a
                                  lot of I/O. It uses the CPU in small bursts.
            Priority-based     Every program is assigned a priority. The CPU is always
            preemptive         allocated to the highest-priority program that wishes to use it.
            scheduling         A low-priority program executing on the CPU is preempted if a
                               higher-priority program wishes to use the CPU.
            involve a lot of computation but few I/O operations, and others are I/O-bound pro-
            grams, which contain very little computation but perform more I/O operations.
            This way, the programs being serviced have the potential to keep the CPU and I/O
            devices busy simultaneously. The OS uses the notion of priority-based preemptive
            scheduling to share the CPU among programs in a manner that would ensure
            good overlap of their CPU and I/O activities. We explain this technique in the
            following.
            Definition   3.4   Priority  A tie-breaking criterion under which a scheduler
            decides     which  request   should  be  scheduled  when  many      requests         await
            service.
                      The kernel assigns numeric priorities to programs. We assume that priorities
            are positive integers and a large value implies a high priority. When many pro-
            grams need the CPU at the same time, the kernel gives the CPU to the program
            with the highest priority. It uses priority in a preemptive manner; i.e., it preempts
            a low-priority program executing on the CPU if a high-priority program needs
            the CPU. This way, the CPU is always executing the highest-priority program
            that needs it. To understand implications of priority-based preemptive schedul-
            ing, consider what would happen if a high-priority program is performing an I/O
            operation, a low-priority program is executing on the CPU, and the I/O operation
            of the high-priority program completes--the kernel would immediately switch the
            CPU to the high-priority program.
                      Assignment of priorities to programs is a crucial decision that can influence
            system throughput. Multiprogramming systems use the following priority assign-
            ment rule: An I/O-bound program should have a higher priority than a CPU-bound
            program. Example 3.1 illustrates operation of this rule.



                                                                   Chapter 3       Overview of Operating Systems       63
                                                                                                                       ·
Execution of Programs in a Multiprogramming System                                        Example                 3.1
A multiprogramming system has progiob, an I/O-bound program, and progcb,
a CPU-bound program. Its operation starts at time 0. In Figure 3.4, the CPU
and I/O activities of these programs are plotted in the form of a timing chart in
which the x axis shows time and the y axis shows CPU and I/O activities of the
two programs. Cumulative CPU and I/O activities are shown at the bottom of
the chart. Note that the chart is not to scale; the CPU activity of progiob has
been exaggerated for clarity.
        Program progiob is the higher priority program. Hence it starts executing
at time 0. After a short burst of CPU activity, it initiates an I/O operation (time
instant t1). The CPU is now switched to progcb. Execution of progcb is thus
concurrent with the I/O operation of progiob. Being a CPU-bound program,
progcb keeps the CPU busy until progiob's I/O completes at t2, at which time
progcb is preempted because progiob has a higher priority. This sequence of
events repeats in the period 0­t6. Deviations from this behavior occur when
progcb initiates an I/O operation. Now both programs are engaged in I/O
operations, which go on simultaneously because the programs use different
I/O devices, and the CPU remains idle until one of them completes its I/O
operation. This explains the CPU-idle periods t6­t7 and t8­t9 in the cumulative
CPU activity. I/O-idle periods occur whenever progiob executes on the CPU
and progcb is not performing I/O (see intervals 0 - t1, t2­t3, and t4­t5). But
the CPU and the I/O subsystem are concurrently busy in the intervals t1­t2,
t3­t4, t5­t6, and t7­t8.
                                                                                       ·
                      CPU activity
             progiob
                      I/O activity
                      CPU activity
             progcb
                      I/O activity
                                    0  t1    t2  t3    t4  t5  t6  t7    t8  t9  Time
                      Cumulative                 Busy              Busy      Busy
                      CPU activity
                      Cumulative       Busy      Busy              Busy
                      I/O activity
Figure  3.4  Timing   chart when I/O-bound program has higher priority.



64  Part 1  Overview
            Table 3.6           Effect  of  Increasing the Degree of Multiprogramming
            Action                               Effect
            Add a CPU-bound                      A CPU-bound program (say, prog3) can be
            program                              introduced to utilize some of the CPU time that was
                                                 wasted in Example 3.1 (e.g., the intervals t6­t7 and
                                                 t8­t9). prog3 would have the lowest priority. Hence
                                                 its presence would not affect the progress of progcb
                                                 and progiob.
            Add an I/O-bound                     An I/O-bound program (say, prog4) can be
            program                              introduced. Its priority would be between the
                                                 priorities of progiob and progcb. Presence of prog4
                                                 would improve I/O utilization. It would not affect
                                                 the progress of progiob at all, since progiob has the
                                                 highest priority, and it would affect the progress of
                                                 progcb only marginally, since prog4 does not use a
                                                 significant amount of CPU time.
                      We can make a few observations from Example 3.1: The CPU utilization
            is good. The I/O utilization is also good; however, I/O idling would exist if
            the system contained many devices capable of operating in the DMA mode.
            Periods   of   concurrent       CPU  and     I/O  activities  are  frequent.   progiob      makes
            very good progress because it is the highest-priority program. It makes very
            light     use  of   the  CPU,   and  so  progcb   also  makes      very  good  progress.    The
            throughput is thus substantially higher than if the programs were executed
            one after another as in a batch processing system. Another important fea-
            ture of this priority assignment is that system throughput can be improved
            by adding more programs. Table 3.6 describes how addition of a CPU-bound
            program        can  reduce  CPU      idling  without    affecting  execution   of  other    pro-
            grams, while addition of an I/O-bound program can improve I/O utilization
            while marginally affecting execution of CPU-bound programs. The kernel can
            judiciously add CPU-bound or I/O-bound programs to ensure efficient use of
            resources.
                      When an appropriate program mix is maintained, we can expect that an
            increase in the degree of multiprogramming would result in an increase in
            throughput. Figure 3.5 shows how the throughput of a system actually varies
            with the degree of multiprogramming. When the degree of multiprogramming
            is 1, the throughput is dictated by the elapsed time of the lone program in the
            system. When more programs exist in the system, lower-priority programs also
            contribute to throughput. However, their contribution is limited by their oppor-
            tunity to use the CPU. Throughput stagnates with increasing values of the degree
            of multiprogramming if low-priority programs do not get any opportunity to
            execute.



                                                                  Chapter 3     Overview of Operating  Systems  65
             Throughput
                                      1     2         3
                                               Degree of multiprogramming
Figure 3.5   Variation of throughput with degree of multiprogramming.
             A process finishes
             I/O or a new subrequest
             is made to it
                     ...                                               Time slice
                                                                           is over
                                 Scheduler                  CPU
             Scheduling                                                Subrequest is
                     queue                  Selected              completed, or I/O
                                            process               operation is started
Figure  3.6  A schematic of round-robin scheduling with time-slicing.
3.6     TIME-SHARING SYSTEMS                                                                                    ·
In  an  interactive  computing        environment,       a  user  submits    a  computational
requirement--a subrequest--to a process and examines its response on the mon-
itor screen. A time-sharing operating system is designed to provide a quick
response to subrequests made by users. It achieves this goal by sharing the CPU
time among processes in such a way that each process to which a subrequest has
been made would get a turn on the CPU without much delay.
     The scheduling technique used by a time-sharing kernel is called round-robin
scheduling with time-slicing. It works as follows (see Figure 3.6): The kernel main-
tains a scheduling queue of processes that wish to use the CPU; it always schedules
the process at the head of the queue. When a scheduled process completes ser-
vicing of a subrequest, or starts an I/O operation, the kernel removes it from
the queue and schedules another process. Such a process would be added at the
end of the queue when it receives a new subrequest, or when its I/O operation
completes. This arrangement ensures that all processes would suffer comparable



66  Part 1  Overview
                 delays before getting to use the CPU. However, response times of processes would
                 degrade if a process consumes too much CPU time in servicing its subrequest.
                 The kernel uses the notion of a time slice to avoid this situation. We use the
                 notation  for the time slice.
                 Definition 3.5 Time Slice      The largest amount of CPU time any time-shared
                 process can consume when scheduled to execute on the CPU.
                      If the time slice elapses before the process completes servicing of a subrequest,
                 the kernel preempts the process, moves it to the end of the scheduling queue, and
                 schedules another process. The preempted process would be rescheduled when it
                 reaches the head of the queue once again. Thus, a process may have to be scheduled
                 several times before it completes servicing of a subrequest. The kernel employs a
                 timer interrupt to implement time-slicing (see Section 2.2.5 and Table 2.2).
                      The appropriate measure of user service in a time-sharing system is the time
                 taken to service a subrequest, i.e., the response time (rt). It can be estimated
                 in the following manner: Let the number of users using the system at any time
                 be n. Let the complete servicing of each user subrequest require exactly  CPU
                 seconds, and let  be the scheduling overhead; i.e., the CPU time consumed by
                 the kernel to perform scheduling. If we assume that an I/O operation completes
                 instantaneously and a user submits the next subrequest immediately after receiv-
                 ing a response to the previous subrequest, the response time (rt) and the CPU
                 efficiency () are given by
                                                rt = n × ( +  )                                       (3.1)
                                                =                                                     (3.2)
                                                   +
                 The actual response time may be different from the value of rt predicted by
                 Eq. (3.1), for two reasons. First, all users may not have made subrequests to their
                 processes. Hence rt would not be influenced by n, the total number of users in the
                 system; it would be actually influenced by the number of active users. Second,
                 user subrequests do not require exactly  CPU seconds to produce a response.
                 Hence the relationship of rt and  with  is more complex than shown in Eqs. (3.1)
                 and (3.2).
                      Example 3.2 illustrates round-robin scheduling with time-slicing, and how it
                 results in interleaved operation of processes.
·
    Example 3.2  Operation of Processes in a Time-Sharing System
                 Processes P1 and P2 follow a cyclic behavior pattern. Each cycle contains a
                 burst of CPU activity to service a subrequest and a burst of I/O activity to
                 report its result, followed by a wait until the next subrequest is submitted to it.
                 The CPU bursts of processes P1 and P2 are 15 and 30 ms, respectively, while
                 the I/O bursts are 100 and 60 ms, respectively.



                                                                      Chapter 3  Overview of Operating Systems  67
    Figure 3.7 shows operation of the processes in a time-sharing system using
a time slice of 10 ms. The table in the top half of Figure 3.7 shows the scheduling
list and scheduling decisions of the kernel, assuming scheduling overhead to
be negligible, while the timing chart shows the CPU and I/O activities of the
processes. Both processes have to be scheduled a few times before they can
complete the CPU bursts of their execution cycle and start I/O. Process P1 uses
the CPU from time 0 to 10 ms and P2 uses the CPU from 10 to 20 ms without
completing the CPU bursts of their execution cycles. P1 is scheduled once again
at 20 ms and starts an I/O operation at 25 ms. Now P2 gets two consecutive
time slices. However, these time slices are separated by the scheduling overhead
because the OS preempts process P2 at 35 ms and schedules it again, since no
other process in the system needs the CPU. P1's I/O operation completes at
125 ms. P2 starts an I/O operation at 45 ms, which completes at 105 ms. Thus,
the response times are 125 ms and 105 ms, respectively.
                                                                                       ·
3.6.1 Swapping of Programs
Throughput of subrequests is the appropriate measure of performance of a time-
sharing operating system. The time-sharing OS of Example 3.2 completes two
subrequests in 125 ms, hence its throughput is 8 subrequests per second over the
period 0 to 125 ms. However, the throughput would drop after 125 ms if users
do not make the next subrequests to these processes immediately. The CPU is
                      Scheduling              Scheduled
Time                  list                    program     Remarks
0                     P1, P2                  P1          P1 is preempted at 10 ms
10                    P2, P1                  P2          P2 is preempted at 20 ms
20                    P1, P2                  P1          P1 starts I/O at 25 ms
25                    P2                      P2          P2 is preempted at 35 ms
35                    P2                      P2          P2 starts I/O at 45 ms
45                    -                       -           CPU is idle
CPU activity  P1
              P2
I/O activity  P1
              P2
                  0           20         40           60  80          100        120
                                                                                 Time
Figure 3.7 Operation  of  processes  P1  and  P2  in  a time-sharing  system.



68  Part 1  Overview
                        Kernel          Kernel     Kernel          Kernel
                                    P1       P1         P4              P1
                                    P2       P3         P3              P3
                        (a)             (b)        (c)             (d)
            Figure 3.8  Swapping: (a) processes in memory between 0 and 105 ms; (b) P2 is      replaced  by
            P3 at 105 ms; (c) P1 is replaced by P4 at 125 ms; (d) P1 is swapped in to service  the next
            subrequest made to it.
            idle after 45 ms because it has no work to perform. It could have serviced a few
            more subrequests, had more processes been present in the system. But what if
            only two processes could fit in the computer's memory? The system throughput
            would be low and response times of processes other than P1 and P2 would suffer.
            The technique of swapping is employed to service a larger number of processes
            than can fit into the computer's memory. It has the potential to improve both
            system performance and response times of processes.
            Definition 3.6 Swapping     The technique of temporarily removing a process
            from the memory of a computer system.
                      The kernel performs a swap-out operation on a process that is not likely to get
            scheduled in the near future by copying its instructions and data onto a disk. This
            operation frees the area of memory that was allocated to the process. The kernel
            now loads another process in this area of memory through a swap-in operation.
            The kernel would overlap the swap-out and swap-in operations with servicing of
            other processes on the CPU, and a swapped-in process would itself get scheduled
            in due course of time. This way, the kernel can service more processes than can
            fit into the computer's memory. Figure 3.8 illustrates how the kernel employs
            swapping. Initially, processes P1 and P2 exist in memory. These processes are
            swapped out when they complete handling of the subrequests made to them, and
            they are replaced by processes P3 and P4, respectively. The processes could also
            have been swapped out when they were preempted. A swapped-out process is
            swapped back into memory before it is due to be scheduled again, i.e., when it
            nears the head of the scheduling queue in Figure 3.6.
            3.7       REAL-TIME OPERATING SYSTEMS                                                            ·
            In a class of applications called real-time applications, users need the computer to
            perform some actions in a timely manner to control the activities in an external
            system, or to participate in them. The timeliness of actions is determined by



                                       Chapter 3  Overview of Operating                 Systems  69
the time constraints of the external system. Accordingly, we define a real-time
application as follows:
Definition 3.7 Real-Time Application  A program that responds to activities in
an external system within a maximum time determined by the external system.
If the application takes too long to respond to an activity, a failure can
occur in the external system. We use the term response requirement of a system
to indicate the largest value of response time for which the system can function
perfectly; a timely response is one whose response time is not larger than the
response requirement of the system.
Consider a system that logs data received from a satellite remote sensor.
The satellite sends digitized samples to the earth station at the rate of 500 samples
per second. The application process is required to simply store these samples in
a file. Since a new sample arrives every two thousandth of a second, i.e., every
2 ms, the computer must respond to every "store the sample" request in less than
2 ms, or the arrival of a new sample would wipe out the previous sample in the
computer's memory. This system is a real-time application because a sample must
be stored in less than 2 ms to prevent a failure. Its response requirement is 1.99 ms.
The deadline of an action in a real-time application is the time by which the action
should be performed. In the current example, if a new sample is received from
the satellite at time t, the deadline for storing it on disk is t + 1.99 ms.
Examples of real-time applications can be found in missile guidance, com-
mand and control applications like process control and air traffic control, data
sampling and data acquisition systems like display systems in automobiles, multi-
media systems, and applications like reservation and banking systems that employ
large databases. The response requirements of these systems vary from a few
microseconds or milliseconds for guidance and control systems to a few seconds
for reservation and banking systems.
3.7.1 Hard and Soft Real-Time Systems
To take advantage of the features of real-time systems while achieving maximum
cost-effectiveness, two kinds of real-time systems have evolved. A hard real-time
system is typically dedicated to processing real-time applications, and provably
meets the response requirement of an application under all conditions. A soft
real-time system makes the best effort to meet the response requirement of a
real-time application but cannot guarantee that it will be able to meet it under
all conditions. Typically, it meets the response requirements in some probabilistic
manner, say, 98 percent of the time. Guidance and control applications fail if they
cannot meet the response requirement, hence they are serviced by hard real-time
systems. Applications that aim at providing good quality of service, e.g., multi-
media applications and applications like reservation and banking, do not have a
notion of failure, so they may be serviced by soft real-time systems--the picture
quality provided by a video-on-demand system may deteriorate occasionally, but
one can still watch the video!



70  Part 1  Overview
            3.7.2 Features of a Real-Time Operating System
            A real-time OS provides the features summarized in Table 3.7. The first three
            features help an application in meeting the response requirement of a system
            as follows: A real-time application can be coded such that the OS can execute
            its parts concurrently, i.e., as separate processes. When these parts are assigned
            priorities and priority-based scheduling is used, we have a situation analogous
            to multiprogramming within the application--if one part of the application ini-
            tiates an I/O operation, the OS would schedule another part of the application.
            Thus, CPU and I/O activities of the application can be overlapped with one
            another, which helps in reducing the duration of an application, i.e., its running
            time. Deadline-aware scheduling is a technique used in the kernel that schedules
            processes in such a manner that they may meet their deadlines.
                      Ability to specify domain-specific events and event handling actions enables
            a real-time application to respond to special conditions in the external system
            promptly. Predictability of policies and overhead of the OS enables an applica-
            tion developer to calculate the worst-case running time of the application and
            decide whether the response requirement of the external system can be met. The
            predictability requirement forces a hard real-time OS to shun features such as vir-
            tual memory whose performance cannot be predicted precisely (see Chapter 12).
            The OS would also avoid shared use of resources by processes, because it can lead
            to delays that are hard to predict and unbounded, i.e., arbitrarily large.
                      A real-time OS employs two techniques to ensure continuity of operation
            when faults occur--fault tolerance and graceful degradation. A fault-tolerant
            computer system uses redundancy of resources to ensure that the system will
            keep functioning even if a fault occurs; e.g., it may have two disks even though
            the application actually needs only one disk. Graceful degradation is the ability
            of a system to fall back to a reduced level of service when a fault occurs and
            to revert to normal operations when the fault is rectified. The programmer can
            Table 3.7       Essential Features of a Real-Time Operating System
            Feature             Explanation
            Concurrency         A programmer can indicate that some parts of an application
            within an           should be executed concurrently with one another. The OS
            application         considers execution of each such part as a process.
            Process priorities  A programmer can assign priorities to processes.
            Scheduling          The OS uses priority-based or deadline-aware scheduling.
            Domain-specific     A programmer can define special situations within the external
            events, interrupts  system as events, associate interrupts with them, and specify
                                event handling actions for them.
            Predictability      Policies and overhead of the OS should be predictable.
            Reliability         The OS ensures that an application can continue to function
                                even when faults occur in the computer.



                                                            Chapter 3  Overview of Operating  Systems  71
assign high priorities to crucial functions so that they would be performed in a
timely manner even when the system operates in a degraded mode.
3.8  DISTRIBUTED OPERATING SYSTEMS                                                                     ·
A distributed computer system consists of several individual computer systems
connected through a network. Each computer system could be a PC, a mul-
tiprocessor system (see Chapter 10), or a cluster, which is itself a group of
computers that work together in an integrated manner (see Section 16.2). Thus,
many resources of a kind, e.g., many memories, CPUs and I/O devices, exist in
the distributed system. A distributed operating system exploits the multiplicity
of resources and the presence of a network to provide the benefits summarized in
Table 3.8. However, the possibility of network faults or faults in individual com-
puter systems complicates functioning of the operating system and necessitates
use of special techniques in its design. Users also need to use special techniques
to access resources over the network. We discuss these aspects in Section 3.8.1.
     Resource sharing has been the traditional motivation for distributed operat-
ing systems. A user of a PC or workstation can use resources such as printers
over a local area network (LAN), and access specialized hardware or software
resources of a geographically distant computer system over a wide area network
(WAN).
     A distributed operating system provides reliability through redundancy of
computer systems, resources, and communication paths--if a computer system
or a resource used in an application fails, the OS can switch the application to
another computer system or resource, and if a path to a resource fails, it can utilize
another path to the resource. Reliability can be used to offer high availability of
resources and services, which is defined as the fraction of time a resource or service
is operable. High availability of a data resource, e.g., a file, can be provided by
keeping copies of the file in various parts of the system.
     Computation speedup implies a reduction in the duration of an application,
i.e., in its running time. It is achieved by dispersing processes of an application
Table 3.8    Benefits  of Distributed Operating Systems
Benefit                Description
Resource sharing       Resources can be utilized across boundaries of individual
                       computer systems.
Reliability            The OS continues to function even when computer
                       systems or resources in it fail.
Computation speedup    Processes of an application can be executed in different
                       computer systems to speed up its completion.
Communication          Users can communicate among themselves irrespective of
                       their locations in the system.



72  Part 1  Overview
            to different computers in the distributed system, so that they can execute at the
            same time and finish earlier than if they were to be executed in a conventional OS.
                      Users of a distributed operating system have user ids and passwords that
            are valid throughout the system. This feature greatly facilitates communication
            between users in two ways. First, communication through user ids automatically
            invokes the security mechanisms of the OS and thus ensures authenticity of com-
            munication. Second, users can be mobile within the distributed system and still
            be able to communicate with other users through the system.
            3.8.1 Special Techniques of Distributed Operating Systems
            A distributed system is more than a mere collection of computers connected to
            a network--functioning of individual computers must be integrated to achieve
            the benefits summarized in Table 3.8. It is achieved through participation of
            all computers in the control functions of the operating system. Accordingly, we
            define a distributed system as follows:
            Definition 3.8 Distributed System        A system consisting of two or more nodes,
            where each node is a computer system with its own clock and memory, some
            networking hardware, and a capability of performing some of the control
            functions of an OS.
                      Table 3.9 summarizes three key concepts and techniques used in a distributed
            OS. Distributed control is the opposite of centralized control--it implies that the
            control functions of the distributed system are performed by several computers
            in the system in the manner of Definition 3.8, instead of being performed by
            a single computer. Distributed control is essential for ensuring that failure of a
            single computer, or a group of computers, does not halt operation of the entire
            system. Transparency of a resource or service implies that a user should be able to
            access it without having to know which node in the distributed system contains
            it. This feature enables the OS to change the position of a software resource or
            service to optimize its use by applications. For example, in a system providing
            Table 3.9     Key Concepts and Techniques Used in a Distributed OS
            Concept/Technique    Description
            Distributed control  A control function is performed through participation of
                                 several nodes, possibly all nodes, in a distributed system.
            Transparency         A resource or service can be accessed without having to
                                 know its location in the distributed system.
            Remote procedure     A process calls a procedure that is located in a different
            call (RPC)           computer system. The RPC is analogous to a procedure or
                                 function call in a programming language, except that the OS
                                 passes parameters to the remote procedure over the network
                                 and returns its results over the network.



                                      Chapter 3                        Overview of Operating Systems  73
transparency, a distributed file system could move a file to the node that contains
a computation using the file, so that the delays involved in accessing the file over
the network would be eliminated. The remote procedure call (RPC) invokes a
procedure that executes in another computer in the distributed system. An appli-
cation may employ the RPC feature to either perform a part of its computation in
another computer, which would contribute to computation speedup, or to access
a resource located in that computer.
3.9  MODERN OPERATING SYSTEMS                                                                         ·
Users engage in diverse activities in a modern computing environment. Hence a
modern operating system cannot use a uniform strategy for all processes; it must
use a strategy that is appropriate for each individual process. For example, as
mentioned in Section 3.1, a user may open a mail handler, edit a few files, execute
some programs, including some programs in the background mode, and watch a
video at the same time. Here, operation of some of the programs may be inter-
active or may involve activities in other nodes of a distributed computer system,
whereas rendering of a video is a soft real-time activity. Hence the OS must use
round-robin scheduling for program executions, use priority-based scheduling for
processes of the video application, and implement remote procedure calls (RPC)
to support activities in another node. Thus, a modern OS uses most concepts and
techniques that we discussed in connection with the batch processing, multipro-
gramming, time-sharing, real-time, and distributed operating systems. Table 3.10
shows typical examples of how the earlier concepts are drawn upon.
     To handle diverse activities effectively, the OS employs strategies that adapt
to the situations encountered during their operation. Some examples of such
strategies are:
·    The kernel employs priority-based scheduling; however, instead of assigning
     fixed priorities to all processes as in a multiprogramming system, it assigns
     fixed high priorities only to processes with real-time constraints, and changes
     current priorities of other processes to suit their recent behavior--increases
     the priority of a process if it has been engaged in an interaction or an I/O
     operation recently, and reduces its priority if it has not been.
·    A modern OS typically uses the feature called virtual memory, whereby only
     some of the parts of a process are held in memory at any time and other
     parts are loaded when needed. The kernel considers the recent behavior of
     a process to decide how much memory it should allocate to the process--it
     allocates less memory if the process had used only a few of its parts recently,
     and allocates more memory if the process had used several of its parts.
·    The kernel provides a plug-and-play capability whereby I/O devices could be
     connected to the computer at any time during its operation, and the kernel
     would select appropriate methods of handling them.
We will see several instances of adaptive strategies in the following chapters.



74               Part 1  Overview
                             Table 3.10           Use of Classical OS Concepts in Modern
                             Computing Environments
                               Concept                  Typical example of use
                             Batch processing           To avoid time-consuming initializations for each use of a
                                                        resource; e.g., database transactions are batch-processed
                                                        in the back office and scientific computations are
                                                        batch-processed in research organizations and clinical
                                                        laboratories.
                             Priority-based             To provide a favored treatment to high-priority
                             preemptive scheduling      applications, and to achieve efficient use of resources by
                                                        assigning high priorities to interactive processes and low
                                                        priorities to noninteractive processes.
                             Time-slicing               To prevent a process from monopolizing the CPU; it helps
                                                        in providing good response times.
                             Swapping                   To increase the number of processes that can be serviced
                                                        simultaneously; it helps in improving system performance
                                                        and response times of processes.
                             Creating multiple          To reduce the duration of an application; it is most
                             processes in an            effective when the application contains substantial CPU
                             application                and I/O activities.
                             Resource sharing           To share resources such as laser printers or services such
                                                        as file servers in a LAN environment.
3.10      SUMMARY                                                                                                   ·
A computing environment consists of a computer                 execute instructions while an I/O operation was
system,     its  interfaces  with      other  systems,  and    in progress. Operating systems exploited this fea-
the services provided by its operating system to               ture to service several programs simultaneously by
its  users  and  their   programs.      Computing       envi-  overlapping an I/O operation within one program
ronments    evolved      with      advances   in    computer   with execution of instructions in another program.
technology and computer applications. Each envi-               A multiprogramming operating system assigned
ronment desired a different combination of effi-               high priorities to I/O-bound programs and per-
cient use and user service, so it was serviced by a            formed priority-based scheduling to achieve good
separate class of operating systems that employed              system performance.
its own concepts and techniques. In this chapter,                    User convenience became important when the
we discussed the concepts and techniques used in               cost  of  computing  hardware     declined.    Accord-
the fundamental classes of operating systems.                  ingly, the time-sharing operating systems focused
     The    batch        processing    operating    systems    on providing fast response to user programs. It
focused on automating processing of a collection of            was achieved through round-robin scheduling with
programs, which reduced CPU idle times between                 time-slicing, which serviced all programs by turn
programs.        Development       of  the  direct  memory     and limited the amount of CPU time a program
access (DMA) technology enabled the CPU to                     could use when it was its turn to use the CPU.



                                                                  Chapter 3  Overview of Operating Systems              75
    A    real-time     computer      application      has     to  performs its control functions in several of these
satisfy time constraints specified by an external                 computers. It achieves efficient use of resources of
system. Hard real-time systems such as mission                    all computers by letting programs share them over
control systems require their time constraints to                 the network, speeds up execution of a program by
be  satisfied  in   a     guaranteed    manner,       whereas     running its parts in different computers at the same
soft real-time systems such as multimedia systems                 time, and provides reliability through redundancy
can tolerate occasional failure to meet their time                of resources and services.
constraints. Real-time operating systems support                  A modern operating system controls a diverse
concurrency within an application program and                     computing environment that has elements of all
employ techniques such as priority-based schedul-                 the classic computing environments, so it has to
ing and deadline-aware scheduling to help meet the                use different techniques for different applications.
time constraints.                                                 It employs an adaptive strategy that selects the
    A    distributed      operating     system  controls      a   most appropriate techniques for each application
group of computer systems that are networked; it                  according to its nature.
TEST     YOUR CONCEPTS                                                                                                       ·
    3.1  Programs   A,    B,  C,     and  D     have  similar          b. Throughput increases almost linearly with
         structure--each of them consists of a single loop                   the degree of multiprogramming
         that contains n statements that perform some                  c. CPU efficiency changes only marginally with
         processing on each element of a single dimen-                       the degree of multiprogramming
         sioned array Z. Other features of these programs              d. CPU efficiency increases linearly with the
         are as follows:                                                     degree of multiprogramming
                                                                  3.3  Classify each of the following statements as true
         Program A:       n = 4 and Z is a huge array.                 or false:
         Program B:       n = 100 and Z is a huge array.               a. Because of presence of the cache memory,
         Program C:       n = 4 and Z is a small array.                      a program requires more CPU time to exe-
         Program D:       n = 100 and Z is a small                           cute in a multiprogramming or time-sharing
                          array.                                             system than it would require if it were to be
                                                                             executed in a batch processing system.
         These programs are executed in a batch process-               b. To  achieve    high  throughput,   a  multipro-
         ing system. List these programs in the descend-                     gramming OS assigns a higher priority to
         ing order by cache hit ratio.                                       CPU-bound programs.
    3.2  A multiprogramming system is used to execute                  c. If a multiprogramming kernel finds that the
         a collection of programs C. The system has                          CPU efficiency is low, it should remove an
         enough memory to accommodate a large num-                           I/O-bound program from memory.
         ber of programs. The programs in C are executed               d. If the time slice in a time-sharing system is
         several times, each time with a different degree of                 too large, processes will complete their oper-
         multiprogramming, and throughput of the sys-                        ation in the same order in which they were
         tem and CPU efficiency are plotted against the                      initiated.
         degree of multiprogramming. In each of the fol-               e. Two persons using the same time-sharing sys-
         lowing cases, what inference can you draw about                     tem at the same time might receive widely
         the nature of programs in C?                                        different response times.
         a. Throughput changes only marginally with                    f. It is incorrect to use masking of interrupts in
         the degree of multiprogramming                                      a real-time operating system.



76       Part 1       Overview
EXERCISES                                                                                                                ·
    3.1  A system is described as overloaded if more work           operation that lasts for 200 ms. The program is
         is directed at it than its capacity to perform             executed in a multiprogramming OS with negli-
         work. It is considered underloaded if some of              gible overhead. Prepare a timing chart showing
         its capacity is going to waste. The following pol-         the CPU and I/O activities of the program and
         icy is proposed to improve the throughput of a             compute its elapsed time in the following cases:
         batch processing system: Classify jobs into small          a. The program has the highest priority in the
         jobs and long jobs depending on their CPU time             system.
         requirements. Form separate batches of short               b. The    program    is   multiprogrammed   with     n
         and long jobs. Execute a batch of long jobs only           other programs with identical characteristics
         if no batches of short jobs exist. Does this policy        and has the lowest priority. Consider cases
         improve the throughput of a batch processing               (i) n = 3, (ii) n = 4, and (iii) n = 5.
         system that is: (a) underloaded? (b) overloaded?     3.7   A multiprogramming operating system has a
    3.2  The kernel of a multiprogramming system classi-            negligible overhead. It services programs that
         fies a program as CPU-bound or I/O-bound and               are identical in size. Each program contains a
         assigns an appropriate priority to it. What would          loop that has n iterations, where each itera-
         be the consequence of a wrong classification of            tion contains computations that consume tc ms
         programs for throughput and turnaround times               of CPU time, followed by I/O operations that
         in a multiprogramming system? What would be                require tio ms. The programs are of two classes;
         the effect of a wrong classification on the plot of        values of n, tc, and tio for these two classes are:
         throughput versus degree of multiprogramming                            Class        n  tc      tio
         of Figure 3.5?                                                          A            5  15      100
    3.3  The CPU of a multiprogramming system is exe-                                 B       6  200     80
         cuting a high-priority program when an inter-
         rupt signaling completion of an I/O operation              The system has sufficient memory to accommo-
         occurs. Show all actions and activities in the OS          date only two programs. Ten programs arrive in
         following the interrupt if                                 the system at time 0, five each of classes A and B.
         a. The I/O operation was started by a lower-               Draw a timing chart showing operation of pro-
         priority program                                           grams in the system until two programs complete
         b. The I/O operation was started by a higher-              their operation. Find their turnaround times.
         priority program.                                    3.8   A program is said to "make progress" if either
         Illustrate each case with the help of a timing             the CPU is executing its instructions or its I/O
         chart.                                                     operation is in progress. The progress coefficient
    3.4  A multiprogramming OS has programs progiob                 of a program is the fraction of its lifetime in the
         and progcb in memory, with progcb having a                 system during which it makes progress. Com-
         higher priority. Draw a timing chart for the sys-          pute progress coefficients of the programs in
         tem analogous to Figure 3.4, and show that                 Exercise 3.6(b).
         the throughput is less than for the system of        3.9   Comment on the validity of the following state-
         Figure 3.4.                                                ment: "A CPU-bound program always has a very
    3.5  Draw a timing chart for a system containing two            low progress coefficient in a multiprogramming
         CPU-bound programs and two I/O-bound pro-                  system."
         grams when (a) CPU-bound programs have a             3.10  A multiprogramming system uses a degree of
         higher priority, (b) I/O-bound programs have a             multiprogramming (m)             1. It is proposed to
         higher priority.                                           double the throughput of the system by augmen-
    3.6  A program consists of a single loop that executes          tation/replacement of its hardware components.
         50 times. The loop contains a computation that             Would   any  of      the  following  three  proposals
         consumes 50 ms of CPU time, followed by an I/O             achieve the desired result?



                                                                   Chapter 3   Overview of Operating Systems                     77
      a. Replace the CPU by a CPU with twice the                   3.16  A computer is operated under a time-sharing
      speed.                                                             OS. It is proposed to add a second CPU to
      b. Expand the memory to twice its present size.                    the computer to improve its throughput. Under
      c. Replace the CPU by a CPU with twice the                         what  conditions       would     addition  of   the     sec-
      speed and expand the memory to twice its                           ond   CPU   improve     throughput   only       if  mem-
      present size.                                                      ory is increased? Under what conditions would
3.11  Programs     being  serviced  in   a  multiprogram-                it improve throughput even if memory is not
      ming system are named P1, . . . , Pm, where m                      increased?
      is the degree of multiprogramming, such that                 3.17  A time-sharing system uses swapping as the fun-
      priority of program Pi     > priority of program                   damental memory management technique. It
      Pi+1. All programs are cyclic in nature, with                      uses the following lists to govern its actions:
      each cycle containing a burst of CPU activity                      a scheduling list, a swapped-out list contain-
      and a burst of I/O activity. Let bicpu and biio be                 ing processes that are swapped out, a being-
      the CPU and I/O bursts of program Pi. Com-                         swapped-out      list   containing  processes       to  be
      ment on the validity of each of the following                      swapped out, and a being-swapped-in list con-
      statements:                                                        taining   processes     to   be  swapped   in.  Explain
      a. CPU idling occurs if biho >     j=h(bcjpu), where               when and why the time-sharing kernel should
      Ph is the highest-priority program.                                put   processes  in     the  being-swapped-out          and
      b. Program      Pm  is  guaranteed    to     receive               being-swapped-in lists.
      CPU time if biio < (bic+pu1 + biio+1) and biio >             3.18  A time-sharing system uses a time slice of 100 ms.
           j=i+1...m(bcj pu) for all values of i = 1, . . . ,            Each process has a cyclic behavior pattern. In
      m - 1,                                                             each cycle, it requires an average of 50 ms of
3.12  A program is said to starve if it does not receive                 CPU time to compute the result of a subrequest
      any CPU time. Which of the following condi-                        and an average of 150 ms to print it on the user's
      tions implies starvation of the lowest-priority                    screen. A process receives a new subrequest 1 sec-
      program in a multiprogramming system? (The                         ond after it has finished printing results of the
      notation is the same as in Exercise 3.11.)                         previous subrequest. The operating system can
      a. For some program Pi, biio <        j =i +1...m (bcjpu ).        accommodate 10 processes in memory at any
                                                                         time; however, it has enough I/O devices for 25
      b. For some program Pi, biio <        j =i +1...m (bcjpu )         processes. The swap-in and swap-out times of
      and bci pu > bijo for all j > i.                                   each process are ts ms each. Calculate the aver-
3.13  A time-sharing system contains n identical pro-                    age throughput of the system over a 10-second
      cesses, each executing a loop that contains a                      period in each of the following cases:
      computation requiring tp CPU seconds and an                        a. The operating system contains 10 processes.
      I/O  operation  requiring     tio  seconds.  Draw        a         b. The operating system contains 20 processes
      graph depicting variation of response time with                         and ts is 750 ms.
      values of the time slice . (Hint: Consider cases                   c. The operating system contains 20 processes
      for tp < ,  < tp < 2 × , and tp > 2 × .)                                and ts is 250 ms.
3.14  Comment on the validity of the following state-              3.19  A real-time application requires a response time
      ment: "Operation of a time-sharing system is                       of 2 seconds. Discuss the feasibility of using a
      identical with operation of a multiprogramming                     time-sharing system for the real-time application
      system executing the same programs if  exceeds                     if the average response time in the time-sharing
      the CPU burst of every program."                                   system is (a) 20 seconds, (b) 2 seconds, or (c) 0.2
3.15  Answer the following with full justifications:                     seconds.
      a. Does swapping improve or degrade the effi-                3.20  A time-sharing system services n processes. It
      ciency of system utilization?                                      uses a time slice of  CPU seconds, and requires
      b. Can swapping be used in a multiprogram-                         ts CPU seconds to switch between processes. A
      ming system?                                                       real-time application requires tc seconds of CPU



78          Part 1      Overview
         time, followed by an I/O operation that lasts                        a. Is this a real-time application? Justify your
         for tio seconds, and has to produce a response                       answer.
         within td seconds. What is the largest value of                      b. Would creation of multiple processes reduce
            for  which    the  time-sharing     system  can   sat-            the response time of the application? If so,
         isfy the response requirements of the real time                      what should be the processes in it? What
         application?                                                         should be their priorities?
3.21     An application program is being developed for                        c. Is it necessary to define any domain-specific
         a microprocessor-based controller for an auto-                       events     and  interrupts?  If   so,  specify  their
         mobile. The application is required to perform                       priorities.
         the following functions:                                   3.22      If two independent events e1 and e2 have the
           i. Monitor and display the speed of the auto-                      probabilities of occurrence pr1 and pr2, where
            mobile                                                            both  pr1  and  pr2  <  1,   the  probability   that
         ii. Monitor the fuel level and raise an alarm, if                    both events occur at the same time is pr1× pr2.
            necessary                                                         A distributed system contains two disks. The
         iii. Display the fuel efficiency, i.e., miles/gallon                 probability that both disks fail is required to
            at current speed                                                  be <0.0001. What should be the probability of
         iv. Monitor the engine condition and raise an                        failure of a disk?
            alarm if an unusual condition arises                    3.23      To obtain computation speedup in a distributed
           v. Periodically record some auxiliary informa-                     system, an application is coded as three parts to
            tion like speed and fuel level (i.e., implement                   be executed on three computer systems under
            a "black box" as in an airliner.)                                 control of a distributed operating system. How-
         Answer the following questions concerning the                        ever, the speedup obtained is <3. List all possible
         application:                                                         reasons for the poor speedup.
BIBLIOGRAPHY                                                                                                                       ·
Literature on batch processing, multiprogramming, and               Unix operating system. Beck et al. (2002), Bovet and
time-sharing systems dates back to the 1970s. Zhao                  Cesati (2005), and Love (2005) discuss the Linux oper-
(1989) and Liu (2000) are good sources for real-time                ating system, while Stevens and Rago (2005) describes
systems. Most operating systems texts cover the classes             Unix,  Linux,        and  BSD  operating    systems.  Mauro
of operating systems described in this chapter; some                and McDougall (2006) discusses Solaris. Russinovich
recent OS texts are Tanenbaum (2001), Bic and Shaw                  and Solomon (2005) describes the Windows operating
(2003),  Nutt    (2004),  Silberschatz  et      al.  (2005),  and   systems.
Stallings (2005). Several comprehensive bibliographies
on operating systems are available on the Internet.                 1.     Bach, M. J. (1986): The Design of the Unix
     Tanenbaum and Renesse (1985) is a good start-                         Operating System, Prentice Hall, Englewood
ing  point  for  a  study      of  distributed  operating     sys-         Cliffs, N.J.
tems. It discusses the major design issues in distributed           2.     Beck, M., H. Bohme, M. Dziadzka, U. Kunitz,
operating systems and contains a survey of some dis-                       R. Magnus, C. Schroter, and D. Verworner
tributed operating systems. Tanenbaum (1995) discusses                     (2002): Linux Kernel Programming, 3rd ed.,
some well-known distributed operating systems in detail.                   Pearson Education, New York.
Coulouris et al. (2001) discusses the concepts and design           3.     Bic, L., and A. C. Shaw (2003): Operating Systems
of distributed systems.                                                    Principles, Prentice Hall, Englewood Cliffs, N.J.
     Several books describe specific modern operating               4.     Bovet, D. P., and M. Cesati (2005): Understanding
systems. Bach (1986) and Vahalia (1996) describe the                       the Linux Kernel, 3rd ed., O'Reilly, Sebastopol.



                                                       Chapter 3    Overview of Operating Systems              79
5.   Coulouris, G., J. Dollimore, and T. Kindberg      17.  Sinha, P. K. (1997): Distributed Operating
     (2001): Distributed Systems--Concepts and              Systems, IEEE Press, New York.
     Design, 3rd ed., Addison-Wesley, New York.        18.  Smith, A. J. (1980): "Multiprogramming and
6.   Crowley, C. (1997): Operating Systems--A Design        memory contention," Software--Practice and
     Oriented Approach, McGraw-Hill, New York.              Experience, 10 (7), 531­552.
7.   Denning, P. J. (1971): "Third generation          19.  Stallings, W. (2005): Operating Systems--
     operating systems," Computing Surveys, 4 (1),          Internals and Design Principles, 5th ed., Pearson
     175­216.                                               Education, New York.
8.   Fortier, P. J. (1988): Design of Distributed      20.  Stevens, W. R., and S. A. Rago (2005): Advanced
     Operating Systems, McGraw-Hill, New York.              Programming in the Unix Environment, 2nd ed.,
9.   Goscinski, A. (1991): Distributed Operating            Addison-Wesley Professional.
     Systems--The Logical Design, Addison-Wesley,      21.  Tanenbaum, A. S. (2003): Computer Networks,
     New York.                                              4th ed., Prentice Hall, Englewood Cliffs, N.J.
10.  Liu, J. W. S. (2000): Real-Time systems, Pearson  22.  Tanenbaum, A. S. (2001): Modern Operating
     Education, New York.                                   Systems, 2nd ed., Prentice Hall, Englewood
11.  Love, R. (2005): Linux Kernel Development,             Cliffs, N.J.
     2nd ed., Novell Press.                            23.  Tanenbaum, A. S., and R. Van Renesse (1985):
12.  Mauro, J., and R. McDougall (2006): Solaris            "Distributed Operating Systems," Computing
     Internals, 2nd ed., Prentice Hall.                     Surveys, 17 (1), 419­470.
13.  Nutt, G. (2004): Operating Systems--A Modern      24.  Tanenbaum, A. S. (1995): Distributed Operating
     Perspective, 3rd ed., Addison-Wesley, Reading,         Systems, Prentice Hall, Englewood Cliffs, N.J.
     Mass.                                             25.  Vahalia, U. (1996): Unix Internals: The New
14.  Russinovich, M. E., and D. A. Solomon (2005):          Frontiers, Prentice Hall, Englewood
     Microsoft Windows Internals, 4th ed., Microsoft        Cliffs, N.J.
     Press, Redmond, Wash.                             26.  Wirth, N. (1969): "On multiprogramming,
15.  Silberschatz, A., P. B. Galvin, and G. Gagne           machine coding, and computer organization,"
     (2005): Operating System Principles, 7th ed.,          Communications of the ACM, 12 (9),
     John Wiley, New York.                                  489­491.
16.  Singhal, M., and N. G. Shivaratri (1994):         27.  Zhao, W. (1989): "Special issue on real-time
     Advanced Concepts in Operating Systems,                operating systems," Operating System Review,
     McGraw-Hill, New York.                                 23, 7.
