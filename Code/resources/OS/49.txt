Cache Memory

     Although cache memory is invisible to the OS, it interacts with other memory man-
     agement hardware. Furthermore, many of the principles used in virtual memory
     schemes (discussed in Chapter 8) are also applied in cache memory.
     Motivation
     On all instruction cycles, the processor accesses memory at least once, to fetch
     the instruction, and often one or more additional times, to fetch operands and/
     or store results. The rate at which the processor can execute instructions is clearly
     limited by the memory cycle time (the time it takes to read one word from or write
     one word to memory). This limitation has been a significant problem because of
     the persistent mismatch between processor and main memory speeds: Over the
     years, processor speed has consistently increased more rapidly than memory access
     speed. We are faced with a trade-off among speed, cost, and size. Ideally, main
     memory should be built with the same technology as that of the processor registers,
     giving memory cycle times comparable to processor cycle times. This has always
     been too expensive a strategy. The solution is to exploit the principle of locality by
     providing a small, fast memory between the processor and main memory, namely
     the cache.

                                                      Block transfer
                             Word transfer
                        CPU                    Cache                         Main memory
                                   Fast                          Slow
                                               (a) Single cache
                   CPU             Level 1            Level 2          Level 3            Main
                                   (L1) cache        (L2) cache        (L3) cache         memory
                          Fastest              Fast              Less              Slow
                                                                 fast
                                         (b) Three-level cache organization
             Figure 1.16     Cache and Main Memory
    Cache Principles
    Cache memory is intended to provide memory access time approaching that of the
    fastest memories available and at the same time support a large memory size that has
    the price of less expensive types of semiconductor memories. The concept is illus-
    trated in Figure 1.16a. There is a relatively large and slow main memory together
    with a smaller, faster cache memory. The cache contains a copy of a portion of main
    memory. When the processor attempts to read a byte or word of memory, a check
    is made to determine if the byte or word is in the cache. If so, the byte or word is
    delivered to the processor. If not, a block of main memory, consisting of some fixed
    number of bytes, is read into the cache and then the byte or word is delivered to
    the processor. Because of the phenomenon of locality of reference, when a block of
    data is fetched into the cache to satisfy a single memory reference, it is likely that
    many of the near-future memory references will be to other bytes in the block.
    Figure 1.16b depicts the use of multiple levels of cache. The L2 cache is slower
    and typically larger than the L1 cache, and the L3 cache is slower and typically
    larger than the L2 cache.
    Figure 1.17 depicts the structure of a cache/main memory system. Main mem-
    ory consists of up to 2n addressable words, with each word having a unique n-bit
    address. For mapping purposes, this memory is considered to consist of a number of
    fixed-length blocks of K words each. That is, there are M  2n/K blocks. Cache con-
    sists of C slots (also referred to as lines) of K words each, and the number of slots is
    considerably less than the number of main memory blocks (CM).5 Some subset
    of the blocks of main memory resides in the slots of the cache. If a word in a block
    5The symbol  means much less than. Similarly, the symbol  means much greater than.

Line                                                    Memory
number       Tag    Block                               address
      0                                                 0
      1                                                 1
      2                                                 2                         Block
                                                        3                         (K words)
C1
                    Block length
                    (K words)
                    (a) Cache
                                                                                  Block
                                                        2n  1
                                                                 Word
                                                                 length
                                                                 (b) Main memory
Figure 1.17  Cache/Main-Memory Structure
      of memory that is not in the cache is read, that block is transferred to one of the
      slots of the cache. Because there are more blocks than slots, an individual slot can-
      not be uniquely and permanently dedicated to a particular block. Therefore, each
      slot includes a tag that identifies which particular block is currently being stored.
      The tag is usually some number of higher-order bits of the address and refers to all
      addresses that begin with that sequence of bits.
             As a simple example, suppose that we have a 6-bit address and a 2-bit tag. The
      tag 01 refers to the block of locations with the following addresses: 010000, 010001,
      010010, 010011, 010100, 010101, 010110, 010111, 011000, 011001, 011010, 011011,
      011100, 011101, 011110, 011111.
             Figure 1.18 illustrates the read operation. The processor generates the address,
      RA, of a word to be read. If the word is contained in the cache, it is delivered to the
      processor. Otherwise, the block containing that word is loaded into the cache and
      the word is delivered to the processor.
      Cache Design
      A detailed discussion of cache design is beyond the scope of this book. Key
      elements are briefly summarized here. We will see that similar design issues must be

                START
                                         RA--read address
                Receive address
                RA from CPU
                Is block         No                   Access main
                containing RA                         memory for block
                in cache?                             containing RA
                          Yes
                Fetch RA word                         Allocate cache
                and deliver                           slot for main
                to CPU                                memory block
                                     Load main                          Deliver RA word
                                     memory block                       to CPU
                                     into cache slot
                DONE
       Figure 1.18         Cache Read Operation
    addressed in dealing with virtual memory and disk cache design. They fall into the
    following categories:
    ·  Cache size
    ·  Block size
    ·  Mapping function
    ·  Replacement algorithm
    ·  Write policy
    ·  Number of cache levels
       We have already dealt with the issue of cache size. It turns out that reason-
    ably small caches can have a significant impact on performance. Another size issue
    is that of block size: the unit of data exchanged between cache and main memory.
    As the block size increases from very small to larger sizes, the hit ratio will at first
    increase because of the principle of locality: the high probability that data in the
    vicinity of a referenced word are likely to be referenced in the near future. As the

     block size increases, more useful data are brought into the cache. The hit ratio will
     begin to decrease, however, as the block becomes even bigger and the probability of
     using the newly fetched data becomes less than the probability of reusing the data
     that have to be moved out of the cache to make room for the new block.
     When a new block of data is read into the cache, the mapping function deter-
     mines which cache location the block will occupy. Two constraints affect the design
     of the mapping function. First, when one block is read in, another may have to be
     replaced. We would like to do this in such a way as to minimize the probability that
     we will replace a block that will be needed in the near future. The more flexible the
     mapping function, the more scope we have to design a replacement algorithm to
     maximize the hit ratio. Second, the more flexible the mapping function, the more
     complex is the circuitry required to search the cache to determine if a given block
     is in the cache.
     The replacement algorithm chooses, within the constraints of the mapping
     function, which block to replace when a new block is to be loaded into the cache and
     the cache already has all slots filled with other blocks. We would like to replace the
     block that is least likely to be needed again in the near future. Although it is impos-
     sible to identify such a block, a reasonably effective strategy is to replace the block
     that has been in the cache longest with no reference to it. This policy is referred to
     as the least-recently-used (LRU) algorithm. Hardware mechanisms are needed to
     identify the least-recently-used block.
     If the contents of a block in the cache are altered, then it is necessary to write it
     back to main memory before replacing it. The write policy dictates when the mem-
     ory write operation takes place. At one extreme, the writing can occur every time
     that the block is updated. At the other extreme, the writing occurs only when the
     block is replaced. The latter policy minimizes memory write operations but leaves
     main memory in an obsolete state. This can interfere with multiple-processor opera-
     tion and with direct memory access by I/O hardware modules.
     Finally, it is now commonplace to have multiple levels of cache, labeled L1
     (cache closest to the processor), L2, and in many cases a third level L3. A discus-
     sion of the performance benefits of multiple cache levels is beyond our scope; see
     [STAL10] for a discussion.
