Linux I/O
      Table 11.5  Device I/O in UNIX
                              Unbuffered I/O  Buffer Cache                   Character Queue
      Disk Drive                      X       X
      Tape Drive                      X       X
      Terminals                                                              X
      Communication Lines                                                    X
      Printers                        X                                      X
      performing unbuffered I/O is locked in main memory and cannot be swapped
      out. This reduces the opportunities for swapping by tying up part of main mem-
      ory, thus reducing the overall system performance. Also, the I/O device is tied up
      with the process for the duration of the transfer, making it unavailable for other
      processes.
      UNIX Devices
      Among the categories of devices recognized by UNIX are the following:
        Disk drives
        Tape drives
        Terminals
        Communication lines
        Printers
         Table 11.5 shows the types of I/O suited to each type of device. Disk drives
      are heavily used in UNIX, are block oriented, and have the potential for reason-
      able high throughput. Thus, I/O for these devices tends to be unbuffered or via
      buffer cache. Tape drives are functionally similar to disk drives and use similar I/O
      schemes.
         Because terminals involve relatively slow exchange of characters, terminal I/O
      typically makes use of the character queue. Similarly, communication lines require
      serial processing of bytes of data for input or output and are best handled by char-
      acter queues. Finally, the type of I/O used for a printer will generally depend on its
      speed. Slow printers will normally use the character queue, while a fast printer might
      employ unbuffered I/O. A buffer cache could be used for a fast printer. However,
      because data going to a printer are never reused, the overhead of the buffer cache is
      unnecessary.
11.9  LINUX I/O
      In general terms, the Linux I/O kernel facility is very similar to that of other UNIX
      implementation, such as SVR4. The Linux kernel associates a special file with each
      I/O device driver. Block, character, and network devices are recognized. In this sec-
      tion, we look at several features of the Linux I/O facility.

     Disk Scheduling
     The default disk scheduler in Linux 2.4 is known as the Linux Elevator, which is
     a variation on the LOOK algorithm discussed in Section 11.5. For Linux 2.6, the
     Elevator algorithm has been augmented by two additional algorithms: the deadline
     I/O scheduler and the anticipatory I/O scheduler [LOVE04]. We examine each of
     these in turn.
     THE  ELEVATOR   SCHEDULER  The elevator scheduler maintains a single queue
     for disk read and write requests and performs both sorting and merging functions
     on the queue. In general terms, the elevator scheduler keeps the list of requests
     sorted by block number. Thus, as the disk requests are handled, the drive moves in
     a single direction, satisfying each request as it is encountered. This general strategy
     is refined in the following manner. When a new request is added to the queue, four
     operations are considered in order:
     1.   If the request is to the same on-disk sector or an immediately adjacent sector
          to a pending request in the queue, then the existing request and the new re-
          quest are merged into one request.
     2.   If a request in the queue is sufficiently old, the new request is inserted at the
          tail of the queue.
     3.   If there is a suitable location, the new request is inserted in sorted order.
     4.   If there is no suitable location, the new request is placed at the tail of the
          queue.
     DEADLINE SCHEDULER       Operation 2 in the preceding list is intended to prevent
     starvation of a request, but is not very effective [LOVE04]. It does not attempt to
     service requests in a given time frame but merely stops insertion-sorting requests
     after a suitable delay. Two problems manifest themselves with the elevator scheme.
     The first problem is that a distant block request can be delayed for a substantial
     time because the queue is dynamically updated. For example, consider the following
     stream of requests for disk blocks: 20, 30, 700, 25. The elevator scheduler reorders
     these so that the requests are placed in the queue as 20, 25, 30, 700, with 20 being the
     head of the queue. If a continuous sequence of low-numbered block requests arrive,
     then the request for 700 continues to be delayed.
          An even more serious problem concerns the distinction between read and
     write requests. Typically, a write request is issued asynchronously. That is, once
     a process issues the write request, it need not wait for the request to actually be
     satisfied. When an application issues a write, the kernel copies the data into an
     appropriate buffer, to be written out as time permits. Once the data are captured
     in the kernel's buffer, the application can proceed. However, for many read oper-
     ations, the process must wait until the requested data are delivered to the appli-
     cation before proceeding. Thus, a stream of write requests (e.g., to place a large
     file on the disk) can block a read request for a considerable time and thus block a
     process.
          To   overcome  these  problems,     the  deadline  I/O  scheduler  makes             use
     of three queues (Figure 11.14). Each incoming request is placed in the sorted

Sorted (elevator) queue
Read FIFO queue
Write FIFO queue
Figure 11.14       The Linux Deadline I/O Scheduler
elevator queue, as before. In addition, the same request is placed at the tail of a
read FIFO queue for a read request or a write FIFO queue for a write request.
Thus, the read and write queues maintain a list of requests in the sequence in
which the requests were made. Associated with each request is an expiration
time, with a default value of 0.5 seconds for a read request and 5 seconds for a
write request. Ordinarily, the scheduler dispatches from the sorted queue. When
a request is satisfied, it is removed from the head of the sorted queue and also
from the appropriate FIFO queue. However, when the item at the head of one of
the FIFO queues becomes older than its expiration time, then the scheduler next
dispatches from that FIFO queue, taking the expired request, plus the next few
requests from the queue. As each request is dispatched, it is also removed from
the sorted queue.
The deadline I/O scheduler scheme overcomes the starvation problem and
also the read versus write problem.
ANTICIPATORY I/O SCHEDULER           The original elevator scheduler and the deadline
scheduler both are designed to dispatch a new request as soon as the existing request
is satisfied, thus keeping the disk as busy as possible. This same policy applies to all
of the scheduling algorithms discussed in Section 11.5. However, such a policy can
be counterproductive if there are numerous synchronous read requests. Typically,
an application will wait until a read request is satisfied and the data available before
issuing the next request. The small delay between receiving the data for the last
read and issuing the next read enables the scheduler to turn elsewhere for a pending
request and dispatch that request.
Because of the principle of locality, it is likely that successive reads from the
same process will be to disk blocks that are near one another. If the scheduler were
to delay a short period of time after satisfying a read request, to see if a new nearby
read request is made, the overall performance of the system could be enhanced.
This is the philosophy behind the anticipatory scheduler, proposed in [IYER01],
and implemented in Linux 2.6.

          In Linux, the anticipatory scheduler is superimposed on the deadline sched-
     uler. When a read request is dispatched, the anticipatory scheduler causes the
     scheduling system to delay for up to 6 ms, depending on the configuration. During
     this small delay, there is a good chance that the application that issued the last
     read request will issue another read request to the same region of the disk. If
     so, that request will be serviced immediately. If no such read request occurs, the
     scheduler resumes using the deadline scheduling algorithm.
          [LOVE04] reports on two tests of the Linux scheduling algorithms. The first
     test involved the reading of a 200-MB file while doing a long streaming write in
     the background. The second test involved doing a read of a large file in the back-
     ground while reading every file in the kernel source tree. The results are listed in
     the following table:
          I/O Scheduler and Kernel           Test 1                       Test 2
          Linux elevator on 2.4              45 seconds            30 minutes, 28 seconds
          Deadline I/O scheduler on 2.6      40 seconds                3 minutes, 30 seconds
          Anticipatory I/O scheduler on 2.6  4.6 seconds               15 seconds
          As can be seen, the performance improvement depends on the nature of
     the workload. But in both cases, the anticipatory scheduler provides a dramatic
     improvement.
     Linux Page Cache
     In Linux 2.2 and earlier releases, the kernel maintained a page cache for reads and
     writes from regular file system files and for virtual memory pages, and a separate
     buffer cache for block I/O. For Linux 2.4 and later, there is a single unified page
     cache that is involved in all traffic between disk and main memory.
          The page cache confers two benefits. First, when it is time to write back dirty
     pages to disk, a collection of them can be ordered properly and written out effi-
     ciently. Second, because of the principle of temporal locality, pages in the page
     cache are likely to be referenced again before they are flushed from the cache, thus
     saving a disk I/O operation.
          Dirty pages are written back to disk in two situations:
         When free memory falls below a specified threshold, the kernel reduces the
          size of the page cache to release memory to be added to the free memory pool.
         When dirty pages grow older than a specified threshold, a number of dirty
          pages are written back to disk.
11.10  WINDOWS I/O
     Figure 11.15 shows the key kernel-mode components related to the Windows I/O
     manager. The I/O manager is responsible for all I/O for the operating system and
     provides a uniform interface that all types of drivers can call.

                                     I/O manager
                                     Cache
                                     manager
                                     File system
                                     drivers
                                     Network
                                     drivers
                                     Hardware
                                     device drivers
                      Figure  11.15  Windows I/O     Manager
Basic I/O Facilities
The I/O manager works closely with four types of kernel components:
  Cache manager: The cache manager handles file caching for all file systems.
   It can dynamically increase and decrease the size of the cache devoted to a
   particular file as the amount of available physical memory varies. The system
   records updates in the cache only and not on disk. A kernel thread, the lazy
   writer, periodically batches the updates together to write to disk. Writing the
   updates in batches allows the I/O to be more efficient. The cache manager
   works by mapping regions of files into kernel virtual memory and then relying
   on the virtual memory manager to do most of the work to copy pages to and
   from the files on disk.
  File system drivers: The I/O manager treats a file system driver as just another
   device driver and routes I/O requests for file system volumes to the appropri-
   ate software driver for that volume. The file system, in turn, sends I/O requests
   to the software drivers that manage the hardware device adapter.
  Network drivers: Windows includes integrated networking capabilities and
   support for remote file systems. The facilities are implemented as software
   drivers rather than part of the Windows Executive.
  Hardware device drivers: These software drivers access the hardware regis-
   ters of the peripheral devices using entry points in the Hardware Abstraction
   Layer. A set of these routines exists for every platform that Windows supports;
   because the routine names are the same for all platforms, the source code of
   Windows device drivers is portable across different processor types.
Asynchronous and Synchronous I/O
Windows offers two modes of I/O operation: asynchronous and synchronous. The
asynchronous mode is used whenever possible to optimize application perfor-
mance. With asynchronous I/O, an application initiates an I/O operation and then
can continue processing while the I/O request is fulfilled. With synchronous I/O, the
application is blocked until the I/O operation completes.
   Asynchronous I/O is more efficient, from the point of view of the calling
thread, because it allows the thread to continue execution while the I/O operation is

     queued by the I/O manager and subsequently performed. However, the application
     that invoked the asynchronous I/O operation needs some way to determine when
     the operation is complete. Windows provides five different techniques for signaling
     I/O completion:
       Signaling the file object: With this approach, the event associated with a
        file object is set when an operation on that object is complete. The thread
        that invoked the I/O operation can continue to execute until it reaches a
        point where it must stop until the I/O operation is complete. At that point,
        the thread can wait until the operation is complete and then continue. This
        technique is simple and easy to use but is not appropriate for handling
        multiple I/O requests. For example, if a thread needs to perform multiple
        simultaneous actions on a single file, such as reading from one portion and
        writing to another portion of the file, with this technique the thread could
        not distinguish between the completion of the read and the completion of
        the write. It would simply know that one of the requested I/O operations on
        this file had finished.
       Signaling an event object: This technique allows multiple simultaneous I/O
        requests against a single device or file. The thread creates an event for each
        request. Later, the thread can wait on a single one of these requests or on an
        entire collection of requests.
       Asynchronous procedure call: This technique makes use of a queue associated
        with a thread, known as the asynchronous procedure call (APC) queue. In this
        case, the thread makes I/O requests, specifying a user-mode routine to call
        when the I/O completes. The I/O manager places the results of each request in
        the calling thread's APC queue. The next time the thread blocks in the kernel,
        the APCs will be delivered, each causing the thread to return to user mode
        and execute the specified routine.
       I/O completion ports: This technique is used on a Windows server to optimize
        the use of threads. The application creates a pool of threads for handling the
        completion of I/O requests. Each thread waits on the completion port, and the
        Kernel wakes threads to handle each I/O completion. One of the advantages
        of this approach is that the application can specify a limit for how many of
        these threads will run at the same time.
       Polling: Asynchronous I/O requests write a status and transfer count into the
        process' user virtual memory when the operation completes. A thread can just
        check these values to see if the operation has completed.
     Software RAID
     Windows supports two sorts of RAID configurations, defined in [MS96] as follows:
       Hardware RAID: Separate physical disks combined into one or more logical
        disks by the disk controller or disk storage cabinet hardware.
       Software RAID: Noncontiguous disk space combined into one or more logical
        partitions by the fault-tolerant software disk driver, FTDISK.

       In hardware RAID, the controller interface handles the creation and regener-
ation of redundant information. The software RAID, available on Windows Server,
implements the RAID functionality as part of the operating system and can be used
with any set of multiple disks. The software RAID facility implements RAID 1
and RAID 5. In the case of RAID 1 (disk mirroring), the two disks containing the
primary and mirrored partitions may be on the same disk controller or different
disk controllers. The latter configuration is referred to as disk duplexing.
Volume Shadow Copies
Shadow copies are an efficient way of making consistent snapshots of volumes
so that they can be backed up. They are also useful for archiving files on a per-
volume basis. If a user deletes a file he or she can retrieve an earlier copy from
any available shadow copy made by the system administrator. Shadow copies are
implemented by a software driver that makes copies of data on the volume before
it is overwritten.
Volume Encryption
Windows supports the encryption of entire volumes, using a feature called
BitLocker. This is more secure than encrypting individual files, as the entire system
works to be sure that the data is safe. Up to three different methods of supplying the
cryptographic key can be provided, allowing multiple interlocking layers of security.
11.11  SUMMARY
The computer system's interface to the outside world is its I/O architecture. This
architecture is designed to provide a systematic means of controlling interaction
with the outside world and to provide the operating system with the information it
needs to manage I/O activity effectively.
       The I/O function is generally broken up into a number of layers, with lower
layers dealing with details that are closer to the physical functions to be performed
and higher layers dealing with I/O in a logical and generic fashion. The result is that
changes in hardware parameters need not affect most of the I/O software.
       A key aspect of I/O is the use of buffers that are controlled by I/O utili-
ties rather than by application processes. Buffering smoothes out the differences
between the internal speeds of the computer system and the speeds of I/O devices.
The use of buffers also decouples the actual I/O transfer from the address space
of the application process. This allows the operating system more flexibility in
performing its memory-management function.
       The aspect of I/O that has the greatest impact on overall system performance
is disk I/O. Accordingly, there has been greater research and design effort in this
area than in any other kind of I/O. Two of the most widely used approaches to
improve disk I/O performance are disk scheduling and the disk cache.
       At any time, there may be a queue of requests for I/O on the same disk. It is
the object of disk scheduling to satisfy these requests in a way that minimizes the

     mechanical seek time of the disk and hence improves performance. The physical
     layout of pending requests plus considerations of locality come into play.
       A disk cache is a buffer, usually kept in main memory, that functions as a
     cache of disk blocks between disk memory and the rest of main memory. Because
     of the principle of locality, the use of a disk cache should substantially reduce the
     number of block I/O transfers between main memory and disk.
11.12  RECOMMENDED READING
     General discussions of computer I/O can be found in most books on computer archi-
     tecture, such as [STAL10]. [MEE96a] provides a good survey of the underlying
     recording technology of disk and tape systems. [MEE96b] focuses on the data stor-
     age techniques for disk and tape systems. [WIED87] contains an excellent discussion
     of disk performance issues, including those relating to disk scheduling. [NG98] looks
     at disk hardware performance issues. [CAO96] analyzes disk caching and disk sched-
     uling. Good surveys of disk scheduling algorithms, with a performance analysis, are
     [WORT94] and [SELT90].
       [PAI00] is an instructive description of an integrated operating-system scheme
     for I/O buffering and caching.
       [DELL00] provides a detailed discussion of Windows NT device drivers plus a
     good overview of the entire Windows I/O architecture.
       An excellent survey of RAID technology, written by the inventors of the
     RAID concept, is [CHEN94]. [CHEN96] analyzes RAID performance. Another
     good paper is [FRIE96]. [DALT96] describes the Windows NT software RAID
     facility in detail. [LEVE10] examines the need to move beyond RAID 6 to a triple-
     parity configuration. [STAI10] is a good survey of the standard RAID levels plus a
     number of common RAID enhancements.
       CAO96   Cao, P., Felten, E., Karlin, A., and Li, K. "Implementation and Performance
       of Integrated Application-Controlled File Caching, Prefetching, and Disk
       Scheduling." ACM Transactions on Computer Systems, November 1996.
       CHEN94  Chen, P., Lee, E., Gibson, G., Katz, R., and Patterson, D. "RAID: High-
       Performance, Reliable Secondary Storage." ACM Computing Surveys, June 1994.
       CHEN96  Chen,  S.,  and  Towsley,  D.  "A  Performance  Evaluation        of  RAID
       Architectures." IEEE Transactions on Computers, October 1996.
       DALT96  Dalton, W., et al. Windows NT Server 4: Security, Troubleshooting, and
       Optimization. Indianapolis, IN: New Riders Publishing, 1996.
       DELL00  Dekker, E., and Newcomer, J. Developing Windows NT Device Drivers: A
       Programmer's Handbook. Reading, MA: Addison Wesley, 2000.
       FRIE96  Friedman, M. "RAID Keeps Going and Going and..." IEEE Spectrum,
       April 1996.
       LEVE10  Leventhal, A. "Triple-Parity RAID and Beyond." Communications of the
       ACM, January 2010.
       MEE96a  Mee, C., and Daniel, E. eds. Magnetic Recording Technology. New York:
       McGraw Hill, 1996.

           MEE96b      Mee, C., and Daniel, E. eds. Magnetic Storage Handbook. New York:
                  McGraw Hill, 1996.
           NG98    Ng, S. "Advances in Disk Technology: Performance Issues." Computer, May 1989.
           PAI00   Pai, V., Druschel, P., and Zwaenepoel, W. "IO-Lite: A Unified I/O Buffering
                  and Caching System." ACM Transactions on Computer Systems, February 2000.
           SELT90     Seltzer, M., Chen, P., and Ousterhout, J. "Disk Scheduling Revisited."
                  Proceedings, USENIX Winter Technical Conference, January 1990.
           STAI10     Staimer, M. "Alternatives to RAID." Storage Magazine, May 2010.
           STAL10     Stallings, W. Computer Organization and Architecture, 8th ed. Upper Saddle
                  River, NJ: Prentice Hall, 2010.
           WIED87      Wiederhold,        G.  File  Organization  for  Database   Design.  New  York:
                  McGraw-Hill, 1987.
           WORT94      Worthington, B., Ganger, G., and Patt, Y. "Scheduling Algorithms for
                  Modern Disk Drives." ACM SiGMETRICS, May 1994.
11.13      KEY TERMS,  REVIEW QUESTIONS,                               AND PROBLEMS
Key Terms
block                    input/output (I/O)                            redundant array of
block-oriented device    I/O buffer                                    independent disks
circular buffer          I/O channel                                   removable disk
device I/O               I/O processor                                 rotational delay
direct memory access     logical I/O                                   sector
disk access time         magnetic disk                                 seek time
disk cache               nonremovable disk                             stream-oriented device
gap                      programmed I/O                                track
hard disk                read/write head                               transfer time
interrupt-driven I/O
       Review Questions
       11.1       List and briefly define three techniques for performing I/O.
       11.2       What is the difference between logical I/O and device I/O?
       11.3       What is the difference between block-oriented devices and stream-oriented devices?
                  Give a few examples of each.
       11.4       Why would you expect improved performance using a double buffer rather than a
                  single buffer for I/O?
       11.5       What delay elements are involved in a disk read or write?
       11.6       Briefly define the disk scheduling policies illustrated in Figure 11.7.
       11.7       Briefly define the seven RAID levels.
       11.8       What is the typical disk sector size?

     Problems
     11.1   Consider a program that accesses a single I/O device and compare unbuffered I/O to
            the use of a buffer. Show that the use of the buffer can reduce the running time by at
            most a factor of two.
     11.2   Generalize the result of Problem 11.1 to the case in which a program refers to n devices.
     11.3   a.  Perform the same type of analysis as that of Table 11.2 for the following sequence
                of disk track requests: 27, 129, 110, 186, 147, 41, 10, 64, 120. Assume that the
                disk head is initially positioned over track 100 and is moving in the direction of
                decreasing track number.
            b.  Do the same analysis, but now assume that the disk head is moving in the direction
                of increasing track number.
     11.4   Consider a disk with N tracks numbered from 0 to (N - 1) and assume that requested
            sectors are distributed randomly and evenly over the disk. We want to calculate the
            average number of tracks traversed by a seek.
            a.  Calculate the probability of a seek of length j when the head is currently posi-
                tioned over track t. (Hint: This is a matter of determining the total number of
                combinations, recognizing that all track positions for the destination of the seek
                are equally likely.)
            b.  Calculate the probability of a seek of length K, for an arbitrary current position
                of the head. (Hint: This involves the summing over all possible combinations of
                movements of K tracks.)
            c.  Calculate the average number of tracks traversed by a seek, using the formula for
                expected value
                                                       N-1
                                               E[x] =  ai *    Pr [x = i]
                                                       i=0
                                          n       n(n + 1)     n           n(n + 1)(2n + 1)
                Hint: Use the equalities a     =       2    ;  a i2  =     6                 .
                                          i=1                  i=1
            d.  Show that for large values of N, the average number of tracks traversed by a seek
                approaches N/3.
     11.5   The following equation was suggested both for cache memory and disk cache memory:
                                               TS = TC + M * TD
            Generalize this equation to a memory hierarchy with N levels instead of just 2.
     11.6   For the frequency-based replacement algorithm (Figure 11.9), define Fnew, Fmiddle,
            and Fold as the fraction of the cache that comprises the new, middle, and old sections,
            respectively. Clearly, Fnew + Fmiddle + Fold = 1. Characterize the policy when
            a.  Fold  =  1  -  Fnew
            b.  Fold = 1/(cache size)
     11.7   Calculate how much disk space (in sectors, tracks, and surfaces) will be required to
            store 300,000 120-byte logical records if the disk is fixed sector with 512 bytes/
            sector, with 96 sectors/track, 110 tracks per surface, and 8 usable surfaces. Ignore
            any file header record(s) and track indexes, and assume that records cannot span
            two sectors.
     11.8   Consider the disk system described in Problem 11.7, and assume that the disk rotates
            at 360 rpm. A processor reads one sector from the disk using interrupt-driven I/O,
            with one interrupt per byte. If it takes 2.5 s to process each interrupt, what percent-
            age of the time will the processor spend handling I/O (disregard seek time)?
     11.9   Repeat the preceding problem using DMA, and assume one interrupt per sector.
     11.10  A 32-bit computer has two selector channels and one multiplexor channel. Each selec-
            tor channel supports two magnetic disk and two magnetic tape units. The multiplexor

       channel has two line printers, two card readers, and ten VDT terminals connected to
       it. Assume the following transfer rates:
       Disk drive                                800 Kbytes/s
       Magnetic tape drive                       200 Kbytes/s
       Line printer                              6.6 Kbytes/s
       Card reader                               1.2 Kbytes/s
       VDT                                       1 Kbyte/s
       Estimate the maximum aggregate I/O transfer rate in this system.
11.11  It should be clear that disk striping can improve the data transfer rate when the strip
       size is small compared to the I/O request size. It should also be clear that RAID 0
       provides improved performance relative to a single large disk, because multiple I/O
       requests can be handled in parallel. However, in this latter case, is disk striping neces-
       sary? That is, does disk striping improve I/O request rate performance compared to a
       comparable disk array without striping?
11.12  Consider a 4-drive, 200 GB-per-drive RAID array. What is the available data storage
       capacity for each of the RAID levels, 0, 1, 3, 4, 5, and 6?

FILE MANAGEMENT
     12.1   Overview
            Files and File Systems
            File Structure
            File Management Systems
     12.2   File Organization and Access
            The Pile
            The Sequential File
            The Indexed Sequential File
            The Indexed File
            The Direct or Hashed File
     12.3   B-Trees
     12.4   File Directories
            Contents
            Structure
            Naming
     12.5   File Sharing
            Access Rights
            Simultaneous Access
     12.6   Record Blocking
     12.7   Secondary Storage Management
            File Allocation
            Free Space Management
            Volumes
            Reliability
     12.8   File System Security
     12.9   UNIX File Management
     12.10  Linux Virtual File System
     12.11  Windows File System
     12.12  Summary
     12.13  Recommended Reading
     12.14  Key Terms, Review Questions, and Problems
520
