Clusters
16.5  CLUSTERS
      Clustering is an alternative to symmetric multiprocessing (SMP) as an approach to
      providing high performance and high availability and is particularly attractive for
      server applications. We can define a cluster as a group of interconnected, whole
      computers working together as a unified computing resource that can create the
      illusion of being one machine. The term whole computer means a system that can
      run on its own, apart from the cluster; in the literature, each computer in a cluster is
      typically referred to as a node.
         [BREW97] lists four benefits that can be achieved with clustering. These can
      also be thought of as objectives or design requirements:
        Absolute scalability: It is possible to create large clusters that far surpass the
         power of even the largest stand-alone machines. A cluster can have dozens or
         even hundreds of machines, each of which is a multiprocessor.
        Incremental scalability: A cluster is configured in such a way that it is possible
         to add new systems to the cluster in small increments. Thus, a user can start
         out with a modest system and expand it as needs grow, without having to go
         through a major upgrade in which an existing small system is replaced with a
         larger system.
        High availability: Because each node in a cluster is a stand-alone computer,
         the failure of one node does not mean loss of service. In many products, fault
         tolerance is handled automatically in software.
        Superior price/performance: By using commodity building blocks, it is possible
         to put together a cluster with equal or greater computing power than a single
         large machine, at much lower cost.
      Cluster Configurations
      In the literature, clusters are classified in a number of different ways. Perhaps the sim-
      plest classification is based on whether the computers in a cluster share access to the
      same disks. Figure 16.15a shows a two-node cluster in which the only interconnection
      is by means of a high-speed link that can be used for message exchange to coordinate
      cluster activity. The link can be a LAN that is shared with other computers that are
      not part of the cluster, or the link can be a dedicated interconnection facility. In the
      latter case, one or more of the computers in the cluster will have a link to a LAN
      or WAN so that there is a connection between the server cluster and remote client
      systems. Note that in the figure, each computer is depicted as being a multiprocessor.
      This is not necessary but does enhance both performance and availability.
         In the simple classification depicted in Figure 16.15, the other alternative is
      a shared-disk cluster. In this case, there generally is still a message link between
      nodes. In addition, there is a disk subsystem that is directly linked to multiple com-
      puters within the cluster. In Figure 16.15b, the common disk subsystem is a RAID
      system. The use of RAID or some similar redundant disk technology is common in
      clusters so that the high availability achieved by the presence of multiple computers
      is not compromised by a shared disk that is a single point of failure.

              P           P                                               P         P
              M      I/O     I/O  High-speed message link                 I/O  I/O        M
                                  (a) Standby server with no shared disk
                                  High-speed message link
              P      P    I/O                                             I/O  P       P
              M      I/O     I/O                                          I/O  I/O        M
                                                     RAID
                                  (b) Shared disk
     Figure 16.15            Cluster Configurations
     A clearer picture of the range of clustering approaches can be gained by look-
     ing at functional alternatives. A white paper from Hewlett Packard [HP96] provides
     a useful classification along functional lines (Table 16.2), which we now discuss.
     A common, older method, known as passive standby, is simply to have one
     computer handle all of the processing load while the other computer remains inac-
     tive, standing by to take over in the event of a failure of the primary. To coordi-
     nate the machines, the active, or primary, system periodically sends a "heartbeat"
     message to the standby machine. Should these messages stop arriving, the standby
     assumes that the primary server has failed and puts itself into operation. This
     approach increases availability but does not improve performance. Further, if the
     only information that is exchanged between the two systems is a heartbeat message,
     and if the two systems do not share common disks, then the standby provides a
     functional backup but has no access to the databases managed by the primary.
     The passive standby is generally not referred to as a cluster. The term cluster
     is reserved for multiple interconnected computers that are all actively doing
     processing while maintaining the image of a single system to the outside world.
     The term active secondary is often used in referring to this configuration. Three
     classifications of clustering can be identified: separate servers, shared nothing, and
     shared memory.

Table 16.2  Clustering  Methods: Benefits and Limitations
Clustering Method        Description                  Benefits                Limitations
Passive Standby          A secondary server           Easy to implement       High cost because the
                         takes over in case of pri-                           secondary server is
                         mary server failure.                                 unavailable for other
                                                                              processing tasks
Active Secondary         The secondary server is      Reduced cost because    Increased complexity
                         also used for processing     secondary servers can
                         tasks.                       be used for processing
Separate Servers         Separate servers have        High availability       High network and server
                         their own disks. Data                                overhead due to copying
                         are continuously copied                              operations
                         from primary to
                         secondary server.
Servers Connected to     Servers are cabled to        Reduced network and     Usually requires disk
Disks                    the same disks, but each     server overhead due to  mirroring or RAID tech-
                         server owns its disks. If    elimination of copying  nology to compensate
                         one server fails, its disks  operations              for risk of disk failure
                         are taken over by the
                         other server.
Servers Share Disks      Multiple servers simul-      Low network and server  Requires lock manager
                         taneously share access       overhead. Reduced risk  software. Usually used
                         to disks.                    of downtime caused by   with disk mirroring or
                                                      disk failure            RAID technology
                 In one approach to clustering, each computer is a separate server with its own
       disks and there are no disks shared between systems (Figure 16.15a). This arrange-
       ment provides high performance as well as high availability. In this case, some type
       of management or scheduling software is needed to assign incoming client requests
       to servers so that the load is balanced and high utilization is achieved. It is desirable
       to have a failover capability, which means that if a computer fails while executing an
       application, another computer in the cluster can pick up and complete the applica-
       tion. For this to happen, data must constantly be copied among systems so that each
       system has access to the current data of the other systems. The overhead of this data
       exchange ensures high availability at the cost of a performance penalty.
                 To reduce the communications overhead, most clusters now consist of serv-
       ers connected to common disks (Figure 16.15b). In one variation of this approach,
       called shared nothing, the common disks are partitioned into volumes, and each
       volume is owned by a single computer. If that computer fails, the cluster must be
       reconfigured so that some other computer has ownership of the volumes of the
       failed computer.
                 It is also possible to have multiple computers share the same disks at the same
       time (called the shared disk approach), so that each computer has access to all of the
       volumes on all of the disks. This approach requires the use of some type of locking
       facility to ensure that data can only be accessed by one computer at a time.

     Operating System Design Issues
     Full exploitation of a cluster hardware configuration requires some enhancements
     to a single-system operating system.
     FAILURE MANAGEMENT  How failures are managed by a cluster depends on the
     clustering method used (Table 16.2). In general, two approaches can be taken to
     dealing with failures: highly available clusters and fault-tolerant clusters. A highly
     available cluster offers a high probability that all resources will be in service. If a
     failure occurs, such as a node goes down or a disk volume is lost, then the queries in
     progress are lost. Any lost query, if retried, will be serviced by a different computer
     in the cluster. However, the cluster operating system makes no guarantee about
     the state of partially executed transactions. This would need to be handled at the
     application level.
        A fault-tolerant cluster ensures that all resources are always available. This
     is achieved by the use of redundant shared disks and mechanisms for backing out
     uncommitted transactions and committing completed transactions.
        The function of switching an application and data resources over from a failed
     system to an alternative system in the cluster is referred to as failover. A related
     function is the restoration of applications and data resources to the original system
     once it has been fixed; this is referred to as failback. Failback can be automated, but
     this is desirable only if the problem is truly fixed and unlikely to recur. If not, auto-
     matic failback can cause subsequently failed resources to bounce back and forth
     between computers, resulting in performance and recovery problems.
     LOAD  BALANCING     A cluster requires an effective capability for balancing the
     load among available computers. This includes the requirement that the cluster
     be incrementally scalable. When a new computer is added to the cluster, the
     load-balancing facility should automatically include this computer in scheduling
     applications. Middleware mechanisms need to recognize that services can appear
     on different members of the cluster and may migrate from one member to another.
     PARALLELIZING COMPUTATION  In some cases, effective use of a cluster requires
     executing software from a single application in parallel. [KAPP00] lists three
     general approaches to the problem:
       Parallelizing compiler: A parallelizing compiler determines, at compile time,
        which parts of an application can be executed in parallel. These are then split
        off to be assigned to different computers in the cluster. Performance depends
        on the nature of the problem and how well the compiler is designed.
       Parallelized application: In this approach, the programmer writes the appli-
        cation from the outset to run on a cluster and uses message passing to move
        data, as required, between cluster nodes. This places a high burden on the
        programmer but may be the best approach for exploiting clusters for some
        applications.
       Parametric computing: This approach can be used if the essence of the applica-
        tion is an algorithm or program that must be executed a large number of times,

                      each time with a different set of starting conditions or parameters. A good
                      example is a simulation model, which will run a large number of different sce-
                      narios and then develop statistical summaries of the results. For this approach
                      to be effective, parametric processing tools are needed to organize, run, and
                      manage the jobs in an orderly manner.
Cluster Computer Architecture
Figure 16.16 shows a typical cluster architecture. The individual computers are con-
nected by some high-speed LAN or switch hardware. Each computer is capable of
operating independently. In addition, a middleware layer of software is installed in
each computer to enable cluster operation. The cluster middleware provides a uni-
fied system image to the user, known as a single-system image. The middleware may
also be responsible for providing high availability, by means of load balancing and
responding to failures in individual components. [HWAN99] lists the following as
desirable cluster middleware services and functions:
                     Single entry point: A user logs on to the cluster rather than to an individual
                      computer.
                     Single file hierarchy: The user sees a single hierarchy of file directories under
                      the same root directory.
                     Single control point: There is a default node used for cluster management and
                      control.
                     Single virtual networking: Any node can access any other point in the cluster,
                      even though the actual cluster configuration may consist of multiple intercon-
                      nected networks. There is a single virtual network operation.
                     Single memory space: Distributed shared memory enables programs to share
                      variables.
                                                                               Parallel applications
                      Sequential applications                                  Parallel programming environment
                                                       Cluster middleware
                                  (Single      system  image and availability  infrastructure)
PC/workstation        PC/workstation                   PC/workstation          PC/workstation                    PC/workstation
Comm SW                         Comm SW                Comm SW                 Comm SW                           Comm SW
Net. interface HW     Net. interface HW                Net. interface HW       Net. interface HW      Net. interface HW
                                               High-speed-network/switch
Figure 16.16          Cluster Computer Architecture

       Single job-management system: Under a cluster job scheduler, a user can
        submit a job without specifying the host computer to execute the job.
       Single user interface: A common graphic interface supports all users, regard-
        less of the workstation from which they enter the cluster.
       Single I/O space: Any node can remotely access any I/O peripheral or disk
        device without knowledge of its physical location.
       Single process space: A uniform process-identification scheme is used. A
        process on any node can create or communicate with any other process on a
        remote node.
       Checkpointing: This function periodically saves the process state and interme-
        diate computing results, to allow rollback recovery after a failure.
       Process migration: This function enables load balancing.
        The last four items on the preceding list enhance the availability of the cluster.
     The remaining items are concerned with providing a single system image.
        Returning to Figure 16.16, a cluster will also include software tools for ena-
     bling the efficient execution of programs that are capable of parallel execution.
     Clusters Compared to SMP
     Both clusters and symmetric multiprocessors provide a configuration with multiple
     processors to support high-demand applications. Both solutions are commercially
     available, although SMP has been around far longer.
        The main strength of the SMP approach is that an SMP is easier to manage
     and configure than a cluster. The SMP is much closer to the original single-processor
     model for which nearly all applications are written. The principal change required
     in going from a uniprocessor to an SMP is to the scheduler function. Another ben-
     efit of the SMP is that it usually takes up less physical space and draws less power
     than a comparable cluster. A final important benefit is that the SMP products are
     well established and stable.
        Over the long run, however, the advantages of the cluster approach are likely
     to result in clusters dominating the high-performance server market. Clusters are
     far superior to SMPs in terms of incremental and absolute scalability. Clusters are
     also superior in terms of availability, because all components of the system can
     readily be made highly redundant.
16.6 WINDOWS CLUSTER SERVER
     Windows Failover Clustering is a shared-nothing cluster, in which each disk volume
     and other resources are owned by a single system at a time.
        The Windows cluster design makes use of the following concepts:
       Cluster Service: The collection of software on each node that manages all
        cluster-specific activity.
       Resource: An item managed by the cluster service. All resources are objects
        representing actual resources in the system, including hardware devices such
