The Memory Hierarchy
1.5  THE MEMORY HIERARCHY
     The design constraints on a computer's memory can be summed up by three ques-
     tions: How much? How fast? How expensive?
         The question of how much is somewhat open ended. If the capacity is there,
     applications will likely be developed to use it. The question of how fast is, in a sense,
     easier to answer. To achieve greatest performance, the memory must be able to
     keep up with the processor. That is, as the processor is executing instructions, we
     would not want it to have to pause waiting for instructions or operands. The final
     question must also be considered. For a practical system, the cost of memory must
     be reasonable in relationship to other components.
         As might be expected, there is a trade-off among the three key characteristics
     of memory: namely, capacity, access time, and cost. A variety of technologies are
     used to implement memory systems, and across this spectrum of technologies, the
     following relationships hold:
        Faster access time, greater cost per bit
        Greater capacity, smaller cost per bit
        Greater capacity, slower access speed
         The dilemma facing the designer is clear. The designer would like to use
     memory technologies that provide for large-capacity memory, both because the
     capacity is needed and because the cost per bit is low. However, to meet perform-
     ance requirements, the designer needs to use expensive, relatively lower-capacity
     memories with fast access times.
         The way out of this dilemma is to not rely on a single memory component or
     technology, but to employ a memory hierarchy. A typical hierarchy is illustrated in
     Figure 1.14. As one goes down the hierarchy, the following occur:
     a.  Decreasing cost per bit
     b.  Increasing capacity
     c.  Increasing access time
     d.  Decreasing frequency of access to the memory by the processor
         Thus, smaller, more expensive, faster memories are supplemented by larger,
     cheaper, slower memories. The key to the success of this organization is the decreas-
     ing frequency of access at lower levels. We will examine this concept in greater
     detail later in this chapter, when we discuss the cache, and when we discuss virtual
     memory later in this book. A brief explanation is provided at this point.
         Suppose that the processor has access to two levels of memory. Level 1 con-
     tains 1,000 bytes and has an access time of 0.1 s; level 2 contains 100,000 bytes and
     has an access time of 1 s. Assume that if a byte to be accessed is in level 1, then
     the processor accesses it directly. If it is in level 2, then the byte is first transferred
     to level 1 and then accessed by the processor. For simplicity, we ignore the time
     required for the processor to determine whether the byte is in level 1 or level 2.

                                                        Riesgte-rs
                                         meInmboorayrd  Cache
                                                        Mmeaminory
             sOtourtabgoeard                            MagCnDeCtD-iDDRcVVO-dDRBDiMsW-lk-RuR-WRAaMy
             stOorfaf-gleine                                        Magnetic tape
Figure 1.14  The Memory Hierarchy
Figure 1.15 shows the general shape of the curve that models this situation. The
figure shows the average access time to a two-level memory as a function of the hit
ratio H, where H is defined as the fraction of all memory accesses that are found
in the faster memory (e.g., the cache), T1 is the access time to level 1, and T2 is the
access time to level 2.4 As can be seen, for high percentages of level 1 access, the
average total access time is much closer to that of level 1 than that of level 2.
In our example, suppose 95% of the memory accesses are found in the cache
(H  0.95). Then the average time to access a byte can be expressed as
(0.95) (0.1 s)  (0.05) (0.1 s  1 s)  0.095  0.055  0.15 s
The result is close to the access time of the faster memory. So the strategy
of using two memory levels works in principle, but only if conditions (a) through
(d) in the preceding list apply. By employing a variety of technologies, a spectrum of
4If the accessed word is found in the faster memory, that is defined as a hit. A miss occurs if the accessed
word is not found in the faster memory.

                         T1  T2
                         T2
    Average access time
                         T1
                                 0                                                        1
                                 Fraction of accesses involving only level 1 (Hit ratio)
    Figure 1.15                     Performance of a Simple Two-Level
                                    Memory
    memory systems exists that satisfies conditions (a) through (c). Fortunately, condi-
    tion (d) is also generally valid.
    The basis for the validity of condition (d) is a principle known as locality of
    reference [DENN68]. During the course of execution of a program, memory refer-
    ences by the processor, for both instructions and data, tend to cluster. Programs
    typically contain a number of iterative loops and subroutines. Once a loop or subrou-
    tine is entered, there are repeated references to a small set of instructions. Similarly,
    operations on tables and arrays involve access to a clustered set of data bytes. Over
    a long period of time, the clusters in use change, but over a short period of time, the
    processor is primarily working with fixed clusters of memory references.
    Accordingly, it is possible to organize data across the hierarchy such that the
    percentage of accesses to each successively lower level is substantially less than that of
    the level above. Consider the two-level example already presented. Let level 2 mem-
    ory contain all program instructions and data. The current clusters can be temporarily
    placed in level 1. From time to time, one of the clusters in level 1 will have to be
    swapped back to level 2 to make room for a new cluster coming in to level 1. On aver-
    age, however, most references will be to instructions and data contained in level 1.
    This principle can be applied across more than two levels of memory. The
    fastest, smallest, and most expensive type of memory consists of the registers inter-
    nal to the processor. Typically, a processor will contain a few dozen such registers,
    although some processors contain hundreds of registers. Skipping down two levels,
    main memory is the principal internal memory system of the computer. Each loca-
    tion in main memory has a unique address, and most machine instructions refer
    to one or more main memory addresses. Main memory is usually extended with a
    higher-speed, smaller cache. The cache is not usually visible to the programmer or,
    indeed, to the processor. It is a device for staging the movement of data between
    main memory and processor registers to improve performance.

        The three forms of memory just described are, typically, volatile and employ
     semiconductor technology. The use of three levels exploits the fact that semicon-
     ductor memory comes in a variety of types, which differ in speed and cost. Data are
     stored more permanently on external mass storage devices, of which the most com-
     mon are hard disk and removable media, such as removable disk, tape, and optical
     storage. External, nonvolatile memory is also referred to as secondary memory or
     auxiliary memory. These are used to store program and data files, and are usually
     visible to the programmer only in terms of files and records, as opposed to individ-
     ual bytes or words. A hard disk is also used to provide an extension to main memory
     known as virtual memory, which is discussed in Chapter 8.
        Additional levels can be effectively added to the hierarchy in software. For
     example, a portion of main memory can be used as a buffer to temporarily hold data
     that are to be read out to disk. Such a technique, sometimes referred to as a disk
     cache (examined in detail in Chapter 11), improves performance in two ways:
       Disk writes are clustered. Instead of many small transfers of data, we have a
        few large transfers of data. This improves disk performance and minimizes
        processor involvement.
       Some data destined for write-out may be referenced by a program before the
        next dump to disk. In that case, the data are retrieved rapidly from the soft-
        ware cache rather than slowly from the disk.
        Appendix 1A examines the performance implications of multilevel memory
     structures.
1.6  CACHE MEMORY
     Although cache memory is invisible to the OS, it interacts with other memory man-
     agement hardware. Furthermore, many of the principles used in virtual memory
     schemes (discussed in Chapter 8) are also applied in cache memory.
     Motivation
     On all instruction cycles, the processor accesses memory at least once, to fetch
     the instruction, and often one or more additional times, to fetch operands and/
     or store results. The rate at which the processor can execute instructions is clearly
     limited by the memory cycle time (the time it takes to read one word from or write
     one word to memory). This limitation has been a significant problem because of
     the persistent mismatch between processor and main memory speeds: Over the
     years, processor speed has consistently increased more rapidly than memory access
     speed. We are faced with a trade-off among speed, cost, and size. Ideally, main
     memory should be built with the same technology as that of the processor registers,
     giving memory cycle times comparable to processor cycle times. This has always
     been too expensive a strategy. The solution is to exploit the principle of locality by
     providing a small, fast memory between the processor and main memory, namely
     the cache.
