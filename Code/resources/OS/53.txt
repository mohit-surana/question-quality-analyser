Direct Memory Access
                                              1.7 / DIRECT MEMORY ACCESS                       31
     block size increases, more useful data are brought into the cache. The hit ratio will
     begin to decrease, however, as the block becomes even bigger and the probability of
     using the newly fetched data becomes less than the probability of reusing the data
     that have to be moved out of the cache to make room for the new block.
     When a new block of data is read into the cache, the mapping function deter-
     mines which cache location the block will occupy. Two constraints affect the design
     of the mapping function. First, when one block is read in, another may have to be
     replaced. We would like to do this in such a way as to minimize the probability that
     we will replace a block that will be needed in the near future. The more flexible the
     mapping function, the more scope we have to design a replacement algorithm to
     maximize the hit ratio. Second, the more flexible the mapping function, the more
     complex is the circuitry required to search the cache to determine if a given block
     is in the cache.
     The replacement algorithm chooses, within the constraints of the mapping
     function, which block to replace when a new block is to be loaded into the cache and
     the cache already has all slots filled with other blocks. We would like to replace the
     block that is least likely to be needed again in the near future. Although it is impos-
     sible to identify such a block, a reasonably effective strategy is to replace the block
     that has been in the cache longest with no reference to it. This policy is referred to
     as the least-recently-used (LRU) algorithm. Hardware mechanisms are needed to
     identify the least-recently-used block.
     If the contents of a block in the cache are altered, then it is necessary to write it
     back to main memory before replacing it. The write policy dictates when the mem-
     ory write operation takes place. At one extreme, the writing can occur every time
     that the block is updated. At the other extreme, the writing occurs only when the
     block is replaced. The latter policy minimizes memory write operations but leaves
     main memory in an obsolete state. This can interfere with multiple-processor opera-
     tion and with direct memory access by I/O hardware modules.
     Finally, it is now commonplace to have multiple levels of cache, labeled L1
     (cache closest to the processor), L2, and in many cases a third level L3. A discus-
     sion of the performance benefits of multiple cache levels is beyond our scope; see
     [STAL10] for a discussion.
1.7  DIRECT MEMORY ACCESS
     Three techniques are possible for I/O operations: programmed I/O, interrupt-driven
     I/O, and direct memory access (DMA). Before discussing DMA, we briefly define
     the other two techniques; see Appendix C for more detail.
     When the processor is executing a program and encounters an instruction
     relating to I/O, it executes that instruction by issuing a command to the appro-
     priate I/O module. In the case of programmed I/O, the I/O module performs the
     requested action and then sets the appropriate bits in the I/O status register but
     takes no further action to alert the processor. In particular, it does not interrupt the
     processor. Thus, after the I/O instruction is invoked, the processor must take some
     active role in determining when the I/O instruction is completed. For this purpose,

32  CHAPTER 1 / COMPUTER SYSTEM OVERVIEW
    the processor periodically checks the status of the I/O module until it finds that the
    operation is complete.
        With programmed I/O, the processor has to wait a long time for the I/O mod-
    ule of concern to be ready for either reception or transmission of more data. The
    processor, while waiting, must repeatedly interrogate the status of the I/O module.
    As a result, the performance level of the entire system is severely degraded.
        An alternative, known as interrupt-driven I/O, is for the processor to issue
    an I/O command to a module and then go on to do some other useful work. The
    I/O module will then interrupt the processor to request service when it is ready to
    exchange data with the processor. The processor then executes the data transfer, as
    before, and then resumes its former processing.
        Interrupt-driven I/O, though more efficient than simple programmed I/O, still
    requires the active intervention of the processor to transfer data between memory
    and an I/O module, and any data transfer must traverse a path through the proces-
    sor. Thus, both of these forms of I/O suffer from two inherent drawbacks:
    1.  The I/O transfer rate is limited by the speed with which the processor can test
        and service a device.
    2.  The processor is tied up in managing an I/O transfer; a number of instructions
        must be executed for each I/O transfer.
        When large volumes of data are to be moved, a more efficient technique is
    required: direct memory access (DMA). The DMA function can be performed by
    a separate module on the system bus or it can be incorporated into an I/O module.
    In either case, the technique works as follows. When the processor wishes to read
    or write a block of data, it issues a command to the DMA module, by sending to the
    DMA module the following information:
       Whether a read or write is requested
       The address of the I/O device involved
       The starting location in memory to read data from or write data to
       The number of words to be read or written
        The processor then continues with other work. It has delegated this I/O opera-
    tion to the DMA module, and that module will take care of it. The DMA module
    transfers the entire block of data, one word at a time, directly to or from memory
    without going through the processor. When the transfer is complete, the DMA
    module sends an interrupt signal to the processor. Thus, the processor is involved
    only at the beginning and end of the transfer.
        The DMA module needs to take control of the bus to transfer data to and from
    memory. Because of this competition for bus usage, there may be times when the
    processor needs the bus and must wait for the DMA module. Note that this is not
    an interrupt; the processor does not save a context and do something else. Rather,
    the processor pauses for one bus cycle (the time it takes to transfer one word across
    the bus). The overall effect is to cause the processor to execute more slowly during
    a DMA transfer when processor access to the bus is required. Nevertheless, for a
    multiple-word I/O transfer, DMA is far more efficient than interrupt-driven or
    programmed I/O.

                 1.8 / MULTIPROCESSOR AND MULTICORE ORGANIZATION                              33
1.8  MULTIPROCESSOR AND MULTICORE ORGANIZATION
     Traditionally, the computer has been viewed as a sequential machine. Most com-
     puter programming languages require the programmer to specify algorithms as
     sequences of instructions. A processor executes programs by executing machine
     instructions in sequence and one at a time. Each instruction is executed in a sequence
     of operations (fetch instruction, fetch operands, perform operation, store results).
         This view of the computer has never been entirely true. At the micro-operation
     level, multiple control signals are generated at the same time. Instruction pipelining,
     at least to the extent of overlapping fetch and execute operations, has been around
     for a long time. Both of these are examples of performing functions in parallel.
         As computer technology has evolved and as the cost of computer hardware
     has dropped, computer designers have sought more and more opportunities for par-
     allelism, usually to improve performance and, in some cases, to improve reliability.
     In this book, we examine the three most popular approaches to providing parallel-
     ism by replicating processors: symmetric multiprocessors (SMPs), multicore com-
     puters, and clusters. SMPs and multicore computers are discussed in this section;
     clusters are examined in Chapter 16.
     Symmetric Multiprocessors
     DEFINITION  An SMP can be defined as a stand-alone computer system with the
     following characteristics:
     1.  There are two or more similar processors of comparable capability.
     2.  These processors share the same main memory and I/O facilities and are inter-
         connected by a bus or other internal connection scheme, such that memory
         access time is approximately the same for each processor.
     3.  All processors share access to I/O devices, either through the same channels
         or through different channels that provide paths to the same device.
     4.  All processors can perform the same functions (hence the term symmetric).
     5.  The system is controlled by an integrated operating system that provides
         interaction between processors and their programs at the job, task, file, and
         data element levels.
         Points 1 to 4 should be self-explanatory. Point 5 illustrates one of the contrasts
     with a loosely coupled multiprocessing system, such as a cluster. In the latter, the
     physical unit of interaction is usually a message or complete file. In an SMP, indi-
     vidual data elements can constitute the level of interaction, and there can be a high
     degree of cooperation between processes.
         An SMP organization has a number of potential advantages over a uniproces-
     sor organization, including the following:
        Performance: If the work to be done by a computer can be organized so that
         some portions of the work can be done in parallel, then a system with multiple
         processors will yield greater performance than one with a single processor of
         the same type.
